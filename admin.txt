Supervised Learning

Program to implement Regression:




Linear regression
Multivariate Linear regression
Logistic regression

Please Explain Each Line so that i can give interview tomorrow on these things. And explain each things in lame language. Literally i don't have any idea how to implement.
ChatGPT said:
Absolutely! Let's start from zero and build up your understanding step-by-step for your interview tomorrow. You're going to do great if you understand the core logic behind each line of code and why it's used.

üî∂ What is Supervised Learning?
It‚Äôs like teaching a kid using examples.

In supervised learning, the algorithm is trained on a labeled dataset (means we give both input and correct output). Based on that, it learns how to map input to output.

üî∑ 1. Linear Regression
üìå Goal:
Predict a value (like salary) based on input (like years of experience).

‚úÖ Step-by-Step Program Explanation:
We‚Äôll use scikit-learn, a Python machine learning library.

python
Copy
Edit
# Step 1: Import necessary libraries
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
üîπExplanation:
pandas: To load and handle data.

LinearRegression: Model to predict value using a straight line.

matplotlib: To draw the graph of prediction.

python
Copy
Edit
# Step 2: Load the data (we use a small sample dataset for demo)
data = {'Experience': [1, 2, 3, 4, 5], 'Salary': [30000, 35000, 40000, 45000, 50000]}
df = pd.DataFrame(data)
üîπExplanation:
We‚Äôre creating a mini table with Experience and Salary.

df is a DataFrame that acts like a small Excel sheet.

python
Copy
Edit
# Step 3: Separate features (X) and label (y)
X = df[['Experience']]
y = df['Salary']
üîπExplanation:
X = input = Experience (independent variable)

y = output = Salary (dependent variable)

You must tell the model:

‚ÄúUse Experience to predict Salary.‚Äù

python
Copy
Edit
# Step 4: Train the model
model = LinearRegression()
model.fit(X, y)
üîπExplanation:
model.fit(X, y): Train the model. It will try to draw a best-fit straight line through the data points.

python
Copy
Edit
# Step 5: Predict salary for 6 years experience
predicted_salary = model.predict([[6]])
print(f"Predicted salary for 6 years experience: {predicted_salary[0]}")
üîπExplanation:
predict([[6]]): Asking model, ‚ÄúIf someone has 6 years of experience, what salary should they get?‚Äù

It will return a number like 55000.

python
Copy
Edit
# Step 6: Plot the results
plt.scatter(X, y, color='blue')  # actual data points
plt.plot(X, model.predict(X), color='red')  # line drawn by model
plt.xlabel('Experience')
plt.ylabel('Salary')
plt.title('Linear Regression')
plt.show()
üîπExplanation:
Shows the points (actual data) and the red line (model's prediction).

Helps in visualizing how well our model is working.

üî∑ 2. Multivariate Linear Regression
When we predict using more than one feature, like Experience and Education.

‚úÖ Code:
python
Copy
Edit
import pandas as pd
from sklearn.linear_model import LinearRegression

# Step 1: Create dataset with 2 features
data = {
    'Experience': [1, 2, 3, 4, 5],
    'Education': [10, 12, 14, 16, 18],  # Years of education
    'Salary': [30000, 35000, 40000, 45000, 50000]
}
df = pd.DataFrame(data)

# Step 2: Separate features and label
X = df[['Experience', 'Education']]  # Two features now
y = df['Salary']

# Step 3: Train model
model = LinearRegression()
model.fit(X, y)

# Step 4: Predict
prediction = model.predict([[6, 20]])
print(f"Predicted salary for 6 years experience and 20 years education: {prediction[0]}")
üîπExplanation:
We added another factor: Education.

model.predict([[6, 20]]): We predict salary based on 6 years experience + 20 years education.

üî∑ 3. Logistic Regression
Used for classification, not prediction of numbers.

üìå Example: Predict if person will buy or not buy (yes or no)
‚úÖ Code:
python
Copy
Edit
import pandas as pd
from sklearn.linear_model import LogisticRegression

# Step 1: Create binary dataset
data = {
    'Age': [22, 25, 47, 52, 46],
    'Salary': [20000, 30000, 50000, 60000, 52000],
    'Buy': [0, 0, 1, 1, 1]  # 0 = No, 1 = Yes
}
df = pd.DataFrame(data)

# Step 2: Features and Label
X = df[['Age', 'Salary']]
y = df['Buy']

# Step 3: Train model
model = LogisticRegression()
model.fit(X, y)

# Step 4: Predict
prediction = model.predict([[30, 40000]])
print(f"Will the person buy? {'Yes' if prediction[0]==1 else 'No'}")
üîπExplanation:
Buy is a binary value: 0 or 1 (classification).

Logistic Regression draws an S-shaped curve to separate "Buy" and "Not Buy".

predict([[30, 40000]]): Asks if a person of 30 years and ‚Çπ40k salary will buy.

üîö Summary Table:
Model	Used For	Output Example
Linear Regression	Predict numbers (1 input)	Salary = ‚Çπ55,000
Multivariate Regression	Predict numbers (multi-input)	Salary = ‚Çπ60,000
Logistic Regression	Predict categories (Yes/No, 0/1)	Will Buy = Yes

‚úÖ Final Tip for Interview:
If asked about difference between Linear and Logistic:

Linear Regression predicts a number.

Logistic Regression predicts a category (Yes/No).

Would you like a simple dataset to practice all 3? I can generate that and give you notebook-style code if you want.

You said:
Yes
ChatGPT said:
Here are the three CSV datasets you can download and use to practice:

üîπ Linear Regression Dataset (Experience vs Salary):
Download linear_regression_data.csv

üîπ Multivariate Linear Regression Dataset (Experience + Education vs Salary):
Download multivariate_regression_data.csv

üîπ Logistic Regression Dataset (Age + Salary ‚Üí Will Buy):
Download logistic_regression_data.csv

Let me know if you want Python code that reads these files and applies the models step by step (ready for Jupyter Notebook or PyCharm). 

You said:
Can you help in here only with the data how to do from basic prepocessing and all after that implement model and then predict
ChatGPT said:
Absolutely! Let's do everything step-by-step for each dataset in this chat itself, starting from:

‚úÖ Data Loading
‚úÖ Preprocessing
‚úÖ Training the model
‚úÖ Prediction
‚úÖ (Optional) Visualization

We'll cover 3 models one by one:

üî∑ PART 1: Linear Regression (üìä Experience ‚Üí Salary)
‚úÖ Step 1: Load the CSV Data
python
Copy
Edit
import pandas as pd

# Load the data
df = pd.read_csv("linear_regression_data.csv")

print(df)
Output:

nginx
Copy
Edit
   Experience  Salary
0           1   30000
1           2   35000
2           3   40000
3           4   45000
4           5   50000
‚úÖ Step 2: Preprocessing (Split X and y)
python
Copy
Edit
X = df[['Experience']]  # Input features should always be 2D
y = df['Salary']        # Target column
‚úÖ Step 3: Train the Model
python
Copy
Edit
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X, y)  # Training
‚úÖ Step 4: Predict
python
Copy
Edit
# Predict salary for 6 years of experience
salary = model.predict([[6]])
print(f"Predicted Salary: ‚Çπ{salary[0]}")
‚úÖ Step 5: Visualize (Optional)
python
Copy
Edit
import matplotlib.pyplot as plt

plt.scatter(X, y, color='blue')  # Original data
plt.plot(X, model.predict(X), color='red')  # Model prediction line
plt.xlabel("Experience")
plt.ylabel("Salary")
plt.title("Linear Regression")
plt.show()
üî∑ PART 2: Multivariate Linear Regression (üìä Experience + Education ‚Üí Salary)
‚úÖ Step 1: Load Data
python
Copy
Edit
df = pd.read_csv("multivariate_regression_data.csv")
print(df)
‚úÖ Step 2: Preprocessing
python
Copy
Edit
X = df[['Experience', 'Education']]
y = df['Salary']
‚úÖ Step 3: Train Model
python
Copy
Edit
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X, y)
‚úÖ Step 4: Predict
python
Copy
Edit
# Predict for Experience = 6, Education = 20
predicted_salary = model.predict([[6, 20]])
print(f"Predicted Salary: ‚Çπ{predicted_salary[0]}")
üî∑ PART 3: Logistic Regression (üî¢ Classification: Will Buy or Not)
‚úÖ Step 1: Load Data
python
Copy
Edit
df = pd.read_csv("logistic_regression_data.csv")
print(df)
‚úÖ Step 2: Preprocessing
python
Copy
Edit
X = df[['Age', 'Salary']]  # Input features
y = df['Buy']              # Labels (0 or 1)
‚úÖ Step 3: Train Model
python
Copy
Edit
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X, y)
‚úÖ Step 4: Predict
python
Copy
Edit
# Predict if a person aged 30 with ‚Çπ40,000 salary will buy
result = model.predict([[30, 40000]])

# Interpret result
print("Will Buy?" , "Yes" if result[0] == 1 else "No")
üß† Summary (for your Interview):
Model Type	Input	Output	Use Case Example
Linear	One feature	Number	Predict salary from experience
Multivariate	Multiple features	Number	Salary from experience + edu
Logistic	Multiple features	0 or 1	Buy or Not based on age/salary

Would you like me to combine all 3 into a single Jupyter Notebook format for practice or keep going model-by-model here?

You said:
Program to apply
Classification:¬†
Decision Tree
 Support Vector Machines with 
linear and non-linear kernel
functions
Na√Øve Bayes Classification¬†

 K nearest Neighbor 
Classification

"Like Regression Model you have explained and in a sequence wise like that only you explain each things in classification model one by one and in very lame language and give me some imporrtant question along with answer so that i can answer in viva. And also tell me why don't we use another model for that particular problem."
ChatGPT said:
Perfect! Let's now go step-by-step for Classification Models, just like we did for Regression.

We'll cover the following in very simple language:

‚úÖ Covered for Each Model:
What it does (in lame terms)

When to use it

Python program with step-by-step explanation

Why not use regression or other models instead

Interview questions with sample answers

üî∑ 1. Decision Tree Classification
üå± Lame Explanation:
Think of it like 20 Questions Game or a flowchart.
It keeps asking questions like ‚ÄúIs age > 30?‚Äù until it can say ‚ÄúYes, Buy‚Äù or ‚ÄúNo, Don‚Äôt Buy.‚Äù

‚úÖ Use Case:
If you want to classify something based on rules or conditions (like age, income, etc.), it's very good.

‚úÖ Program:
python
Copy
Edit
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# Step 1: Sample data
data = {
    'Age': [22, 25, 47, 52, 46],
    'Salary': [20000, 30000, 50000, 60000, 52000],
    'Buy': [0, 0, 1, 1, 1]
}
df = pd.DataFrame(data)

# Step 2: Features and target
X = df[['Age', 'Salary']]
y = df['Buy']

# Step 3: Train decision tree
model = DecisionTreeClassifier()
model.fit(X, y)

# Step 4: Predict
print(model.predict([[30, 40000]]))  # Output: [0] or [1]
ü§î Why not use Linear Regression?
Because regression gives numbers, and we want categories (Yes/No).

üéØ Viva Questions:
Question	Sample Answer
What is a decision tree?	It's like a flowchart that splits data based on conditions to make a decision.
When do you use it?	When we need simple yes/no or categorical outputs, and decision logic is easy to break into rules.

üî∑ 2. Support Vector Machine (SVM)
üå± Lame Explanation:
Imagine two groups of points on paper. SVM draws the best possible line to separate them, as far as possible from both.

‚úÖ Use Case:
Best when the data is cleanly separated and you need high accuracy.

‚úÖ Program (Linear Kernel):
python
Copy
Edit
from sklearn.svm import SVC

# Same dataset
X = df[['Age', 'Salary']]
y = df['Buy']

# Train SVM with linear kernel
model = SVC(kernel='linear')
model.fit(X, y)

print(model.predict([[30, 40000]]))
‚úÖ Program (Non-Linear - RBF Kernel):
python
Copy
Edit
# RBF = Radial Basis Function, good for non-linear separation
model = SVC(kernel='rbf')
model.fit(X, y)

print(model.predict([[30, 40000]]))
ü§î Why not use Decision Tree?
Trees can overfit (memorize too much).

SVM is often more accurate and better when points aren‚Äôt split by simple rules.

üéØ Viva Questions:
Question	Sample Answer
What is SVM?	A model that finds the best line (or curve) to separate classes.
What‚Äôs the difference between linear and non-linear kernels?	Linear draws a straight line, non-linear uses curves to split complex data.

üî∑ 3. Na√Øve Bayes Classification
üå± Lame Explanation:
Think like Google‚Äôs spam filter: ‚ÄúIf the message contains ‚Äòfree‚Äô, ‚Äòoffer‚Äô, and ‚Äòwin‚Äô, it‚Äôs probably spam.‚Äù
Na√Øve Bayes uses probability to guess the category.

‚úÖ Use Case:
Best when you have text data like spam detection, sentiment analysis.

‚úÖ Program:
python
Copy
Edit
from sklearn.naive_bayes import GaussianNB

X = df[['Age', 'Salary']]
y = df['Buy']

model = GaussianNB()
model.fit(X, y)

print(model.predict([[30, 40000]]))
ü§î Why not use SVM or Tree?
SVM needs more training time.

Na√Øve Bayes works faster and better for text and probability-based cases.

üéØ Viva Questions:
Question	Sample Answer
What is Na√Øve Bayes?	It's a classifier based on probability. It assumes all features are independent.
Why is it called na√Øve?	Because it naively assumes that all inputs are independent of each other.

üî∑ 4. K-Nearest Neighbor (KNN)
üå± Lame Explanation:
It‚Äôs like asking your 5 closest friends: ‚ÄúDo you buy or not?‚Äù ‚Äî and deciding based on majority.

‚úÖ Use Case:
When you don‚Äôt know the internal pattern, just want to use what the closest examples did.

‚úÖ Program:
python
Copy
Edit
from sklearn.neighbors import KNeighborsClassifier

X = df[['Age', 'Salary']]
y = df['Buy']

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X, y)

print(model.predict([[30, 40000]]))
ü§î Why not use SVM or Decision Tree?
KNN doesn‚Äôt ‚Äútrain‚Äù a model ‚Äî it‚Äôs just memory-based.

Use when data is small and simple.

üéØ Viva Questions:
Question	Sample Answer
What is KNN?	It's a lazy algorithm that checks the 'K' nearest neighbors and decides based on majority class.
What is the main drawback?	It becomes slow on large datasets since it stores all the data.

‚úÖ Summary Table:
Model	Easy Example	Best For	Weakness
Decision Tree	If Age > 30 ‚Üí Buy	Rule-based decisions	Overfitting
SVM	Line to separate classes	High accuracy separation	Slow on large datasets
Na√Øve Bayes	Spam filter (probability)	Text-based or probability logic	Assumes independence
KNN	Ask 5 closest neighbors	Small datasets with no pattern	Very slow on large datasets

If you'd like, I can also help you with Jupyter Notebook-ready code, or test file setups.

Would you like a single dataset to test all these models with just copy-paste code?

You said:
I want different dataset in which data is not clean and all after cleaning then process these classification model one by one.
ChatGPT said:
Here‚Äôs a real-world-style dirty dataset you can use for classification model practice (with missing and messy values):

üìÅ Download classification_dirty_data.csv

üü° What‚Äôs inside the dataset?
Age	Salary	Gender	Buy
25	30000	Male	0
NaN	40000	Female	1
47	50000	Female	1
52	None	Male	1
thirty	60000	Male	1
33	thirty five thousand	Other	0
29	42000	Male	0
41	48000	NaN	1
36	51000	Female	NaN
"45"	53000	Male	1

Let me now walk you through:

üîß Step-by-step data cleaning
üìä Apply classification models:

Decision Tree

SVM

Na√Øve Bayes

KNN

Shall we begin with Step 1: Data Cleaning and Preprocessing? 

You said:
Yes 
ChatGPT said:
Awesome! Let's begin with:

‚úÖ STEP 1: Data Cleaning and Preprocessing
We‚Äôll do the following:

Load the dataset

Handle missing values (NaN)

Convert messy text (like "thirty", "thirty five thousand") into numbers

Convert categorical column (Gender) into numeric

Handle target column (Buy) if any missing

Final cleaned data ready for classification

üîΩ Load and See Raw Data
python
Copy
Edit
import pandas as pd

# Load the dirty dataset
df = pd.read_csv("classification_dirty_data.csv")

# View raw data
print(df)
üîÑ Step 2: Fix "Age" column
python
Copy
Edit
# Replace string 'thirty' and '45' with proper numbers
df['Age'] = df['Age'].replace({'thirty': 30, '45': 45})

# Convert to numeric (coerce converts any error to NaN)
df['Age'] = pd.to_numeric(df['Age'], errors='coerce')
üîÑ Step 3: Fix "Salary" column
python
Copy
Edit
# Replace string with number
df['Salary'] = df['Salary'].replace({'thirty five thousand': 35000})

# Convert to numeric
df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')
üîÑ Step 4: Handle missing values
python
Copy
Edit
# Fill missing Age/Salary with mean
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Salary'].fillna(df['Salary'].mean(), inplace=True)

# Fill missing Gender with mode (most common)
df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)

# Fill missing Buy with mode
df['Buy'].fillna(df['Buy'].mode()[0], inplace=True)
üîÑ Step 5: Convert Categorical "Gender" ‚Üí Numeric
python
Copy
Edit
# One-hot encoding or simple mapping
df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1, 'Other': 2})
‚úÖ Final Cleaned Data
python
Copy
Edit
print(df)
üß† Summary of Cleaned Dataset
Age	Salary	Gender	Buy
25	30000	0	0
37	40000	1	1
47	50000	1	1
52	45111	0	1
30	60000	0	1
33	35000	2	0
29	42000	0	0
41	48000	1	1
36	51000	1	1
45	53000	0	1

‚úÖ Now your dataset is clean and numeric ‚Äî ready for classification models!

Would you like to move on to applying the Decision Tree model next using this cleaned data?

You said:
yes now next
ChatGPT said:
Perfect! Now that our data is clean, let‚Äôs move to:

üî∑ STEP 2: Decision Tree Classification
‚úÖ What are we doing?
We‚Äôll use this data to train a model to predict if a customer will buy a product (Buy = 1 or 0) based on:

Age

Salary

Gender

üß† Step-by-step in Very Simple Language:
1Ô∏è‚É£ Choose Features (inputs) and Target (output)
python
Copy
Edit
X = df[['Age', 'Salary', 'Gender']]  # Inputs
y = df['Buy']                        # Output (Label)
2Ô∏è‚É£ Train the Decision Tree Model
python
Copy
Edit
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X, y)  # Train the model
3Ô∏è‚É£ Make a Prediction
Example: Predict if a 35-year-old Female with ‚Çπ45,000 salary will buy.

python
Copy
Edit
prediction = model.predict([[35, 45000, 1]])  # Gender 1 = Female
print("Will Buy?" , "Yes" if prediction[0] == 1 else "No")
üß™ 4Ô∏è‚É£ (Optional) Test with multiple entries
python
Copy
Edit
# Test with multiple new people
test_data = [
    [30, 40000, 0],  # Male
    [40, 50000, 1],  # Female
    [28, 35000, 2],  # Other
]

results = model.predict(test_data)
print("Predictions:", results)
üìä 5Ô∏è‚É£ Visualize the Tree (Optional)
python
Copy
Edit
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plot_tree(model, feature_names=['Age', 'Salary', 'Gender'], class_names=['No', 'Yes'], filled=True)
plt.show()
üéØ Viva Q&A (Decision Tree)
Question	Simple Answer
What is a Decision Tree?	It‚Äôs a model that splits data by asking questions (like a flowchart) until it decides a class.
What are the advantages?	Easy to understand, no need to scale data, works for both classification and regression.
What‚Äôs the drawback?	It can overfit (memorize) if not pruned or limited.

‚úÖ Now you're done with Decision Tree!

Shall we continue with SVM (Support Vector Machine) ‚Äì Linear and Non-Linear?

You said:
yes
ChatGPT said:
Great! Let‚Äôs now go to:

üî∑ STEP 3: Support Vector Machine (SVM) ‚Äì Linear & Non-Linear
üß† What is SVM (In Lame Terms)?
Think of it like drawing the best line to split two groups (Buy = 1 and Buy = 0).
It tries to maximize the space between the line and the nearest data points on both sides.

Linear SVM ‚Üí draws a straight line (good when data is cleanly separable).

Non-linear SVM (RBF kernel) ‚Üí draws a curve when the data can‚Äôt be split with a line.

‚úÖ Step-by-Step Code for Linear SVM
python
Copy
Edit
from sklearn.svm import SVC

# Inputs and output
X = df[['Age', 'Salary', 'Gender']]
y = df['Buy']

# Linear SVM Model
linear_model = SVC(kernel='linear')
linear_model.fit(X, y)

# Predict for a 35-year-old Female with ‚Çπ45,000 salary
prediction = linear_model.predict([[35, 45000, 1]])
print("Linear SVM Prediction:", "Will Buy" if prediction[0] == 1 else "Won't Buy")
‚úÖ Step-by-Step Code for Non-Linear SVM (RBF Kernel)
python
Copy
Edit
# Non-linear SVM Model using RBF kernel
rbf_model = SVC(kernel='rbf')
rbf_model.fit(X, y)

# Predict for same person
prediction = rbf_model.predict([[35, 45000, 1]])
print("Non-Linear SVM Prediction:", "Will Buy" if prediction[0] == 1 else "Won't Buy")
üîç What‚Äôs the Difference Between Linear and Non-Linear?
Linear SVM	Non-Linear SVM (RBF)
Draws a straight line	Draws a curve (for complex data)
Faster	Slightly slower but more flexible
Works when classes are well separated	Works when data is mixed and overlapping

üéØ Viva Q&A (SVM)
Question	Simple Answer
What is SVM?	A model that finds the best line (or curve) that separates two categories.
What is a kernel?	A method used by SVM to draw complex boundaries (like curves).
What is the difference between linear and RBF kernel?	Linear draws a straight line, RBF draws curves for complex data.
When not to use SVM?	On very large datasets or when interpretability is more important.

‚úÖ Now we‚Äôve covered Decision Tree and SVM (both types).

Would you like to move to Na√Øve Bayes next?

You said:
yes
ChatGPT said:
Awesome! Let‚Äôs move on to:

üî∑ STEP 4: Na√Øve Bayes Classification
üß† What is Na√Øve Bayes? (In Lame Terms)
Think of it like guessing a class based on probability.
It asks: ‚ÄúBased on age, salary, gender‚Ä¶ what‚Äôs the chance that this person will buy?‚Äù

It uses math (Bayes‚Äô Theorem) + assumes all features are independent (that‚Äôs the ‚Äúna√Øve‚Äù part).

‚úÖ When to Use Na√Øve Bayes?
When features are categorical or independent

For text spam detection, medical diagnosis, etc.

Very fast and works great even with small datasets

‚úÖ Step-by-Step Code
python
Copy
Edit
from sklearn.naive_bayes import GaussianNB

# Input & Output
X = df[['Age', 'Salary', 'Gender']]
y = df['Buy']

# Train the model
nb_model = GaussianNB()
nb_model.fit(X, y)

# Predict for a new person
prediction = nb_model.predict([[35, 45000, 1]])  # Female
print("Na√Øve Bayes Prediction:", "Will Buy" if prediction[0] == 1 else "Won't Buy")
üß™ Test on multiple people
python
Copy
Edit
test_data = [
    [30, 40000, 0],  # Male
    [40, 50000, 1],  # Female
    [28, 35000, 2],  # Other
]

results = nb_model.predict(test_data)
print("Predictions (Na√Øve Bayes):", results)
üéØ Viva Q&A (Na√Øve Bayes)
Question	Simple Answer
What is Na√Øve Bayes?	A model that uses probabilities to classify based on past data.
Why is it called ‚ÄúNa√Øve‚Äù?	It assumes all input features are independent (which is usually not true).
Where is it used?	Spam detection, sentiment analysis, medical diagnosis.
What are the advantages?	Fast, simple, works well with small data.
Disadvantages?	Assumes independence, may be less accurate on correlated data.

‚úÖ Now you‚Äôve done Na√Øve Bayes as well.

Shall we move on to the last model: K-Nearest Neighbors (KNN)?

You said:
yes
ChatGPT said:
Perfect! Let's wrap up classification with:

üî∑ STEP 5: K-Nearest Neighbors (KNN) Classification
üß† What is KNN? (In Lame Terms)
Imagine you're in a room full of people.
You want to predict if a new person will buy ‚Äî so you look at their K nearest neighbors (the most similar people).
If most of them bought, you say the new person will also buy.

It uses distance to measure similarity (e.g., age, salary).

It‚Äôs like "asking your neighbors what they did".

‚úÖ When to Use KNN?
When you want simple and interpretable results

Works well for small to medium datasets

You must scale your data because it depends on distance

‚úÖ Step-by-Step Code
üîπ 1. Import & Preprocess (Scaling)
python
Copy
Edit
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

# Inputs and outputs
X = df[['Age', 'Salary', 'Gender']]
y = df['Buy']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
üîπ 2. Train the KNN Model
python
Copy
Edit
knn_model = KNeighborsClassifier(n_neighbors=3)  # Try with k=3
knn_model.fit(X_scaled, y)
üîπ 3. Predict for New Person
python
Copy
Edit
new_person = scaler.transform([[35, 45000, 1]])  # Female
prediction = knn_model.predict(new_person)
print("KNN Prediction:", "Will Buy" if prediction[0] == 1 else "Won't Buy")
üîπ 4. Predict for Multiple People
python
Copy
Edit
test_data = scaler.transform([
    [30, 40000, 0],
    [40, 50000, 1],
    [28, 35000, 2],
])

results = knn_model.predict(test_data)
print("KNN Predictions:", results)
üîç What is K in KNN?
K = number of neighbors to check

If K=3, look at 3 closest people

Too small ‚Üí may be noisy

Too large ‚Üí may miss local patterns

üéØ Viva Q&A (KNN)
Question	Simple Answer
What is KNN?	A model that predicts based on the K nearest neighbors' behavior
What does ‚ÄúK‚Äù mean?	The number of neighbors to check before deciding
Do we need to scale data?	Yes, because it uses distance for similarity
Advantages?	Simple, no training time
Disadvantages?	Slow for large data, sensitive to irrelevant features or unscaled data

‚ö†Ô∏è Why not use other models?
Model	When Not To Use
Decision Tree	When overfitting is a concern
SVM	With large or noisy data
Na√Øve Bayes	When features are highly correlated
KNN	When dataset is very big or unscaled

‚úÖ You‚Äôve now implemented and understood:

Decision Tree

SVM (Linear & Non-linear)

Na√Øve Bayes

KNN



Ensemble Techniques
Improving the Performance 
Program to demonstrate Evaluation
of the model

 

Feature Selection

 

Cross Validation Techniques

 

Hyper parameter tuning

"Explain each topic in simple and lame languages and provide practise question with dataset and provide some viva question along with that so that i can answer to them each words in the code"
ChatGPT said:
Great! Let‚Äôs walk through these advanced but important topics in Machine Learning step by step in very simple, beginner-friendly language. We'll go through:

Ensemble Techniques

Improving Model Performance

Evaluation of the Model

Feature Selection

Cross Validation Techniques

Hyperparameter Tuning

After each section, I‚Äôll include:

‚úÖ Simple Python practice with a dataset

üéØ Viva questions

üß† Answers with code-level explanation

‚úÖ 1. Ensemble Techniques (Teamwork of Models)
üìò What it means (Simple):
Imagine you want to make a decision (like choosing a movie). You ask multiple friends and go with the majority choice. That‚Äôs an ensemble!

üß† Types of Ensemble Methods:

Method	Idea	Analogy
Bagging	Train many models on different parts of data	Ask 5 friends about 5 different movies
Boosting	Train models one after another, each learning from previous mistake	Like a student learning from previous errors
Voting	Combine predictions from multiple models	Majority wins
Stacking	Combine predictions using another model	A teacher (meta-model) checks all students' answers

‚úÖ Python Code: Voting Classifier (Ensemble of KNN, DT, SVM)

python
Copy
Edit
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# 3 models
knn = KNeighborsClassifier()
dt = DecisionTreeClassifier()
svm = SVC(probability=True)

# Ensemble model
voting_clf = VotingClassifier(estimators=[('knn', knn), ('dt', dt), ('svm', svm)], voting='soft')
voting_clf.fit(X_train, y_train)

print("Accuracy:", voting_clf.score(X_test, y_test))
üéØ Viva Questions:

What is ensemble learning?

What is the difference between Bagging and Boosting?

What is Voting Classifier?

‚úÖ 2. Improving Model Performance
üìò Simple Meaning:
Making your model smarter, faster, and more accurate using:

Clean data

Feature engineering

Regularization

Cross-validation

Hyperparameter tuning

üéØ Viva Questions:

What are the different ways to improve model performance?

What is overfitting and underfitting?

‚úÖ 3. Model Evaluation (How well model is doing)
üìò Simple Meaning:
After training a model, you must check how good it is.
Key terms:

Accuracy = (Correct / Total)

Precision = Focus on positive predictions (e.g., how many emails marked spam are actually spam)

Recall = Focus on detecting all actual positives

F1-Score = Balance between precision and recall

Confusion Matrix = A table showing TP, TN, FP, FN

‚úÖ Python Code:

python
Copy
Edit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

y_pred = voting_clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, y_pred, average='macro'))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
üéØ Viva Questions:

What is the difference between Precision and Recall?

When to use F1-Score?

What does a confusion matrix show?

‚úÖ 4. Feature Selection (Pick only useful columns)
üìò Simple Meaning:
Removing useless features (columns) from your data, like:

Irrelevant: hair color for predicting salary

Repetitive: two columns showing same thing

Techniques:

Manual (domain knowledge)

Statistical (correlation, chi2, ANOVA)

Automated (RFE, SelectKBest)

‚úÖ Python Code (SelectKBest):

python
Copy
Edit
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
X_new = SelectKBest(score_func=chi2, k=2).fit_transform(X, y)

print("Original shape:", X.shape)
print("New shape after feature selection:", X_new.shape)
üéØ Viva Questions:

Why do we use feature selection?

What is SelectKBest?

What happens if we don‚Äôt remove irrelevant features?

‚úÖ 5. Cross Validation (Fair Testing)
üìò Simple Meaning:
Instead of testing once, test multiple times on different parts of data to get an average performance.
It's like taking 5 mock tests before your real exam.

Types:

K-Fold

Stratified K-Fold

‚úÖ Python Code:

python
Copy
Edit
from sklearn.model_selection import cross_val_score

scores = cross_val_score(voting_clf, X, y, cv=5)
print("Cross-validation scores:", scores)
print("Average CV score:", scores.mean())
üéØ Viva Questions:

What is k-fold cross-validation?

Why do we use cross-validation?

What is the benefit of CV over single train-test split?

‚úÖ 6. Hyperparameter Tuning (Finding best settings)
üìò Simple Meaning:
Trying different settings to make model better. Like adjusting ingredients to improve your cooking.

Techniques:

Grid Search (try all combinations)

Random Search (try random combinations)

‚úÖ Python Code (Grid Search with SVM):

python
Copy
Edit
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

grid = GridSearchCV(SVC(), param_grid, cv=3)
grid.fit(X_train, y_train)

print("Best parameters:", grid.best_params_)
print("Best score:", grid.best_score_)
üéØ Viva Questions:

What is hyperparameter tuning?

What is the difference between GridSearchCV and RandomSearchCV?

Why tuning is important?

üß™ Sample Dataset for Practice
You can practice using:

Iris Dataset (comes with sklearn)

Or download a messy dataset from: https://www.kaggle.com/datasets
(Search: Titanic, Heart Disease, Diabetes)

If you'd like, I can provide you a downloadable dataset with missing values and unnecessary columns for practice.

Would you like:

A custom practice dataset with messy data (NaNs, irrelevant columns)?

Practice assignment-style questions for each concept above?

Let me know and I‚Äôll prepare those for you!

You said:
ye
ChatGPT said:
Here is your messy dataset for practice:
üëâ Download messy_dataset.csv

üìä Features in the dataset:
Name: Irrelevant for prediction (to be dropped)

Age: Has missing values

Salary: Has missing values

Gender: Has a missing value

Has_Degree: Categorical (needs encoding)

Random_Col: Useless column

Target: 0 or 1 (your classification target)

‚úÖ Practice Tasks:
Clean the data:

Drop Name and Random_Col

Fill missing Age and Salary with mean

Handle missing Gender by filling most frequent

Encode Gender and Has_Degree to numbers

Apply Feature Selection:

Use SelectKBest to choose top features for predicting Target

Train a classifier:

Try DecisionTreeClassifier or KNN

Use train_test_split

Evaluate the model:

Accuracy, Precision, Recall, F1-Score, Confusion Matrix

Do Cross Validation and Hyperparameter Tuning

*****************************************************************************************************************************************************************
***Data PreProssing***

//Box plot will help us to find out the mean values
import seaborn as sns
sns.boxplot(data=titanic, x=titanic.Pclass, y=titanic.Age, palette='rocket')


//Accuracy (True Positive )
tn, fp, fn, tp = cfm.ravel()
accuracy = (tp + tn) / (tn + fp + fn + tp)
accuracy

//One Hot Encoding 
titanic_dummies = pd.get_dummies(titanic_cat2).astype(int)


//LabelEncoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit_transform(titanic_cat2['Embarked'])


//Matplotlib & SNS
--Heatmap
  sns.heatmap(df.isnull(), yticklabels=False)


--Count plot
A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable.
  sns.countplot(data=df, x='Survived', hue='Sex')

--displot
### To see the distribution of age throughout the data.
sns.distplot(df['Age'])


***Unsupervised Learning***


***--Apriori Algo***

df = pd.read_csv('/content/puchased_data.csv.csv', header=None)
print(df.to_string())

import numpy as np

records = []
for i in range(len(df)):
    row = []
    for j in range(len(df.columns)):
        value = df.values[i, j]
        if pd.notna(value):
            row.append(str(value))
    records.append(row)

records

!pip install apyori
from apyori import apriori

association_rule = apriori(records, min_support=0.5, min_confidence=0.75)

association_result = list(association_rule)

association_result



****--Kmeans****
from google.colab import files
import pandas as pd
import warnings
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')


plt.scatter(income_df.Age, income_df.Income)

plt.scatter(income_df.Age, income_df.Income)

kmean = KMeans(n_clusters=3)
kmean

y_pred = kmean.fit_predict(income_df[['Age', 'Income']])
y_pred

income_df['Cluster'] = y_pred
income_df

//for visulization 
df1 = income_df[income_df.Cluster == 0]
df2 = income_df[income_df.Cluster == 1]
df3 = income_df[income_df.Cluster == 2]

df1

df2

df3

income_df.describe()

plt.scatter(df1.Age, df1.Income, color='red')
plt.scatter(df2.Age, df2.Income, color='green')
plt.scatter(df3.Age, df3.Income, color='blue')
plt.xlabel('Age')
plt.ylabel('Income')
plt.title("Age Income Scatter Plot")

//for properly scaled diagram
scaler = MinMaxScaler()

scaler.fit(income_df[['Income']])

income_df['Income'] = scaler.transform(income_df[['Income']])

scaler.fit(income_df[['Age']])

income_df['Age'] = scaler.transform(income_df[['Age']])

income_df

km=KMeans(n_clusters=3)
y_predict=km.fit_predict(income_df[['Age','Income']])
y_predict
income_df['Cluster']=y_predict
income_df

df1 = income_df[income_df.Cluster == 0]
df2 = income_df[income_df.Cluster == 1]
df3 = income_df[income_df.Cluster == 2]

plt.scatter(df1.Age, df1.Income, color='red')
plt.scatter(df2.Age, df2.Income, color='green')
plt.scatter(df3.Age, df3.Income, color='blue')
plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1] , color = 'purple',marker="*")
plt.xlabel('Age')
plt.ylabel('Income')
plt.title("Age Income Scatter Plot")

kmean.cluster_centers_


****--Elbow Method****
## Elbow Method is used on the clustering to identify the number of clusters

from google.colab import files
import pandas as pd
import warnings
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

mall_cust_df.info()

mall_cust_df

mall_cust_df.shape

mall_cust_df.describe()

X = mall_cust_df[["Annual Income (k$)", "Spending Score (1-100)"]].values
X

kmean = KMeans(n_clusters=3)
kmean

y_pred = kmean.fit_predict(X)

y_pred

mall_cust_df['Cluster'] = y_pred

mall_cust_df

kmean.cluster_centers_

sum_of_squared_distance = []

for k in range(1, 10):
  km = KMeans(n_clusters=k)
  km = km.fit(X)
  sum_of_squared_distance.append(km.inertia_)

sum_of_squared_distance

##Plot sum_of_squared_distance & determine optimal k using Elbow method
plt.plot(range(1, 10), sum_of_squared_distance)
plt.title("Elbow Method")
plt.xlabel("No of Clusters")
plt.xlabel("Sum of Squared Distances")



***Supervises Learning***
**Linear Regression **

from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model
import warnings
warnings.filterwarnings('ignore')

houseData.info()

houseData.head()

X = houseData.Area
Y = houseData.Price

plt.figure(dpi=150)
plt.xlabel('Flat Area')
plt.ylabel('Sales Price')
plt.scatter(X, Y, marker='.', color='red')

reg = linear_model.LinearRegression()

reg.fit(houseData[['Area']], houseData.Price)

reg.predict([[1900]])

//slope
reg.coef_

//intercept
reg.intercept_

//Manually checking
y = reg.coef_ * 1900 + reg.intercept_
y

*** to get MSE Error ***

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
print("Testing Errors")
print("MAE: ", mean_absolute_error(y_test, y_pred))
print("MSE: ", mean_squared_error(y_test, y_pred))
print("R2 Square: ", r2_score(y_test, y_pred))


**Multivariate Regression**

houseData = pd.read_csv(filename)
houseData.info()

houseData.head()

//preprocessing
//if the value is NaN
bedroomMean = houseData.Bedroom.mean()
bedroomMean
houseData.Bedroom = houseData.Bedroom.fillna(value=bedroomMean)
houseData

//regression
reg = linear_model.LinearRegression()

reg.fit(houseData[['Area', 'Bedroom', 'Age']], houseData.Price)

//slope
reg.coef_

reg.predict([[1900]])

//intecept
reg.intercept_

//predict for diff values
reg.predict([[3000, 3, 50]])

//manually checking
y = reg.coef_[0] * 3000 + reg.coef_[1] * 3 + reg.coef_[2] * 50 + reg.intercept_
y


**Logistic Regression**
from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

logisticData.info()
plt.scatter(logisticData.Age, logisticData.Purchased, marker='*', color='green')
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(logisticData[['Age']], logisticData.Purchased)
model.predict([[27]])

or

X = sampleData[['Age']]
Y = sampleData.Purchased
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
y_pred
print("Actual Price: ", y_test)
print("Actual Price: ", y_pred)


***Clustering***
**Decision Tree**
from google.colab import files
import pandas as pd
import warnings
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn import tree
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

X = golf_df.drop(['Play'], axis=1)
y = golf_df.Play

one_hot_data = pd.get_dummies(X)
one_hot_data = one_hot_data.astype(int)
one_hot_data

clf = DecisionTreeClassifier()
clf

plt.figure(figsize=(15, 10))
plot_tree(clf_train, filled=True, rounded=True, fontsize=15, feature_names=one_hot_data.columns)
plt.savefig('out_playgolf.pdf')

prediction = clf_train.predict([[0, 0, 1, 0, 1, 0, 0, 1, 0]])
prediction

X_train, X_test, y_train, y_test = train_test_split(one_hot_data, y, test_size=0.3)

clf = clf.fit(X_train, y_train)
clf

y_pred = clf.predict(X_test)

accuracy_score(y_test, y_pred)

cfm = confusion_matrix(y_test, y_pred)
cfm

//accuracy score
accuracy_score(y_test, y_pred)

//Precision score
print(f"Computed: {precision_score(y_test, y_pred, average=None)}")

//recall_score
print(f"Computed: {recall_score(y_test, y_pred, average=None)}")


//f1_score
print(f"Computed: {f1_score(y_test, y_pred, average=None)}")

//classification_report
print(classification_report(y_test, y_pred, target_names = ['0', '1']))


***KNN***
from google.colab import files
import pandas as pd
import warnings
from sklearn import tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

result_df.info()

X = result_df.drop(['Result'], axis=1)
y = result_df.Result

knn = KNeighborsClassifier(n_neighbors=3)
knn

train_df = knn.fit(X, y)
train_df

prediction = train_df.predict([[6, 8]])

prediction

--if u got iris data
X = iris_df.drop(['variety'], axis=1)
y = iris_df.variety
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
knn = KNeighborsClassifier(n_neighbors=50)
knn
train_df = knn.fit(X_train, y_train)
train_df
y_pred = knn.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Precision: {precision_score(y_test, y_pred, average='weighted')}")
print(f"Recall: {recall_score(y_test, y_pred, average='weighted')}")
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted')}")


***Hyperparameter Tunning
from sklearn.model_selection import GridSearchCV

grid_params = {
    'n_neighbors': [5, 7, 9, 11, 13, 15],
    'metric': ['minkowski', 'euclidean', 'manhattan']
}
gs = GridSearchCV(knn, grid_params, verbose=2, cv=3)
gs
g_res
g_res.best_score_
g_res.best_params_

**Naive Bayes**
flu_df = pd.read_csv(filename)
X = flu_df.drop(['Flue'], axis=1)
y = flu_df.Flue
le = LabelEncoder()
flu_df['Chills'] = le.fit_transform(flu_df['Chills'])
flu_df['RunnyNose'] = le.fit_transform(flu_df['RunnyNose'])
flu_df['Headache'] = le.fit_transform(flu_df['Headache'])
flu_df['Fever'] = le.fit_transform(flu_df['Fever'])

# Alternate way to perform lable encoding on X
# X = X.apply(le.fit_transform)

X = flu_df.drop(['Flue'], axis=1)
berNB = BernoulliNB()
berNB
train_df = berNB.fit(X, y)
train_df
prediction = train_df.predict([[1, 0, 1, 1]])
prediction


**SVM-classifier**
from google.colab import files
import pandas as pd
import warnings
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

social_df = pd.read_csv(filename)

X = social_df.drop(['Purchased', 'User ID',	'Gender'], axis=1)
y = social_df.Purchased

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
svModel = SVC(kernel='rbf')
svModel.fit(X_train, y_train)
y_pred = svModel.predict(X_test)
y_pred
accuracy_score(y_test, y_pred)

//diff kernal
//svModel2 = SVC(kernel='poly')
//svm1 = svModel2.fit(X_train, y_train)
//svm1

//svModel3 = SVC(kernel='linear')
//svm2 = svModel3.fit(X_train, y_train)
//svm2


***SVM***
from google.colab import files
import pandas as pd
import warnings
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

social_df = pd.read_csv(filename)
X = social_df.drop(['Purchased', 'User ID',	'Gender'], axis=1)
y = social_df.Purchased
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
svModel = SVC(kernel='rbf')
svModel.fit(X_train, y_train)
y_pred = svModel.predict(X_test)
y_pred
accuracy_score(y_test, y_pred)


**Random Forest**
--ada boost
from google.colab import files
import pandas as pd
import warnings
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

titanic_df = pd.read_csv(filename2)

titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)
titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0], inplace=True)
titanic_df['Fare'].fillna(titanic_df['Fare'].median(), inplace=True)
titanic_df.drop('Cabin', axis=1, inplace=True)
le = LabelEncoder()
titanic_df['Sex'] = le.fit_transform(titanic_df['Sex'])
titanic_df['Embarked'] = le.fit_transform(titanic_df['Embarked'])
features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
X = titanic_df[features]
y = titanic_df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(titanic_df.isnull().sum())

--AdaBoost Classifer
abc = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=42)
model = abc.fit(X_train, y_train)
y_pred_abc = model.predict(X_test)

print("AdaBoost Classifier:")
print(confusion_matrix(y_test, y_pred_abc))
print(classification_report(y_test, y_pred_abc))
print("Accuracy:", accuracy_score(y_test, y_pred_abc))

# Comparing Evaluation
print("AdaBoost Classifier:")
print(confusion_matrix(y_test, y_pred_abc))
print(classification_report(y_test, y_pred_abc))
print("Accuracy:", accuracy_score(y_test, y_pred_abc))

print("\nDecision Tree Classifier:")
print(confusion_matrix(y_test, y_pred_dtc))
print(classification_report(y_test, y_pred_dtc))
print("Accuracy:", accuracy_score(y_test, y_pred_dtc))


--Bagging

from google.colab import files
import pandas as pd
import warnings
from sklearn.ensemble import BaggingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

heart_df = pd.read_csv(filename)

X = heart_df.drop(columns=["target"], axis=1)
y = heart_df.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
bagging_model1 = BaggingClassifier(estimator= SVC(), n_estimators=100, oob_score=True, max_samples=0.8, random_state=42)
bagging_model1
bagging_model1.fit(X_train, y_train)
bagging_model1.oob_score_

//Hyperparameter Tunning
bagging_model2 = BaggingClassifier(estimator= DecisionTreeClassifier(), n_estimators=100, oob_score=True, max_samples=0.8, random_state=42)
bagging_model2.fit(X_train, y_train)
bagging_model2.oob_score_


//compare baggging and standalone
model3 = DecisionTreeClassifier()
model3.fit(X_train, y_train)
y_pred1 = model3.predict(X_test)
print(classification_report(y_test, y_pred1))
model4 = SVC(kernel='rbf')
model4.fit(X_train, y_train)
y_pred2 = model4.predict(X_test)
print(classification_report(y_test, y_pred2))


//Cross validation
score1 = cross_val_score(bagging_model1, X, y, cv=5)
score1
score1.mean()
score2 = cross_val_score(bagging_model2, X, y, cv=5)
score2.mean()
score3 = cross_val_score(model3, X, y, cv=5)
score3.mean()
score4 = cross_val_score(model4, X, y, cv=5)
score4.mean()

mean_score1 = score1.mean()
mean_score2 = score2.mean()
mean_score3 = score3.mean()
mean_score4 = score4.mean()

data = {
    "Model": ["Bagging Model 1 (SVC)", "Bagging Model 2 (Tree)", "Model 3 (Tree)", "Model 4 (SVC)"],
    "Mean Score": [mean_score1, mean_score2, mean_score3, mean_score4]
}

comparison_df = pd.DataFrame(data
comparison_df

*****************************************************************************************************************************************************************
https://docs.google.com/document/d/1jDz_eIpbCM_h2pbxMNF_PT6v9ZU8BU3ZDKyCyvoPZzk/edit?tab=t.0


https://docs.google.com/document/d/1fCW9J8oUBnU7Q9cXZizkRpDcflSPRhxpzM2P595aSeY/edit?usp=sharing
