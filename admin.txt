***Creating synthetic dataset with faker***
pip install faker
import faker
fake = faker.Faker()
for _ in range(5):
    print("Name: " ,fake.name())
    print("Address: ",fake.address())
    print("DOB: ", fake.date_of_birth())
    print("City: ", fake.city())
    print("Country: ", fake.country())
    print("Email: " , fake.email())
    print("Phone: ", fake.phone_number())
    print("Review: " , fake.text())
    print("-----------------")

fake = faker.Faker("gu_IN")
# hi_IN : for Hindi
for _ in range(5):
    print("Name: " ,fake.name())
    print("Address: ",fake.address())
    print("DOB: ", fake.date_of_birth())
    print("City: ", fake.city())
    print("Country: ", fake.country())
    print("Email: " , fake.email())
    print("Phone: ", fake.phone_number())
    print("Review: " , fake.text())
    print("-----------------")

import pandas as pd
import random
fake = faker.Faker()
data_dict= []
genders = ['Male', 'Female']
courses = ['MBA','MCA']
for _ in range(100):
  data_dict.append({"Name: ": fake.name(), "Address: " : fake.address(), "Gender: " : random.choice(genders),
                    "City: ": fake.city(), "Country: ": fake.country(),
                    "Review: ": fake.text(), "Courses: ": random.choice(courses)})
df = pd.DataFrame(data_dict)
# print(df)
df.to_csv("Data.csv", index=False)
fp = pd.read_csv("Data.csv")
fp

import faker
import faker_commerce
import random
import asyncio
import pandas as pd
faker = faker.Faker()
faker.add_provider(faker_commerce.Provider)
positive_words = ["versatile","excellent","worthy","great","good","friendly","efficient","durable","latest"]
negative_words = ["worst","bad","pathetic","trash","poor","disappointing","useless","broken","expensive"]
positive_phrases = [
    "Excellent quality",
    "Fast shipping",
    "Great value for money",
    "User-friendly website",
    "Responsive customer service",
    "Easy checkout process",
    "Wide product selection",
    "Accurate product descriptions",
    "Secure payment options",
    "Hassle-free returns"
]
negative_phrases = [
    "Poor product quality",
    "Late delivery",
    "Misleading product images",
    "Difficult return policy",
    "Unresponsive customer support",
    "Hidden shipping fees",
    "Complicated checkout process",
    "Out of stock frequently",
    "Incorrect product descriptions",
    "Payment system errors"
]
data = []
def generate_review(prod, type="positive"):  # removed undefined "product" default
    if type == "positive":
        return f"{prod} is {random.choice(positive_words)} and {random.choice(positive_phrases)}"
    else:
        return f"{prod} is {random.choice(negative_words)} and {random.choice(negative_phrases)}"
async def generate_positive_data():
    for _ in range(5):
        product = faker.ecommerce_name()
        data.append([product, generate_review(product, "positive"), "positive"])
        await asyncio.sleep(2)
async def generate_negative_data():
    for _ in range(5):
        product = faker.ecommerce_name()
        data.append([product, generate_review(product, "negative"), "negative"])
        await asyncio.sleep(1)
await asyncio.gather(generate_positive_data(), generate_negative_data())
product_df = pd.DataFrame(data, columns=["Product", "Review", "Sentiment"])
product_df

import faker
import faker_commerce
import random
import asyncio
import pandas as pd
faker = faker.Faker()
faker.add_provider(faker_commerce.Provider)
product_names_marathi = [
    "рд╕реНрдорд╛рд░реНрдЯрдлреЛрди", "рд▓реЕрдкрдЯреЙрдк", "рдЯреАрд╡реНрд╣реА", "рдлреНрд░рд┐рдЬ", "рд╡реЙрд╢рд┐рдВрдЧ рдорд╢реАрди",
    "рдХреЕрдореЗрд░рд╛", "рд╣реЗрдбрдлреЛрди", "рд╕реНрдкреАрдХрд░реНрд╕", "рдШрдбреНрдпрд╛рд│", "рд╕рд╛рдпрдХрд▓",
    "рдПрдЕрд░ рдХрдВрдбрд┐рд╢рдирд░", "рдорд┐рдХреНрд╕рд░ рдЧреНрд░рд╛рдЗрдВрдбрд░", "рдЗрд▓реЗрдХреНрдЯреНрд░рд┐рдХ рдХреЗрддрд▓реА", "рдЯреЕрдмрд▓реЗрдЯ",
    "рдкреНрд░рд┐рдВрдЯрд░", "рдХреВрд▓рд░", "рдмреБрдХ", "рдмреЕрдЧ", "рд╢реВрдЬ", "рдЯреА-рд╢рд░реНрдЯ"
]
positive_words_marathi = [
    "рдЙрддреНрдХреГрд╖реНрдЯ ЁЯСН", "рдЬрд▓рдж ЁЯЪА", "рд╡рд┐рд╢реНрд╡рд╛рд╕рд╛рд░реНрд╣ тЬЕ", "рдЯрд┐рдХрд╛рдК ЁЯТк", "рд╕реБрдВрджрд░ ЁЯМЯ",
    "рдЖрд░рд╛рдорджрд╛рдпрдХ ЁЯЫЛя╕П", "рдЙрдкрдпреБрдХреНрдд ЁЯОп", "рдкрд░рд╡рдбрдгрд╛рд░реЗ ЁЯТ░", "рд╕реЛрдкрдВ ЁЯЦ▒я╕П", "рдЪрд╛рдВрдЧрд▓рд╛ ЁЯПЕ",
    "рдЖрдХрд░реНрд╖рдХ тЬи", "рд╕рд╢рдХреНрдд ЁЯТе", "рд╡рд┐рд╢реНрд╡рд╕рдиреАрдп ЁЯФТ", "рдЧреБрдгрд╡рддреНрддрд╛рдкреВрд░реНрдг ЁЯПЖ", "рддрдпрд╛рд░рджрд╛рд░ ЁЯФз",
    "рддрдЬреНрдЮрд╛рдЪрд╛ ЁЯза", "рд╕реБрд▓рдн ЁЯФС", "рдирд╡реАрдирддрдо ЁЯЖХ", "рдлрд╛рдпрджреНрдпрд╛рдЪреЗ ЁЯТб", "рдкреНрд░рднрд╛рд╡реА ЁЯФе"
]
positive_phrases_marathi = [
    "рдЬрд▓рдж рд╕реЗрд╡рд╛ тЪб", "рд╡рд╛рдкрд░рдгреНрдпрд╛рд╕ рд╕реЛрдкрд╛ ЁЯШК", "рд╡рд┐рд╕реНрддреГрдд рдкрд░реНрдпрд╛рдп ЁЯУж", "рд╕реБрд░рдХреНрд╖рд┐рдд рдкреЗрдореЗрдВрдЯ ЁЯФТ", "рдкрд░рддрд╛рд╡рд╛ рд╕реЛрдкрд╛ ЁЯФД",
    "рдЧреНрд░рд╛рд╣рдХрд╛рдВрд╕рд╛рдареА рдореИрддреНрд░реАрдкреВрд░реНрдг ЁЯдЭ", "рдирд╡реАрдирддрдо рдореЙрдбреЗрд▓ ЁЯЖХ", "рдЙрдЪреНрдЪ рджрд░реНрдЬрд╛рдЪрд╛ тЬи", "рд╕рдорд╛рдзрд╛рдирдХрд╛рд░рдХ ЁЯСНЁЯП╗", "рдЙрддреНрддрдо рджрд░реНрдЬрд╛рдЪрд╛ рдЕрдиреБрднрд╡ ЁЯМЯ",
    "рдпреЛрдЧреНрдп рдХрд┐рдВрдордд ЁЯТ░", "рддрдВрддреНрд░рдЬреНрдЮрд╛рдирд╛рдд рдЖрдШрд╛рдбреАрд╡рд░ ЁЯЦея╕П", "рджрд░реНрдЬреЗрджрд╛рд░ рдмрд╛рдВрдзрдХрд╛рдо ЁЯПЧя╕П", "рдкрд░рдлреЗрдХреНрдЯ рдлрд┐рдЯрд┐рдВрдЧ ЁЯСМ", "рдЬрд▓рдж рд╡рд┐рддрд░рдг ЁЯЪЪ",
    "рдЙрдЪреНрдЪ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ тЪЩя╕П", "рд╕рдВрдкреВрд░реНрдг рд╣рдореА ЁЯЫбя╕П", "рд╕рд╣рдЬ рд╡рд╛рдкрд░рдХрд░реНрддрд╛ рдЗрдВрдЯрд░рдлреЗрд╕ ЁЯЦ▒я╕П", "рдЕрддреНрдпрд╛рдзреБрдирд┐рдХ рдбрд┐рдЭрд╛рдЗрди ЁЯОи", "рд╡рд┐рд╢реНрд╡рд╕рдиреАрдп рдмреНрд░рдБрдб ЁЯМР"
]
negative_words_marathi = [
    "рдЦрд░рд╛рдм ЁЯШЮ", "рдЙрд╢реАрд░ тП░", "рдлрд╕рд╡рдгреВрдХ тЭМ", "рддреБрдЯрд▓реЗрд▓реЗ ЁЯФз", "рддреНрд░рд╛рд╕рджрд╛рдпрдХ ЁЯШб",
    "рдорд╣рд╛рдЧрдбреЗ ЁЯТ╕", "рдЕрд╡рдШрдб ЁЯШХ", "рдирд┐рдХреГрд╖реНрдЯ тЪая╕П", "рдЪреБрдХреАрдЪреЗ ЁЯУЙ", "рдирд┐рд░рд╛рд╢рд╛ ЁЯУ╡",
    "рдЕрдХрд╛рд░реНрдпрдХреНрд╖рдо ЁЯЫС", "рдзреЛрдХрд╛рджрд╛рдпрдХ тШая╕П", "рджреБрд░реНрдмрд▓ ЁЯТФ", "рдлреЗрд▓ тЭМ", "рдЕрдпреЛрдЧреНрдп тЭЧ",
    "рдЕрд╕рдВрддреБрд╖реНрдЯ ЁЯШа", "рдЕрд╡рд┐рд╢реНрд╡рд╕рдиреАрдп ЁЯФУ", "рдХрдореА рджрд░реНрдЬрд╛рдЪрд╛ ЁЯСО", "рд╡рд┐рдХреГрдд ЁЯФД", "рдЕрд╕рд╛рдзрд╛рд░рдг ЁЯРв"
]
negative_phrases_marathi = [
    "рдкрд░рддрд╛рд╡рд╛ рдХрдареАрдг ЁЯФТ", "рд╕реНрдЯреЙрдХ рд╕рдВрдкрд▓реЗрд▓реЗ ЁЯЫС", "рдЫрд╛рдпрд╛рдЪрд┐рддреНрд░ рдЪреБрдХреАрдЪреЗ ЁЯУ╕тЭМ", "рдЪреБрдХреАрдЪрд╛ рдЖрдХрд╛рд░ ЁЯУПтЭМ", "рд╣рд╛рдпрд╢рд┐рдкрд┐рдВрдЧ рд╢реБрд▓реНрдХ ЁЯТ░тЪая╕П",
    "рд╡рд┐рд╕реНрдХрд│реАрдд рдкреНрд░рдХреНрд░рд┐рдпрд╛ ЁЯФДтЭМ", "рдЯреНрд░реЕрдХрд┐рдВрдЧ рдЙрдкрд▓рдмреНрдз рдирд╛рд╣реА ЁЯХ╡я╕ПтАНтЩВя╕ПтЭМ", "рдЖрд╡рд╛рдЬ рдЦреВрдк рдХрдореА ЁЯФЗ", "рдЧреБрдгрд╡рддреНрддрд╛ рдирд┐рдХреГрд╖реНрдЯ ЁЯСО", "рдЕрдпреЛрдЧреНрдп рдлрд┐рдЯрд┐рдВрдЧ ЁЯСХ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдЕрдкреБрд░реА ЁЯУ╡", "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЙрд╢реАрд░ тП│", "рдкреНрд░реЙрдбрдХреНрдЯ рдЦрд░рд╛рдм рдЭрд╛рд▓рд╛ ЁЯФз", "рдХрдореА рдЯрд┐рдХрд╛рдКрдкрдгрд╛ ЁЯХ░я╕П", "рдлрд╕рд╡рдгреВрдХ рдХрд░рдгрд╛рд░рд╛ рд╡рд░реНрдгрди ЁЯУДтЭМ",
    "рд╡рд╛рдкрд░рдгреНрдпрд╛рд╕ рдЕрд╡рдШрдб ЁЯзй", "рдХрдореА рдХрд╛рд░реНрдпрдХреНрд╖рдо тЪая╕П", "рд╡рд╛рдЯрдкрд╛рдЪреА рд╕рдорд╕реНрдпрд╛ ЁЯЪл", "рдЦреВрдк рдХрд┐рдВрдордд ЁЯТ╕", "рд╡рд┐рдХреНрд░реАрдирдВрддрд░ рд╕реЗрд╡рд╛ рдирд╛рд╣реА ЁЯЫС"
]
data = []
def generate_review(prod, type="positive"):  # removed undefined "product" default
    # product = faker.ecommerce_name()
    if type == "positive":
        return f"{prod} is {random.choice(positive_words_marathi)} and {random.choice(positive_phrases_marathi)}"
    else:
        return f"{prod} is {random.choice(negative_words_marathi)} and {random.choice(negative_phrases_marathi)}"
# for _ in range(100):
#   product = random.choice(product_names_marathi)
#   data.append([product, generate_review(product,"positive"),"positive"])
# for _ in range(100):
#   product = random.choice(product_names_marathi)
#   data.append([product, generate_review(product, "negative"),"negative"])
async def generate_positive_data():
    for _ in range(50):
        product = random.choice(product_names_marathi)
        data.append([product, generate_review(product, "positive"), "positive"])
        await asyncio.sleep(2)
async def generate_negative_data():
    for _ in range(50):
        product = random.choice(product_names_marathi)
        data.append([product, generate_review(product, "negative"), "negative"])
        await asyncio.sleep(1)
await asyncio.gather(generate_positive_data(), generate_negative_data())
product_df = pd.DataFrame(data, columns=["Product", "Review", "Sentiment"])
product_df.to_csv("ProductData.csv", index=False)
fp2 = pd.read_csv("ProductData.csv")
fp2

***actual webscrape***
from bs4 import BeautifulSoup
import requests
import pandas as pd
url = "https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India"
header = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
}
response = requests.get(url, headers=header)
response
html_content = BeautifulSoup(response.content, "html.parser")
html_content
table = html_content.find_all("table", {"class":"wikitable sortable"})
table
df = pd.read_html(str(table))
df[0]

***more webscrape***
!pip install bs4
from bs4 import BeautifulSoup
html_code = """
  </head>
  <body>
  <header>
    <h1>Welcome to My Sample Page </h1>
  </header>
  <main>
    <p>This is a basic webpage created with HTML. You can add more content and style it with CSS. </p>
    <p>Check out this <a href="https://www.example.com" target="_blank">Link</a> to visit an external website</p>
  </main>
</body>
</html>
"""
html_content = BeautifulSoup(html_code, 'html.parser')
html_content.text

from bs4 import BeautifulSoup
html_code = """
</head>
<body>
  <header>
    <h1>Welcome to My Sample Page </h1>
  </header>
  <div> this is div1 </div>
  <div> this is div2 </div>
  <p> This is first para </p>
  <p>This is a basic webpage created with HTML. You can add more content and style it with CSS. </p>
  <p>Check out this <a href="https://www.example.com" target="_blank">Link</a> to visit an external website</p>
  <a href="https://www.google.com" target="_blank">Click here</a>
  <p>This is last para</p>
  <span id="product">Samsung s24</span>
  <span class="samsung">Samsung s24</span>
  <span class="samsung">Samsung s24</span>
  <span class="samsung">Samsung s9 fe</span>
</body>
</html>
"""
html_content = BeautifulSoup(html_code, 'html.parser')
type(html_content)
html_content.text
html_content.find("p").text   #finds the first tag appearance
all_paras = html_content.find_all("p")   #gives all para
for p in all_paras:
  print(p.get_text())
html_content.find("a")
html_content.find('a')["href"]
html_content.find('span',{'id':'product'}) #find tag by id
# html_content.find_all('span',{'class':'samsung'})
# html_content.find_all('span', class_='samsung')
html_content.select('span.samsung')
# to select only 1st
html_content.select_one('span.samsung')

import requests
from bs4 import BeautifulSoup
import pandas as pd
all_quotes = []
all_authors = []
all_tags = []
url = "https://quotes.toscrape.com/"
# url ="https://www.amazon.in/s?bbn=81107432031&rh=n%3A81107432031%2Cp_85%3A10440599031&_encoding=UTF8&content-id=amzn1.sym.58c90a12-100b-4a2f-8e15-7c06f1abe2be&pd_rd_r=e20b3f9a-cf23-43c9-9e60-57cb9ff8ae3a&pd_rd_w=Bh4O7&pd_rd_wg=KUr0O&pf_rd_p=58c90a12-100b-4a2f-8e15-7c06f1abe2be&pf_rd_r=TWFY6TGT2J08H42WP3AF&ref=pd_hp_d_atf_unk"
# url = "https://www.somaiya.edu/en/"

# url = "https://www.amazon.in/s?bbn=81107432031&rh=n%3A81107432031%2Cp_85%3A10440599031&_encoding=UTF8&content-id=amzn1.sym.58c90a12-100b-4a2f-8e15-7c06f1abe2be&pd_rd_r=e20b3f9a-cf23-43c9-9e60-57cb9ff8ae3a&pd_rd_w=Bh4O7&pd_rd_wg=KUr0O&pf_rd_p=58c90a12-100b-4a2f-8e15-7c06f1abe2be&pf_rd_r=TWFY6TGT2J08H42WP3AF&ref=pd_hp_d_atf_unk"
header = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
}
response = requests.get(url, headers=header)
response
html_content = BeautifulSoup(response.content, 'html.parser')
html_content
all_spans = html_content.select("span.text")
all_authors_spans = html_content.select("small.author")
for span in all_spans:
  all_quotes.append(span.text)
for span in all_authors_spans:
  all_authors.append(span.text)
quotes_dataset = pd.DataFrame({
    "Quotes" : all_quotes,
    "Authors" : all_authors
})
quotes_dataset

import requests
from bs4 import BeautifulSoup
import pandas as pd
all_quotes = []
all_authors = []
all_tags = []
url = "https://quotes.toscrape.com/"
header = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
}
response = requests.get(url, headers=header)
response
html_content = BeautifulSoup(response.content, 'html.parser')
html_content
all_spans = html_content.select("span.text")
all_authors_spans = html_content.select("small.author")
all_div_tags = html_content.select("div.tags")
all_div_tags
for div in all_div_tags:
  tag_list = " , ".join([a.get_text() for a in div.select("a.tag")])
  all_tags.append(tag_list)
all_tags
for span in all_spans:
  all_quotes.append(span.text)
for span in all_authors_spans:
  all_authors.append(span.text)
quotes_dataset = pd.DataFrame({
    "Quotes" : all_quotes,
    "Authors" : all_authors,
    "Tags" : all_tags
})
quotes_dataset

import requests
from bs4 import BeautifulSoup
import pandas as pd
all_quotes = []
all_authors = []
all_tags = []
header = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
}
for i in range(1,7):
  url = f"https://quotes.toscrape.com/page/{i}/"
  response = requests.get(url, headers=header)
  response
  html_content = BeautifulSoup(response.content, 'html.parser')
  html_content
  all_spans = html_content.select("span.text")
  all_authors_spans = html_content.select("small.author")
  all_div_tags = html_content.select("div.tags")
  all_div_tags
  for div in all_div_tags:
    tag_list = " , ".join([a.get_text() for a in div.select("a.tag")])
    all_tags.append(tag_list)
  all_tags
  for span in all_spans:
    all_quotes.append(span.text)
  for span in all_authors_spans:
    all_authors.append(span.text)
  quotes_dataset = pd.DataFrame({
      "Quotes" : all_quotes,
      "Authors" : all_authors,
      "Tags" : all_tags
  })
quotes_dataset

import requests
from bs4 import BeautifulSoup
import pandas as pd
all_quotes = []
all_authors = []
all_tags = []
header = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
}
def scrap_pages(start_page, end_page):
  for i in range(start_page, end_page):
    url = f"https://quotes.toscrape.com/page/{i}/"
    response = requests.get(url, headers=header)
    response
    html_content = BeautifulSoup(response.content, 'html.parser')
    html_content
    all_spans = html_content.select("span.text")
    all_authors_spans = html_content.select("small.author")
    all_div_tags = html_content.select("div.tags")
    all_div_tags
    for div in all_div_tags:
      tag_list = " , ".join([a.get_text() for a in div.select("a.tag")])
      all_tags.append(tag_list)
    all_tags
    for span in all_spans:
      all_quotes.append(span.text)
    for span in all_authors_spans:
      all_authors.append(span.text)
    quotes_dataset = pd.DataFrame({
        "Quotes" : all_quotes,
        "Authors" : all_authors,
        "Tags" : all_tags
    })
  return quotes_dataset
start = int(input("Enter start page: "))
end = int(input("Enter end page: "))
scrap_pages(start,end)

import requests
from bs4 import BeautifulSoup
import pandas as pd
all_titles = []
all_rates = []
all_movie_desc = []
header = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
}
url = f"https://www.loksatta.com/maharashtra/"
response = requests.get(url, headers=header)
response
html_content = BeautifulSoup(response.content, 'html.parser')
html_content
# all_desc = html_content.find_all("p", class_="article_excerpt")
# all_desc
def scrap_pages(start_page, end_page):
    all_titles = []
    all_dates = []
    all_article_desc = []
    for i in range(start_page, end_page + 1):
      url = f"https://www.loksatta.com/maharashtra/page/{i}/"
      response = requests.get(url, headers=header)
      html_content = BeautifulSoup(response.content, 'html.parser')
      all_title_names = html_content.select("a.article_title")
      all_dates_given = html_content.select("span.article_date")
      all_desc = html_content.select("p.article_excerpt")
      for title in all_title_names:
          all_titles.append(title.text.strip())
      for date in all_dates_given:
          all_dates.append(date.text.strip())
      for desc in all_desc:
          all_article_desc.append(desc.text.strip())
      # for desc in all_desc:
      #     desc_list = " , ".join([d.get_text().strip() for d in desc.select("p")])
      #     all_article_desc.append(desc_list)
    quotes_dataset = pd.DataFrame({
        "Title": pd.Series(all_titles),
        "Date": pd.Series(all_dates),
        "Description": pd.Series(all_article_desc)
    })
    return quotes_dataset
start = int(input("Enter start page: "))
end = int(input("Enter end page: "))
scrap_pages(start,end)

***Whatsapp Scraping***
# reading file
with open("/content/WhatsApp Chat with Family katta.txt","r",encoding="utf-8") as file:
  data = file.readlines()
data

import re
import string
import pandas as pd
timestamps = []
messages = []
senders = []
# here / , is literal string
# date : (\d{1,2}/\d{1,2}/\d{2})
# {} range
# [either or]
# matches only for single line
pattern = r"^(\d{2}/\d{2}/\d{2}), (\d{2}:\d{2}) - ([^:]+): (.*)"
for chat in data:
  chat = chat.strip()
  # print(chat)
  match = re.match(pattern,chat)
  if match:
    date, time,sender, message = match.groups()
    # print("timestamp: " + date + "\ntime: "+time + "\nsender: " + sender + "\nmessage: " + message)
    timestamps.append(f"{date} {time}")
    senders.append(sender)
    messages.append(message)
  else:
    if messages:
      messages[-1] += " " + chat;
    # print("not matches")
df = pd.DataFrame({
    "timestamps" : timestamps,
    "senders" : senders,
    "messages" : messages
});
df.tail(10)

import matplotlib.pyplot as plt
import seaborn as sns
df["senders"].value_counts().plot(kind="bar")

***speech to text***
!pip install SpeechRecognition
!pip install pydub

from pydub import AudioSegment
import speech_recognition as sr
import os
from pickle import TRUE
from genericpath import exists
recogniser = sr.Recognizer()
# source = AudioSegment.from_mp3("/content/talking-46836.mp3")
source = AudioSegment.from_wav("/content/harvard.wav")
# source = AudioSegment.from_mp3("/content/marathi.mp3")
# source.export("/content/talking-46836.wav", format="wav")
with sr.AudioFile("/content/harvard.wav") as source:
  audio_source = recogniser.record(source)
  print("recognising......")
  try:
    # speech_text = recogniser.recognize_google(audio_source, language="mr-IN")
    speech_text = recogniser.recognize_google(audio_source)
    print("converted text: \n")
    print(speech_text)
  except:
    print("sorry could not recognize")

***video to text(English)***
from moviepy.editor import VideoFileClip
video = VideoFileClip("/content/Raman Ramachandran _ Careers .mp4")
video
audio_from_video = video.audio.write_audiofile("raman_ramchandran.mp3")
audio_from_video

from pydub import AudioSegment
import speech_recognition as sr
import os
recogniser = sr.Recognizer()
source = AudioSegment.from_mp3("/content/raman_ramchandran.mp3")
source.export("/content/raman_ramchandran.wav", format="wav")
with sr.AudioFile("/content/raman_ramchandran.wav") as source:
  audio_source = recogniser.record(source)
  print("recognizing........")
  try:
    speech_text = recogniser.recognize_google(audio_source)
    print("converted text: \n")
    print(speech_text)
  except:
    print("sorry could not recognize")

***video to text(Marathi)***
from moviepy.editor import VideoFileClip
video = VideoFileClip("/content/marathi_video.mp4")
video
audio_from_video = video.audio.write_audiofile("marathi_video.mp3")
audio_from_video

from pydub import AudioSegment
import speech_recognition as sr
import os
recogniser = sr.Recognizer()
source = AudioSegment.from_mp3("/content/marathi_video.mp3")
source.export("/content/marathi_video.wav", format="wav")
with sr.AudioFile("/content/marathi_video.wav") as source:
  audio_source = recogniser.record(source)
  print("recognizing........")
  try:
    speech_text = recogniser.recognize_google(audio_source, language="mr-IN")
    print("converted text: \n")
    print(speech_text)
  except:
    print("sorry could not recognize")

***Extract text from image***
!pip install pillow
!pip install pytesseract
!sudo apt install tesseract-ocr
!sudo apt install tesseract-dev
!sudo apt install tesseract-ocr-mar
!sudo apt install tesseract-ocr-hin
from PIL import Image
import pytesseract
image=Image.open("/content/hindi_poem.jpeg")
poem_text=pytesseract.image_to_string(image,lang="hin")
print(poem_text)

