=============================================================================
ANN (Artificial Neural Network) - GENERIC TEMPLATE
=============================================================================
Use Case: Binary/Multi-class Classification, Regression
Works with: ANY tabular/CSV dataset
=============================================================================

============================================================================
STEP 1: IMPORT LIBRARIES
============================================================================
import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam, SGD
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

============================================================================
STEP 2: LOAD AND EXPLORE DATASET
============================================================================
# Load data (choose one method)
# Method 1: From CSV file
df = pd.read_csv('your_dataset.csv')

# Method 2: From URL
# url = 'https://example.com/dataset.csv'
# df = pd.read_csv(url)

# Method 3: From Excel
# df = pd.read_excel('your_dataset.xlsx')

# Explore the data
print("First 5 rows:")
print(df.head())
print("\nDataset shape:", df.shape)
print("\nData types and info:")
print(df.info())
print("\nMissing values:")
print(df.isna().sum())
print("\nDuplicate rows:", df.duplicated().sum())
print("\nBasic statistics:")
print(df.describe())

============================================================================
STEP 3: PREPARE DATA (X and y)
============================================================================
# Method 1: Drop target column (if you know column name)
X = df.drop('target_column_name', axis=1)  # Replace with your target column
y = df['target_column_name']

# Method 2: By column position (if target is last column)
# X = df.iloc[:, :-1]  # All columns except last
# y = df.iloc[:, -1]   # Last column

# Method 3: By column index range
# X = df.iloc[:, 0:8]  # Columns 0-7 (8 features)
# y = df.iloc[:, 8]    # Column 8 (target)

# Method 4: Select specific columns
# X = df[['feature1', 'feature2', 'feature3']]
# y = df['target']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Number of features: {X.shape[1]}")
print(f"Number of samples: {X.shape[0]}")

============================================================================
STEP 4: ENCODING (If needed for categorical data)
============================================================================
# Check unique values in target
print("\nTarget unique values:", y.unique())
print("Target value counts:")
print(y.value_counts())

# Option 1: Encode target variable (if categorical)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_encoded = le.fit_transform(y)
print(f"\nOriginal classes: {le.classes_}")
print(f"Encoded range: {y_encoded.min()} to {y_encoded.max()}")

# Use y_encoded for further steps if you encoded
y = y_encoded

# Option 2: One-hot encode target (for categorical_crossentropy loss)
# from keras.utils import to_categorical
# y = to_categorical(y)

# Option 3: Encode categorical features
# X = pd.get_dummies(X, drop_first=True)

# Option 4: Manual encoding for specific columns
# X['column_name'] = X['column_name'].map({'cat1': 0, 'cat2': 1})

============================================================================
STEP 5: STANDARDIZATION (Feature Scaling)
============================================================================
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Alternative: Min-Max Scaling (0-1 range)
# from sklearn.preprocessing import MinMaxScaler
# scaler = MinMaxScaler()
# X_scaled = scaler.fit_transform(X)

print(f"\nScaled data shape: {X_scaled.shape}")
print(f"Scaled data range: [{X_scaled.min():.2f}, {X_scaled.max():.2f}]")

============================================================================
STEP 6: TRAIN-TEST SPLIT
============================================================================
from sklearn.model_selection import train_test_split

# Common split ratios: 0.2 (80-20), 0.3 (70-30), 0.25 (75-25)
TEST_SIZE = 0.3  # Adjust as needed
RANDOM_STATE = 42  # For reproducibility

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

print(f"\nTraining samples: {X_train.shape[0]}")
print(f"Testing samples: {X_test.shape[0]}")
print(f"Features: {X_train.shape[1]}")

============================================================================
STEP 7: BUILD ANN MODEL
============================================================================
# Determine number of features and classes
num_features = X_train.shape[1]
num_classes = len(np.unique(y))  # For classification

print(f"\nBuilding model with {num_features} input features")
print(f"Number of classes: {num_classes}")

model = Sequential()

# Input layer + First hidden layer
# Neurons typically: 64, 128, 256 (powers of 2)
model.add(Dense(64, activation='relu', input_shape=(num_features,)))

# Hidden layers (decrease neurons each layer)
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))

# Optional: Add Dropout to prevent overfitting
# from keras.layers import Dropout
# model.add(Dropout(0.2))  # Drop 20% of neurons

# Output layer - CHOOSE ONE based on your task:

# For BINARY classification (2 classes):
model.add(Dense(1, activation='sigmoid'))

# For MULTI-CLASS classification (3+ classes):
# model.add(Dense(num_classes, activation='softmax'))
# OR
# model.add(Dense(num_classes, activation='sigmoid'))

# For REGRESSION (predicting continuous values):
# model.add(Dense(1, activation='linear'))

# View model architecture
model.summary()

============================================================================
STEP 8: COMPILE MODEL
============================================================================
# Choose optimizer and loss based on your task

# OPTIMIZER OPTIONS:
# 1. Adam (most common, adaptive learning rate)
from keras.optimizers import Adam
optimizer = Adam(learning_rate=0.01)  # Try: 0.001, 0.01, 0.1

# 2. SGD (Stochastic Gradient Descent)
# from tensorflow.keras.optimizers import SGD
# optimizer = SGD(learning_rate=0.01, momentum=0.9)

# 3. Simple string (uses defaults)
# optimizer = 'adam'

# LOSS FUNCTION - CHOOSE ONE:
# Binary classification (2 classes):
loss = 'binary_crossentropy'

# Multi-class (integer labels like 0,1,2):
# loss = 'sparse_categorical_crossentropy'

# Multi-class (one-hot encoded labels):
# loss = 'categorical_crossentropy'

# Regression:
# loss = 'mse'  # or 'mae', 'huber'

# METRICS:
metrics = ['accuracy']  # For classification
# metrics = ['mae']  # For regression

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

print(f"\nModel compiled with:")
print(f"  Optimizer: {optimizer if isinstance(optimizer, str) else type(optimizer).__name__}")
print(f"  Loss: {loss}")
print(f"  Metrics: {metrics}")

============================================================================
STEP 9: TRAIN THE MODEL
============================================================================
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=10,
    verbose=1
)

# With validation split
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=10,
    verbose=1
)

============================================================================
STEP 10: EVALUATE THE MODEL
============================================================================
loss, accuracy = model.evaluate(X_test, y_test)
print("\nLoss: %.2f, and Accuracy : %.2f%%" % (loss, accuracy*100))

============================================================================
STEP 11: MAKE PREDICTIONS
============================================================================
# Get predictions (probabilities)
predictions = model.predict(X_test)

# Convert to class labels
if predictions.shape[1] == 1 or len(predictions.shape) == 1:
    # Binary classification
    y_pred = (predictions > 0.5).astype(int).flatten()
else:
    # Multi-class classification  
    y_pred = np.argmax(predictions, axis=1)

print("\nSample Predictions (first 10):")
for i in range(min(10, len(y_pred))):
    correct = "âœ“" if y_pred[i] == y_test[i] else "âœ—"
    print(f"{correct} Predicted: {y_pred[i]}, Actual: {y_test[i]}")

# Calculate accuracy manually
accuracy_manual = np.mean(y_pred == y_test)
print(f"\nManual Accuracy: {accuracy_manual:.4f} ({accuracy_manual*100:.2f}%)")

============================================================================
STEP 12: VISUALIZATIONS
============================================================================

# Plot 1: Training History (Accuracy and Loss)
plt.figure(figsize=(12, 4))

# Accuracy subplot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Model Accuracy Over Epochs', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

# Loss subplot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', linewidth=2)
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Model Loss Over Epochs', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Plot 2: Confusion Matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.title('Confusion Matrix', fontsize=14)
plt.tight_layout()
plt.show()

# Plot 3: Classification Report
from sklearn.metrics import classification_report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

============================================================================
STEP 13: SAVE AND LOAD MODEL (Optional)
============================================================================
# Save the trained model
model.save('my_ann_model.keras')  # New format (recommended)
print("\nModel saved!")

# Save the scaler (IMPORTANT)
import joblib
joblib.dump(scaler, 'scaler.pkl')
print("Scaler saved!")

# Load later:
# from keras.models import load_model
# loaded_model = load_model('my_ann_model.keras')
# loaded_scaler = joblib.load('scaler.pkl')

# Make predictions:
# scaled_new_data = loaded_scaler.transform(new_data)
# predictions = loaded_model.predict(scaled_new_data)

============================================================================
QUICK REFERENCE GUIDE
============================================================================

ðŸ“Œ WHEN TO USE ANN:
   - Tabular/CSV data (structured data)
   - Classification: Binary or Multi-class
   - Regression: Predicting continuous values
   - NOT for images (use CNN) or text sequences (use RNN)

ðŸ“Œ COMPLETE WORKFLOW:
   1. Load data â†’ 2. Explore â†’ 3. Prepare X,y â†’ 4. Encode (if needed)
   5. Scale â†’ 6. Split â†’ 7. Build model â†’ 8. Compile â†’ 9. Train
   10. Evaluate â†’ 11. Predict â†’ 12. Visualize

ðŸ“Œ ACTIVATION FUNCTIONS:
   - Hidden layers: 'relu' (most common), 'tanh', 'elu'
   - Binary output: 'sigmoid'
   - Multi-class output: 'softmax' (recommended), 'sigmoid'
   - Regression output: 'linear' or None

ðŸ“Œ LOSS FUNCTIONS:
   Binary: 'binary_crossentropy'
   Multi-class (integer labels): 'sparse_categorical_crossentropy'
   Multi-class (one-hot): 'categorical_crossentropy'
   Regression: 'mse', 'mae', 'huber'

ðŸ“Œ OPTIMIZERS & LEARNING RATES:
   Adam(learning_rate=0.001)  # Default, good starting point
   Adam(learning_rate=0.01)   # Faster learning
   SGD(learning_rate=0.01, momentum=0.9)
   SGD(learning_rate=0.001, momentum=0.9)
   Common LR values: 0.1, 0.01, 0.001, 0.0001

ðŸ“Œ TYPICAL ARCHITECTURE:
   Input(num_features) â†’ Dense(64, relu) â†’ Dense(32, relu) â†’ 
   Dense(16, relu) â†’ Output(num_classes, activation)
   
   Pattern: Decrease neurons by half each layer
   Common neurons: 512, 256, 128, 64, 32, 16, 8

ðŸ“Œ HYPERPARAMETERS TO ADJUST:
   Epochs: 50, 100, 200, 300, 500
   Batch size: 10, 16, 32, 64, 128
   Test size: 0.2 (80-20), 0.3 (70-30), 0.25 (75-25)
   Validation split: 0.2 (20% of training data)

ðŸ“Œ PREPROCESSING CHECKLIST:
   âœ“ Check for missing values (df.isna().sum())
   âœ“ Check for duplicates (df.duplicated().sum())
   âœ“ Encode categorical target (LabelEncoder)
   âœ“ Encode categorical features (get_dummies or LabelEncoder)
   âœ“ Scale features (StandardScaler or MinMaxScaler)
   âœ“ Split data (train_test_split)

ðŸ“Œ COMMON ISSUES & SOLUTIONS:
   Problem: Low accuracy
   - Add more layers/neurons
   - Train for more epochs
   - Try different learning rate
   - Check if data is properly scaled
   
   Problem: Overfitting (train acc >> test acc)
   - Add Dropout layers
   - Reduce model complexity
   - Get more training data
   - Use regularization (L1, L2)
   
   Problem: Underfitting (both low)
   - Add more layers/neurons
   - Train longer
   - Reduce regularization

ðŸ“Œ EVALUATION METRICS:
   Classification: accuracy, precision, recall, F1-score
   Regression: MSE, MAE, RÂ² score
   Use confusion matrix for classification

ðŸ“Œ IMPORTANT NOTES:
   - Always scale your data before training!
   - Save the scaler with the model
   - Use same scaler for new predictions
   - validation_split uses % of training data
   - Set random_state for reproducibility
   - Monitor both training and validation metrics

ðŸ“Œ PREDICTION ON NEW DATA:
   1. Load model and scaler
   2. Scale new data using saved scaler
   3. Predict using loaded model
   4. Convert probabilities to classes if needed
   - Classification: accuracy, loss
   - Confusion matrix for detailed analysis

ðŸ“Œ PREPROCESSING STEPS:
   1. Check shape, info, null values, duplicates
   2. Separate X and y
   3. Encode categorical variables (LabelEncoder)
   4. Scale features (StandardScaler)
   5. Train-test split (test_size=0.2 or 0.3)

ðŸ“Œ YOUR CODING PATTERN:
   model = Sequential()
   model.add(Dense(neurons, activation='relu', input_shape=(features,)))
   model.add(Dense(neurons, activation='relu'))
   model.add(Dense(output_neurons, activation='sigmoid/softmax/linear'))
   
   model.compile(optimizer=optimizer, loss='loss_function', metrics=['accuracy'])
   model.fit(X_train, y_train, epochs=100, batch_size=10)
   
   loss, accuracy = model.evaluate(X_test, y_test)
   print("\nLoss: %.2f, and Accuracy : %.2f%%" % (loss, accuracy*100))



=============================================================================
CNN (Convolutional Neural Network) - GENERIC TEMPLATE
=============================================================================
Use Case: Image Classification with Spatial Feature Extraction
Works with: ANY image dataset
=============================================================================

============================================================================
STEP 1: IMPORT LIBRARIES
============================================================================
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint

============================================================================
STEP 2: LOAD DATASET
============================================================================
# Option 1: Load from Keras built-in datasets
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
# Other options: mnist, cifar10, cifar100

# Option 2: Load from directory
# from keras.preprocessing.image import ImageDataGenerator
# datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
# train_data = datagen.flow_from_directory('path/to/train', target_size=(height, width))

# Option 3: Load custom numpy arrays
# x_train = np.load('train_images.npy')
# y_train = np.load('train_labels.npy')
# x_test = np.load('test_images.npy')
# y_test = np.load('test_labels.npy')

print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
print(f"Image dimensions: {x_train.shape[1]}x{x_train.shape[2]}")
if len(x_train.shape) == 4:
    print(f"Channels: {x_train.shape[3]}")

============================================================================
STEP 3: DEFINE CLASS LABELS
============================================================================
# Option 1: Manually define your class names
class_labels = ['Class_0', 'Class_1', 'Class_2']  # Replace with your classes

# Option 2: Auto-generate based on number of classes
num_classes = len(np.unique(y_train))
class_labels = [f'Class_{i}' for i in range(num_classes)]

# Option 3: For specific datasets (examples)
# Fashion MNIST:
# class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
#                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# CIFAR-10:
# class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer',
#                 'dog', 'frog', 'horse', 'ship', 'truck']

print(f"Number of classes: {len(class_labels)}")
print(f"Class labels: {class_labels}")

============================================================================
STEP 4: VISUALIZE SAMPLE IMAGE
============================================================================
img_index = 0  # Change to any index
label_index = y_train[img_index]
print(f"Sample {img_index}: y = {label_index} ({class_labels[label_index]})")

plt.figure(figsize=(6, 6))
plt.imshow(x_train[img_index], cmap='gray')  # Use cmap='gray' for grayscale
plt.colorbar()
plt.title(f'Label: {class_labels[label_index]}')
plt.grid(False)
plt.show()

============================================================================
STEP 5: DATA NORMALIZATION
============================================================================
print(f"Original data range: [{x_train.min()}, {x_train.max()}]")

# Normalize to 0-1 range if needed
if x_train.max() > 1:
    print("Normalizing data...")
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0
else:
    print("Data already normalized")
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')

print(f"Normalized data range: [{x_train.min():.2f}, {x_train.max():.2f}]")
print(f"Number of train samples: {len(x_train)}")
print(f"Number of test samples: {len(x_test)}")

============================================================================
STEP 6: SPLIT TRAIN/VALIDATION SETS (Optional)
============================================================================
# Option 1: Manual split
VALIDATION_SIZE = 5000  # Adjust based on dataset size

(x_train, x_valid) = x_train[VALIDATION_SIZE:], x_train[:VALIDATION_SIZE]
(y_train, y_valid) = y_train[VALIDATION_SIZE:], y_train[:VALIDATION_SIZE]

print(f"Training samples: {len(x_train)}")
print(f"Validation samples: {len(x_valid)}")
print(f"Test samples: {len(x_test)}")

# Option 2: Use validation_split in model.fit() instead (skip this step)
# validation_split=0.2 will automatically use 20% of training data

============================================================================
STEP 7: RESHAPE DATA FOR CNN
============================================================================
# CNNs need shape: (samples, height, width, channels)
# Grayscale images: channels = 1
# RGB images: channels = 3

# Check current shape
print(f"Current x_train shape: {x_train.shape}")

# Reshape if needed (for grayscale images)
if len(x_train.shape) == 3:
    # Add channel dimension
    w, h = x_train.shape[1], x_train.shape[2]
    x_train = x_train.reshape(x_train.shape[0], w, h, 1)
    x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)
    x_test = x_test.reshape(x_test.shape[0], w, h, 1)
    print(f"Reshaped to: {x_train.shape}")
else:
    print("Data already has channel dimension")

# For RGB images (already have 3 channels), no reshaping needed
img_height, img_width, img_channels = x_train.shape[1], x_train.shape[2], x_train.shape[3]

============================================================================
STEP 8: ONE-HOT ENCODE LABELS
============================================================================
# Check unique labels
num_classes = len(np.unique(y_train))
print(f"Number of classes: {num_classes}")
print(f"Unique labels: {np.unique(y_train)}")

# One-hot encode labels (for categorical_crossentropy loss)
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

# Alternative: Keep as integers (for sparse_categorical_crossentropy loss)
# Skip this step if using sparse_categorical_crossentropy

# Print shapes
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
print(f"{x_train.shape[0]} training samples")
print(f"{x_valid.shape[0]} validation samples")
print(f"{x_test.shape[0]} test samples")

============================================================================
STEP 9: BUILD CNN MODEL ARCHITECTURE
============================================================================
# Auto-detect parameters from data
input_shape = (img_height, img_width, img_channels)
num_classes = y_train.shape[1] if len(y_train.shape) > 1 else len(np.unique(y_train))

print(f"Building CNN with:")
print(f"  Input shape: {input_shape}")
print(f"  Number of classes: {num_classes}")

model = tf.keras.Sequential()

# First Convolutional Block
# Filters: 32, 64, 128 (increase for complex images)
model.add(tf.keras.layers.Conv2D(
    filters=64,  # Try: 32, 64, 128
    kernel_size=2,  # Try: 2, 3, 5
    padding='same', 
    activation='relu', 
    input_shape=input_shape
))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
model.add(tf.keras.layers.Dropout(0.3))  # 0.2-0.5 to prevent overfitting

# Second Convolutional Block (optional, for deeper networks)
model.add(tf.keras.layers.Conv2D(
    filters=32,  # Usually decrease from previous
    kernel_size=2, 
    padding='same', 
    activation='relu'
))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
model.add(tf.keras.layers.Dropout(0.3))

# Add more Conv blocks for complex datasets:
# model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2, padding='same', activation='relu'))
# model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
# model.add(tf.keras.layers.Dropout(0.3))

# Flatten and Dense Layers
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(256, activation='relu'))  # Try: 128, 256, 512
model.add(tf.keras.layers.Dropout(0.5))  # Higher dropout for dense layers
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

# Display model summary
model.summary()

============================================================================
STEP 10: COMPILE THE MODEL
============================================================================
# Use categorical_crossentropy for one-hot encoded labels
# Use sparse_categorical_crossentropy for integer labels

model.compile(
    loss='categorical_crossentropy',  # For one-hot encoded
    # loss='sparse_categorical_crossentropy',  # For integer labels
    optimizer='adam',  # Or: Adam(learning_rate=0.001)
    metrics=['accuracy']
)

print("\nModel compiled successfully!")

============================================================================
STEP 11: SETUP CALLBACKS (Optional but Recommended)
============================================================================
from keras.callbacks import ModelCheckpoint, EarlyStopping

# Save best model during training
checkpointer = ModelCheckpoint(
    filepath='model.weights.best.keras',
    verbose=1,
    save_best_only=True
)

# Stop training if no improvement
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,  # Stop after 10 epochs without improvement
    restore_best_weights=True
)

# Use callbacks in fit(): callbacks=[checkpointer, early_stop]

============================================================================
STEP 12: TRAIN THE MODEL
============================================================================
EPOCHS = 10  # Adjust based on dataset (10-50 typical)
BATCH_SIZE = 64  # Common: 32, 64, 128

print(f"\nTraining for {EPOCHS} epochs with batch size {BATCH_SIZE}...")

history = model.fit(
    x_train,
    y_train,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(x_valid, y_valid),
    callbacks=[checkpointer],  # Add early_stop if using
    verbose=1
)

# Alternative: Use validation_split instead of separate validation set
# history = model.fit(
#     x_train, y_train,
#     batch_size=BATCH_SIZE,
#     epochs=EPOCHS,
#     validation_split=0.2,
#     callbacks=[checkpointer],
#     verbose=1
# )

print("\nTraining completed!")

============================================================================
STEP 13: LOAD BEST WEIGHTS (If using ModelCheckpoint)
============================================================================
model.load_weights('model.weights.best.keras')
print("Best weights loaded!")

============================================================================
STEP 14: EVALUATE ON TEST SET
============================================================================
score = model.evaluate(x_test, y_test, verbose=0)

print("\n" + "="*50)
print("MODEL EVALUATION RESULTS")
print("="*50)
print(f"Test Loss: {score[0]:.4f}")
print(f"Test Accuracy: {score[1]:.4f} ({score[1]*100:.2f}%)")
print("="*50)

# Plot training history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

============================================================================
STEP 15: MAKE PREDICTIONS
============================================================================
y_hat = model.predict(x_test)

# Get predicted classes
y_pred = np.argmax(y_hat, axis=1)
y_true = np.argmax(y_test, axis=1)

print("\nSample predictions (first 10):")
for i in range(min(10, len(y_pred))):
    correct = "âœ“" if y_pred[i] == y_true[i] else "âœ—"
    confidence = y_hat[i][y_pred[i]]
    print(f"{correct} Sample {i}: Predicted={class_labels[y_pred[i]]}, "
          f"Actual={class_labels[y_true[i]]}, Confidence={confidence:.2%}")

# Calculate metrics
from sklearn.metrics import classification_report, confusion_matrix

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=class_labels))

print("\nConfusion Matrix:")
print(confusion_matrix(y_true, y_pred))

============================================================================
STEP 16: VISUALIZE PREDICTIONS
============================================================================
# Select random samples to visualize
num_samples = 15
random_indices = np.random.choice(x_test.shape[0], size=num_samples, replace=False)

figure = plt.figure(figsize=(20, 8))
for i, index in enumerate(random_indices):
    ax = figure.add_subplot(3, 5, i+1, xticks=[], yticks=[])
    
    # Display image
    ax.imshow(np.squeeze(x_test[index]), cmap='gray')
    
    predict_index = np.argmax(y_hat[index])
    true_index = np.argmax(y_test[index])
    
    # Set title - Green if correct, Red if wrong
    ax.set_title(
        "{} ({})\n{:.1f}%".format(
            class_labels[predict_index],
            class_labels[true_index],
            100 * np.max(y_hat[index])
        ),
        color=("green" if predict_index == true_index else "red")
    )

plt.tight_layout()
plt.show()

# Confusion Matrix Heatmap
import seaborn as sns
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix Heatmap')
plt.tight_layout()
plt.show()

============================================================================
QUICK REFERENCE GUIDE
============================================================================

ðŸ“Œ WHEN TO USE CNN:
   - Image classification (spatial patterns matter)
   - Object detection
   - Image segmentation
   - Any 2D grid-like data
   - Better than DNN for images (preserves spatial relationships)

ðŸ“Œ CNN vs DNN:
   - CNN: Preserves 2D structure, learns spatial features
   - DNN: Flattens image immediately, treats pixels independently
   - CNN typically gives better accuracy for images

ðŸ“Œ CNN ARCHITECTURE PATTERN:
   Conv2D â†’ MaxPooling2D â†’ Dropout â†’ 
   Conv2D â†’ MaxPooling2D â†’ Dropout â†’ 
   Flatten â†’ Dense â†’ Dropout â†’ Dense(output)

ðŸ“Œ KEY LAYER PARAMETERS:

   Conv2D:
   - filters: 32, 64, 128 (increase for complex images)
   - kernel_size: 2, 3, 5 (3 most common)
   - padding: 'same' (keeps size) or 'valid' (reduces size)
   - activation: 'relu'
   - input_shape: (height, width, channels) - only first layer

   MaxPooling2D:
   - pool_size: 2 (most common, reduces dimension by half)
   - Reduces spatial dimensions, keeps important features

   Dropout:
   - Rate: 0.2-0.3 (after Conv/Pool), 0.5 (after Dense)
   - Prevents overfitting by randomly dropping neurons

ðŸ“Œ DATA SHAPE REQUIREMENTS:
   Input: (num_samples, height, width, channels)
   - Grayscale: (n, h, w, 1)
   - RGB: (n, h, w, 3)
   - Must reshape if shape is (n, h, w)

ðŸ“Œ PREPROCESSING STEPS:
   1. Load data
   2. Normalize: x / 255.0 (if 0-255 range)
   3. Reshape: Add channel dimension if needed
   4. One-hot encode labels (for categorical_crossentropy)
   5. Split train/validation/test

ðŸ“Œ LOSS FUNCTIONS:
   - categorical_crossentropy: For one-hot encoded labels
   - sparse_categorical_crossentropy: For integer labels
   - binary_crossentropy: For binary classification

ðŸ“Œ TYPICAL FILTER PROGRESSION:
   64 â†’ 32 â†’ Dense(256) â†’ Dense(num_classes)
   OR
   32 â†’ 64 â†’ 128 â†’ Dense(512) â†’ Dense(num_classes)
   (Can increase or decrease filters each layer)

ðŸ“Œ COMMON HYPERPARAMETERS:
   - Epochs: 10-50 (CNNs train slower than ANNs)
   - Batch size: 32, 64, 128
   - Validation split: 0.2
   - Learning rate: 0.001 (Adam default)

ðŸ“Œ CALLBACKS:
   ModelCheckpoint:
   - Saves best model during training
   - save_best_only=True (keeps only best weights)
   
   EarlyStopping:
   - Stops if no improvement
   - patience=10 (wait 10 epochs)
   - restore_best_weights=True

ðŸ“Œ PREDICTION OUTPUT:
   - model.predict() returns probabilities
   - np.argmax(predictions, axis=1): Get predicted classes
   - np.max(predictions, axis=1): Get confidence scores

ðŸ“Œ VISUALIZATION TIPS:
   - Green = Correct prediction
   - Red = Wrong prediction
   - Show format: "Predicted (Actual) Confidence%"
   - Use confusion matrix heatmap for overall performance

ðŸ“Œ IMPROVING MODEL:
   - More Conv layers: Deeper network, learns complex features
   - More filters: Wider network, learns more patterns
   - Larger kernel_size: Captures larger features
   - More Dense neurons: Better feature combination
   - Add Dropout: Reduce overfitting
   - Data augmentation: Artificially increase dataset size

ðŸ“Œ DATASET-SPECIFIC SETTINGS:
   
   Fashion-MNIST / MNIST (28x28 grayscale):
   - Input: (28, 28, 1)
   - Filters: 64â†’32
   - Dense: 256
   
   CIFAR-10/100 (32x32 RGB):
   - Input: (32, 32, 3)
   - Filters: 32â†’64â†’128
   - Dense: 512
   
   Custom high-res images:
   - Resize to manageable size (64x64, 128x128, 224x224)
   - More Conv layers needed
   - More filters (128, 256, 512)

ðŸ“Œ DEBUGGING TIPS:
   - Check input shape matches model's input_shape
   - Verify labels are one-hot encoded (if using categorical_crossentropy)
   - Ensure data is normalized (0-1 range)
   - Check num_classes matches output layer neurons
   - Monitor val_loss: if increasing, model is overfitting

ðŸ“Œ MEMORY OPTIMIZATION:
   - Reduce batch_size if GPU memory error
   - Use smaller input images
   - Reduce number of filters
   - Use model.fit() with steps_per_epoch for large datasets

ðŸ“Œ SAVING/LOADING:
   model.save('cnn_model.keras')
   loaded_model = load_model('cnn_model.keras')
   
   # For predictions on new images:
   # 1. Load and resize image
   # 2. Normalize (/ 255.0)
   # 3. Reshape to (1, height, width, channels)
   # 4. Predict: model.predict(image)



=============================================================================
DNN (Deep Neural Network) - GENERIC TEMPLATE
=============================================================================
Use Case: Image Classification, Complex Pattern Recognition
Works with: ANY image dataset
=============================================================================

============================================================================
STEP 1: IMPORT LIBRARIES
============================================================================
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

============================================================================
STEP 2: LOAD DATASET
============================================================================

# Option A: Load from Keras built-in datasets
# dataset = keras.datasets.fashion_mnist  # or mnist, cifar10, cifar100
# (train_image, train_labels), (test_image, test_labels) = dataset.load_data()

# Option B: Load custom images from directory
# from keras.preprocessing.image import ImageDataGenerator
# train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
# train_data = train_datagen.flow_from_directory('path/to/train', target_size=(height, width))

# Option C: Load from numpy arrays
# train_image = np.load('train_images.npy')
# train_labels = np.load('train_labels.npy')
# test_image = np.load('test_images.npy')
# test_labels = np.load('test_labels.npy')

============================================================================
STEP 3: EXPLORE THE DATASET
============================================================================
print("Train image shape:", train_image.shape)  # e.g., (60000, 28, 28)
print("Train labels shape:", train_labels.shape)  # e.g., (60000,)
print("Test image shape:", test_image.shape)
print("Test labels shape:", test_labels.shape)

# Print number of samples
print(train_image.shape[0], 'train set')
print(test_image.shape[0], 'test set')

# Check data range and type
print("Min pixel value:", train_image.min())
print("Max pixel value:", train_image.max())
print("Data type:", train_image.dtype)

============================================================================
STEP 4: DEFINE CLASS NAMES (Optional - for visualization)
============================================================================
# Replace with your actual class names
class_names = ['Class_0', 'Class_1', 'Class_2', 'Class_3', 'Class_4',
               'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']

# Or automatically generate based on number of classes
# num_classes = len(np.unique(train_labels))
# class_names = [f'Class_{i}' for i in range(num_classes)]

============================================================================
STEP 5: VISUALIZE SAMPLE IMAGE
============================================================================
img_index = 0  # Change to any index
label_index = train_labels[img_index]
print(f"Sample {img_index}: Label = {label_index} ({class_names[label_index]})")

plt.figure()
plt.imshow(train_image[img_index], cmap='gray')  # Use cmap='gray' for grayscale
plt.colorbar()
plt.grid(False)
plt.title(f'Label: {class_names[label_index]}')
plt.show()

============================================================================
STEP 6: DATA NORMALIZATION
============================================================================
# Check if data needs normalization
if train_image.max() > 1:
    print("Normalizing data from 0-255 to 0-1...")
    train_image = train_image / 255.0
    test_image = test_image / 255.0
else:
    print("Data already normalized")

# Alternative explicit conversion:
# train_image = train_image.astype('float32') / 255
# test_image = test_image.astype('float32') / 255

============================================================================
STEP 7: DISPLAY MULTIPLE IMAGES (Quality Check)
============================================================================
num_images = min(25, len(train_image))  # Show up to 25 images
grid_size = int(np.sqrt(num_images))

plt.figure(figsize=(10, 10))
for i in range(num_images):
    plt.subplot(grid_size, grid_size, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_image[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

============================================================================
STEP 8: BUILD DNN MODEL
============================================================================
# Get image dimensions and number of classes
img_height, img_width = train_image.shape[1], train_image.shape[2]
num_classes = len(np.unique(train_labels))

print(f"Image dimensions: {img_height}x{img_width}")
print(f"Number of classes: {num_classes}")

model = keras.Sequential([
    # Flatten layer: Convert 2D image to 1D array
    keras.layers.Flatten(input_shape=(img_height, img_width)),
    
    # Hidden layers - adjust neurons based on your dataset
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(64, activation=tf.nn.relu),
    keras.layers.Dense(32, activation=tf.nn.relu),
    
    # Optional: Add Dropout to prevent overfitting
    # keras.layers.Dropout(0.2),
    
    # Output layer - neurons = number of classes
    keras.layers.Dense(num_classes, activation=tf.nn.softmax),
])

# View model architecture
model.summary()

============================================================================
STEP 9: COMPILE THE MODEL
============================================================================
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

============================================================================
STEP 10: TRAIN THE MODEL
============================================================================

# Choose epochs and batch size based on your dataset
EPOCHS = 10  # Start with 10, increase if needed
BATCH_SIZE = 32  # Common values: 16, 32, 64

# Option 1: Simple training
history = model.fit(train_image, train_labels, 
                    epochs=EPOCHS, 
                    batch_size=BATCH_SIZE,
                    validation_split=0.2,  # Use 20% for validation
                    verbose=1)

# Option 2: Incremental training (if needed)
# Start with fewer epochs, then continue
# model.fit(train_image, train_labels, epochs=10)
# model.fit(train_image, train_labels, epochs=20)  # Continues from epoch 10

# Option 3: With validation data
# model.fit(train_image, train_labels, 
#           epochs=EPOCHS,
#           validation_data=(test_image, test_labels))

============================================================================
STEP 11: EVALUATE THE MODEL
============================================================================
test_loss, test_acc = model.evaluate(test_image, test_labels, verbose=0)
print("\n" + "="*50)
print(f"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"Test Loss: {test_loss:.4f}")
print("="*50)

# Plot training history
plt.figure(figsize=(12, 4))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.grid(True)

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.grid(True)

plt.tight_layout()
plt.show()

============================================================================
STEP 12: MAKE PREDICTIONS
============================================================================
predictions = model.predict(test_image)

# View prediction for any image
sample_idx = 0  # Change to any index
print(f"\nPrediction probabilities for image {sample_idx}:")
print(predictions[sample_idx])

# Get the class with highest probability
predicted_class = np.argmax(predictions[sample_idx])
confidence = np.max(predictions[sample_idx])
actual_class = test_labels[sample_idx]

print(f"\nPredicted class: {predicted_class} ({class_names[predicted_class]})")
print(f"Confidence: {confidence*100:.2f}%")
print(f"Actual class: {actual_class} ({class_names[actual_class]})")
print(f"Correct: {predicted_class == actual_class}")

============================================================================
STEP 13: VISUALIZATION FUNCTIONS
============================================================================

# Function to plot image with prediction
def plot_image(i, predictions_array, true_labels, images):
    predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img, cmap=plt.cm.binary)
    
    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'green'
    else:
        color = 'red'
    
    plt.xlabel("{} {:2.0f}% ({})".format(
        class_names[predicted_label],
        100 * np.max(predictions_array),
        class_names[true_label]
    ), color=color)

# Function to plot prediction confidence bars
def plot_value_array(i, predictions_array, true_label):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    num_classes = len(predictions_array)
    plt.xticks(range(num_classes), range(num_classes), rotation=45)
    plt.yticks([])
    thisplot = plt.bar(range(num_classes), predictions_array, color='#777777')
    plt.ylim([0, 1])
    predicted_label = np.argmax(predictions_array)
    
    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('green')

============================================================================
STEP 14: VISUALIZE PREDICTIONS
============================================================================

# Single prediction visualization
i = 0  # Change to any index
plt.figure(figsize=(8, 3))
plt.subplot(1, 2, 1)
plot_image(i, predictions, test_labels, test_image)
plt.subplot(1, 2, 2)
plot_value_array(i, predictions, test_labels)
plt.tight_layout()
plt.show()

# Visualize multiple predictions (grid)
num_rows = 5
num_cols = 3
num_images = num_rows * num_cols

plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
    plt.subplot(num_rows, 2*num_cols, 2*i+1)
    plot_image(i, predictions, test_labels, test_image)
    plt.subplot(num_rows, 2*num_cols, 2*i+2)
    plot_value_array(i, predictions, test_labels)
plt.tight_layout()
plt.show()

# Calculate and display accuracy
correct_predictions = np.sum(np.argmax(predictions, axis=1) == test_labels)
total_predictions = len(test_labels)
accuracy = correct_predictions / total_predictions
print(f"\nCorrect predictions: {correct_predictions}/{total_predictions}")
print(f"Accuracy: {accuracy*100:.2f}%")

============================================================================
QUICK REFERENCE GUIDE
============================================================================

ðŸ“Œ DNN vs ANN:
   - DNN: For images, complex patterns (uses Flatten layer)
   - ANN: For tabular data (uses Dense layers directly)

ðŸ“Œ TYPICAL DNN ARCHITECTURE:
   Flatten(height, width) â†’ Dense(128, relu) â†’ Dense(64, relu) â†’ 
   Dense(32, relu) â†’ Dense(num_classes, softmax)

ðŸ“Œ KEY PARAMETERS TO ADJUST:
   - img_height, img_width: Your image dimensions
   - num_classes: Number of categories in your dataset
   - EPOCHS: 10 (small dataset) to 100+ (large dataset)
   - BATCH_SIZE: 16, 32, 64, 128
   - neurons: 128â†’64â†’32 (decrease by half each layer)

ðŸ“Œ KEY LAYERS:
   - Flatten: Converts 2D/3D data to 1D
   - Dense: Fully connected layer
   - Dropout: Prevents overfitting (use 0.2-0.5)
   - Activation: relu (hidden), softmax (output multi-class)

ðŸ“Œ ACTIVATION FUNCTIONS:
   - Hidden layers: 'relu' (most common)
   - Output (multi-class): 'softmax'
   - Output (binary): 'sigmoid'

ðŸ“Œ LOSS FUNCTIONS:
   - Multi-class (integer labels): 'sparse_categorical_crossentropy'
   - Multi-class (one-hot): 'categorical_crossentropy'
   - Binary: 'binary_crossentropy'
   - Regression: 'mse'

ðŸ“Œ OPTIMIZER OPTIONS:
   - 'adam' (most common, adaptive learning rate)
   - 'sgd', 'rmsprop', 'adamax'
   - Adam(learning_rate=0.001)  # Custom learning rate

ðŸ“Œ DATA PREPROCESSING CHECKLIST:
   1. Load dataset
   2. Check shape and dimensions
   3. Normalize: image / 255.0 (if 0-255 range)
   4. Define class names
   5. Verify data types (float32 recommended)

ðŸ“Œ PREDICTION INTERPRETATION:
   - predictions[i]: Array of probabilities for each class
   - np.argmax(predictions[i]): Predicted class index
   - np.max(predictions[i]): Confidence score (0-1)

ðŸ“Œ VISUALIZATION TIPS:
   - Use plt.imshow() for images
   - cmap='gray' for grayscale images
   - Green label = Correct prediction
   - Red label = Wrong prediction
   - Bar chart shows confidence for all classes

ðŸ“Œ COMMON DATASET SHAPES:
   - MNIST/Fashion-MNIST: (28, 28) grayscale
   - CIFAR-10/100: (32, 32, 3) RGB
   - Custom: (height, width) or (height, width, channels)

ðŸ“Œ WHEN TO USE VALIDATION:
   - validation_split=0.2: Use 20% of training data
   - validation_data=(x_val, y_val): Separate validation set
   - Helps detect overfitting during training

ðŸ“Œ IMPROVING MODEL PERFORMANCE:
   1. Add more layers (deeper network)
   2. Add more neurons per layer (wider network)
   3. Add Dropout layers (reduce overfitting)
   4. Increase epochs (more training)
   5. Adjust learning rate
   6. Use data augmentation (for images)
   7. Try different optimizers

ðŸ“Œ DEBUGGING TIPS:
   - If accuracy stuck: Check learning rate, add/remove layers
   - If overfitting: Add Dropout, reduce complexity, get more data
   - If underfitting: Add layers, increase neurons, train longer
   - Check data shape: Must match input_shape in model



=============================================================================
EXPLAINABLE AI (XAI) - GENERIC TEMPLATE
=============================================================================
Use Case: Understanding Model Predictions, Feature Importance, LIME
Works with: ANY tabular classification/regression model
=============================================================================

============================================================================
STEP 1: IMPORT LIBRARIES
============================================================================
import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from lime import lime_tabular
from lime.lime_tabular import LimeTabularExplainer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Install LIME if needed
# !pip install lime

============================================================================
STEP 2: LOAD AND EXPLORE DATASET
============================================================================
# Load data
data = pd.read_csv('your_dataset.csv')
df = pd.DataFrame(data)

# Explore data
print(df.info())
print(df.describe())
print(df.isnull().sum())
print(df.duplicated().sum())
print(df.shape)

============================================================================
STEP 3: PREPARE FEATURES AND TARGET
============================================================================
# Separate features and target
X = df.drop('target_column', axis=1)
y = df['target_column']

# Check unique values in target
print(y.unique())

============================================================================
STEP 4: ENCODE TARGET (For Multi-class)
============================================================================
# One-hot encode the target variable
y_encoded = pd.get_dummies(y)
y_encoded = pd.DataFrame(y_encoded)

print(y_encoded.head())

============================================================================
STEP 5: TRAIN-TEST SPLIT
============================================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, 
    test_size=0.2, 
    random_state=42
)

============================================================================
STEP 6: FEATURE SCALING
============================================================================
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

============================================================================
STEP 7: BUILD AND TRAIN MODEL
============================================================================
model = Sequential([
    Dense(11, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(6, activation='softmax')  # 6 classes for wine quality (3-8)
])

# For binary classification:
# model = Sequential([
#     Dense(11, activation='relu', input_shape=(X_train.shape[1],)),
#     Dense(32, activation='relu'),
#     Dense(1, activation='sigmoid')
# ])

============================================================================
STEP 8: COMPILE MODEL
============================================================================
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # multi-class
    # loss='binary_crossentropy',  # binary
    metrics=['accuracy']
)

============================================================================
STEP 9: TRAIN MODEL
============================================================================
model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=16,
    validation_split=0.2,
    verbose=1
)

============================================================================
STEP 10: EVALUATE MODEL
============================================================================
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

============================================================================
STEP 11: CREATE LIME EXPLAINER
============================================================================
explainer = LimeTabularExplainer(
    X_train,
    feature_names=X.columns,  # Original feature names
    class_names=y.unique(),   # Target class names
    mode='classification'     # 'classification' or 'regression'
)

============================================================================
STEP 12: EXPLAIN A SINGLE INSTANCE
============================================================================
# Select instance to explain
i = 0  # Index of test sample
instance = X_test[i]

# Generate explanation
exp = explainer.explain_instance(
    instance,
    model.predict,
    num_features=10  # Show top 10 features
)

============================================================================
STEP 13: VISUALIZE EXPLANATION
============================================================================
# Plot the explanation
fig = exp.as_pyplot_figure()
plt.title(f'LIME Explanation for Instance {i}')
plt.tight_layout()
plt.show()

============================================================================
STEP 14: PRINT EXPLANATION DETAILS
============================================================================
# Print feature contributions
print(f"\n{'='*60}")
print(f"LIME Explanation for Instance {i}")
print(f"{'='*60}")

# Get prediction
prediction = model.predict(instance.reshape(1, -1))
predicted_class = np.argmax(prediction)
print(f"\nPredicted Class: {predicted_class}")
print(f"Prediction Probabilities: {prediction[0]}")

# Get feature importance
print("\nFeature Contributions:")
for feature, weight in exp.as_list():
    print(f"{feature}: {weight:.4f}")

============================================================================
STEP 15: SAVE EXPLANATION AS HTML
============================================================================
exp.save_to_file('lime_explanation.html')
print("\nâœ… LIME explanation saved to 'lime_explanation.html'")

============================================================================
STEP 16: EXPLAIN MULTIPLE INSTANCES
============================================================================
# Explain first 5 test instances
num_instances = 5

for i in range(num_instances):
    instance = X_test[i]
    exp = explainer.explain_instance(
        instance,
        model.predict,
        num_features=5
    )
    
    print(f"\n{'='*60}")
    print(f"Instance {i}")
    print(f"{'='*60}")
    
    prediction = model.predict(instance.reshape(1, -1))
    predicted_class = np.argmax(prediction)
    print(f"Predicted Class: {predicted_class}")
    
    print("Top 5 Feature Contributions:")
    for feature, weight in exp.as_list():
        print(f"  {feature}: {weight:.4f}")

============================================================================
STEP 17: FEATURE IMPORTANCE VISUALIZATION
============================================================================
# Aggregate feature importance across multiple instances
feature_importance = {}
num_samples = 10

for i in range(min(num_samples, len(X_test))):
    instance = X_test[i]
    exp = explainer.explain_instance(
        instance,
        model.predict,
        num_features=len(X.columns)
    )
    
    for feature, weight in exp.as_list():
        feature_name = feature.split()[0]  # Extract feature name
        if feature_name not in feature_importance:
            feature_importance[feature_name] = []
        feature_importance[feature_name].append(abs(weight))

# Calculate average importance
avg_importance = {k: np.mean(v) for k, v in feature_importance.items()}

# Sort by importance
sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)

# Plot
plt.figure(figsize=(10, 6))
features = [f[0] for f in sorted_features[:10]]
importance = [f[1] for f in sorted_features[:10]]

plt.barh(features, importance)
plt.xlabel('Average Absolute Weight')
plt.ylabel('Feature')
plt.title('Top 10 Feature Importance (LIME)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

============================================================================
QUICK REFERENCE GUIDE
============================================================================

ðŸ“Œ WHAT IS LIME?
   - Local Interpretable Model-agnostic Explanations
   - Explains individual predictions
   - Works with any model (model-agnostic)
   - Creates local linear approximation

ðŸ“Œ LIME WORKFLOW:
   1. Train your model
   2. Create LimeTabularExplainer
   3. Explain specific instances
   4. Visualize and analyze

ðŸ“Œ LimeTabularExplainer PARAMETERS:
   - training_data: Scaled training data (X_train)
   - feature_names: Column names from original data
   - class_names: Target class labels
   - mode: 'classification' or 'regression'

ðŸ“Œ explain_instance PARAMETERS:
   - data_row: Single instance to explain
   - predict_fn: model.predict function
   - num_features: Number of features to show

ðŸ“Œ PREPROCESSING FOR LIME:
   1. Scale features (StandardScaler)
   2. Keep original column names
   3. Use scaled data for explainer
   4. Use original feature names for display

ðŸ“Œ VISUALIZATION OPTIONS:
   - exp.as_pyplot_figure(): Matplotlib plot
   - exp.show_in_notebook(): Jupyter display
   - exp.save_to_file('file.html'): Save as HTML

ðŸ“Œ INTERPRETATION:
   - Positive weight: Feature increases prediction
   - Negative weight: Feature decreases prediction
   - Larger magnitude: More important feature
   - Shows feature value ranges

ðŸ“Œ COMMON USE CASES:
   - Wine quality prediction
   - Credit scoring
   - Medical diagnosis
   - Fraud detection
   - Customer churn

ðŸ“Œ ACTIVATION & LOSS FOR MULTI-CLASS:
   - Hidden layers: 'relu'
   - Output: 'softmax'
   - Loss: 'categorical_crossentropy'
   - One-hot encode target: pd.get_dummies()

ðŸ“Œ YOUR CODING PATTERN:
   1. Load data â†’ df.info(), describe(), isnull()
   2. X = features, y = target
   3. One-hot encode: pd.get_dummies(y)
   4. Split: train_test_split
   5. Scale: StandardScaler
   6. Build model: Sequential with Dense layers
   7. Compile: adam, categorical_crossentropy
   8. Fit: epochs=10, batch_size=16
   9. LIME: Create explainer with X_train
   10. Explain: explain_instance with model.predict

ðŸ“Œ FEATURE NAMES:
   - Use original column names: X.columns
   - Before scaling, not after
   - Helps with interpretation

ðŸ“Œ TYPICAL ARCHITECTURE:
   Dense(11, relu) â†’ Dense(32, relu) â†’ Dense(classes, softmax)
   
   Input neurons match number of features
   Hidden layer larger than input
   Output matches number of classes

ðŸ“Œ BATCH SIZE & EPOCHS:
   - batch_size: 16 (smaller for LIME models)
   - epochs: 10, 20, 50
   - validation_split: 0.2

ðŸ“Œ LIME vs SHAP:
   - LIME: Faster, local explanations
   - SHAP: More accurate, global + local
   - LIME easier to implement
   - Both model-agnostic

ðŸ“Œ TIPS:
   - Explain multiple instances for patterns
   - Save explanations as HTML for reports
   - Use num_features to control detail
   - Aggregate explanations for overall importance
   - Green bars = positive contribution
   - Orange/Red bars = negative contribution

ðŸ“Œ DEBUGGING:
   - Ensure X_train is scaled
   - feature_names must match columns
   - class_names should be strings/labels
   - predict function should return probabilities


=============================================================================
DEEP LEARNING - GENERIC TEMPLATES INDEX & QUICK GUIDE
=============================================================================
All templates work with ANY dataset - just plug in your data!
Location: /home/shaun/Downloads/pro/DeeplearningLAB/CheatSheets_TXT/
=============================================================================

============================================================================
ðŸ“š AVAILABLE CHEAT SHEETS
============================================================================

1. ANN_CheatSheet.txt
   âœ… Artificial Neural Network - GENERIC TEMPLATE
   - Use for: ANY tabular/CSV dataset (classification/regression)
   - Examples: Customer churn, price prediction, disease diagnosis
   - Key: Automatically detects features, scales data, handles any # of classes
   - Works with: CSV, Excel, pandas DataFrames

2. DNN_CheatSheet.txt
   âœ… Deep Neural Network - GENERIC TEMPLATE
   - Use for: ANY image dataset (flattened approach)
   - Examples: MNIST, Fashion-MNIST, CIFAR, custom images
   - Key: Auto-detects image dimensions and # of classes
   - Works with: Keras datasets, numpy arrays, image directories

3. CNN_CheatSheet.txt
   âœ… Convolutional Neural Network - GENERIC TEMPLATE
   - Use for: ANY image dataset (spatial feature extraction)
   - Examples: Object recognition, medical imaging, satellite images
   - Key: Auto-configures input shape and output classes
   - Works with: Grayscale (28x28), RGB (32x32, 224x224), custom sizes

4. RNN_CheatSheet.txt
   âœ… Recurrent Neural Network - GENERIC TEMPLATE
   - Use for: ANY text dataset (classification/generation)
   - Examples: Sentiment analysis, spam detection, text generation
   - Key: Generic tokenization, handles any vocabulary size
   - Works with: CSV text data, .txt files, pandas DataFrames

5. ExplainableAI_CheatSheet.txt
   âœ… LIME Explainability - GENERIC TEMPLATE
   - Use for: Explaining ANY ML model predictions
   - Examples: Feature importance for any classification/regression
   - Key: Model-agnostic, works with tabular data
   - Works with: Any trained model + original dataset

============================================================================
ðŸŽ¯ QUICK DECISION TREE - WHICH MODEL TO USE?
============================================================================

YOUR DATA TYPE â†’ RECOMMENDED MODEL

ðŸ“Š Tabular/CSV Data (numbers, categories, features in columns)
   â”œâ”€ Classification/Regression â†’ ANN_CheatSheet.txt
   â””â”€ Want to explain predictions â†’ ExplainableAI_CheatSheet.txt

ðŸ–¼ï¸ Image Data (photos, pictures, visual data)
   â”œâ”€ Simple classification â†’ DNN_CheatSheet.txt
   â””â”€ Complex patterns/features â†’ CNN_CheatSheet.txt (RECOMMENDED)

ðŸ“ Text Data (sentences, reviews, documents)
   â”œâ”€ Sentiment/Classification â†’ RNN_CheatSheet.txt (Part 1)
   â””â”€ Text generation/Next word â†’ RNN_CheatSheet.txt (Part 2)

============================================================================
ðŸ”§ UNIVERSAL WORKFLOW FOR ANY DATASET
============================================================================

ALL TEMPLATES FOLLOW THIS PATTERN:

1. LOAD YOUR DATA
   - CSV: pd.read_csv('file.csv')
   - Images: keras.datasets or from directory
   - Text: From CSV or text files

2. AUTO-DETECT PARAMETERS
   âœ… Templates automatically detect:
   - Number of features
   - Number of classes
   - Image dimensions
   - Input/output shapes

3. PREPROCESS
   - Normalize/Scale (automatic checks)
   - Encode labels if needed
   - Reshape for model type

4. BUILD MODEL
   - Auto-configured architecture
   - Adjust neurons/filters as needed

5. TRAIN
   - Set EPOCHS and BATCH_SIZE
   - Monitor validation metrics

6. EVALUATE & PREDICT
   - Automatic metrics calculation
   - Visualization included

============================================================================
ðŸ“Œ KEY CHANGES - ALL TEMPLATES ARE NOW GENERIC!
============================================================================

âœ… NO hardcoded dataset names
âœ… NO hardcoded dimensions (28x28, etc.)
âœ… NO hardcoded number of classes
âœ… AUTO-DETECTION of all parameters
âœ… FLEXIBLE data loading (CSV, images, text)
âœ… WORKS with ANY dataset size
âœ… INCLUDES multiple loading methods

YOU ONLY NEED TO:
1. Load your data
2. (Optional) Define class names
3. Set EPOCHS and BATCH_SIZE
4. Run!

============================================================================
ðŸ“– COMMON PARAMETERS FROM YOUR CODE
============================================================================

OPTIMIZERS YOU USE:
âœ… Adam(learning_rate=0.01)
âœ… SGD(learning_rate=0.01, momentum=0.9)
âœ… SGD(learning_rate=0.001, momentum=0.9)
âœ… SGD(learning_rate=0.0001, momentum=0.9)
âœ… SGD(learning_rate=0.0005, momentum=0.9)

TYPICAL ARCHITECTURES:
âœ… ANN: Dense(64) â†’ Dense(32) â†’ Dense(16) â†’ Output
âœ… DNN: Flatten â†’ Dense(128) â†’ Dense(64) â†’ Dense(32) â†’ Output
âœ… CNN: Conv2D(64) â†’ MaxPool â†’ Conv2D(32) â†’ MaxPool â†’ Flatten â†’ Dense(256) â†’ Output
âœ… RNN: Embedding â†’ SimpleRNN(100) â†’ Dense(vocab_size)

ACTIVATION FUNCTIONS:
âœ… Hidden layers: 'relu', sometimes 'tanh'
âœ… Binary output: 'sigmoid'
âœ… Multi-class output: 'softmax' or 'sigmoid'
âœ… Regression: 'linear'

LOSS FUNCTIONS:
âœ… Binary: 'binary_crossentropy'
âœ… Multi-class (int): 'sparse_categorical_crossentropy'
âœ… Multi-class (one-hot): 'categorical_crossentropy'
âœ… Regression: 'mse'

TRAINING PARAMETERS:
âœ… Epochs: 10, 100, 300, 500
âœ… Batch size: 10, 16, 32, 64
âœ… Test size: 0.2, 0.3
âœ… Validation split: 0.2

============================================================================
ðŸ’¡ EXAM PREPARATION TIPS
============================================================================

ðŸŽ¯ WHAT TO MEMORIZE:

1. IMPORT STATEMENTS
   from keras.models import Sequential
   from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Embedding, SimpleRNN
   from sklearn.preprocessing import StandardScaler, LabelEncoder
   from sklearn.model_selection import train_test_split

2. DATA PREPROCESSING SEQUENCE
   Load â†’ Check (shape, info, nulls) â†’ Split X,y â†’ Encode â†’ Scale â†’ Train-test split

3. MODEL BUILDING PATTERN
   Sequential() â†’ Add layers â†’ compile() â†’ fit() â†’ evaluate() â†’ predict()

4. LAYER CONFIGURATIONS
   - Input shape: (features,) for Dense, (height, width, channels) for Conv2D
   - Hidden neurons: Powers of 2 (64, 32, 16) or multiples (128, 256)
   - Output neurons: 1 for binary, num_classes for multi-class

5. COMMON MISTAKES TO AVOID
   âœ— Forgetting to scale data
   âœ— Wrong activation on output layer
   âœ— Mismatch between loss function and output
   âœ— Not reshaping images for CNN
   âœ— Forgetting to one-hot encode for categorical_crossentropy

6. VISUALIZATION CODE
   - Training history: plt.plot(history.history['accuracy'])
   - Confusion matrix: confusion_matrix(y_test, y_pred)
   - Images: plt.imshow(image), plt.colorbar()

============================================================================
ðŸš€ QUICK START TEMPLATES
============================================================================

MINIMAL CODE FOR EACH TYPE:

ðŸ”¹ ANN (Tabular Classification):

model = Sequential([
    Dense(64, activation='relu', input_shape=(features,)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=100, batch_size=10)

ðŸ”¹ DNN (Image Classification):

model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=10)

ðŸ”¹ CNN (Image with Convolution):

model = Sequential([
    Conv2D(64, kernel_size=2, activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(pool_size=2),
    Flatten(),
    Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

ðŸ”¹ RNN (Text Classification):

model = Sequential([
    Embedding(vocab_size, 10, input_length=maxlen),
    SimpleRNN(16),
    Dense(3, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

============================================================================
ðŸ“ EXAM DAY CHECKLIST
============================================================================

âœ… Know which model for which data type
âœ… Remember preprocessing steps (scale, encode, split)
âœ… Know layer types: Dense, Conv2D, MaxPooling2D, Flatten, Embedding, SimpleRNN
âœ… Activation functions: relu, sigmoid, softmax, linear
âœ… Loss functions: binary_crossentropy, sparse_categorical_crossentropy, mse
âœ… Optimizer syntax: Adam(learning_rate=0.01), SGD(learning_rate=0.01, momentum=0.9)
âœ… How to reshape data for CNN: (samples, height, width, channels)
âœ… Tokenization for RNN: Tokenizer â†’ fit_on_texts â†’ texts_to_sequences â†’ pad_sequences
âœ… Evaluation: model.evaluate(X_test, y_test)
âœ… Prediction: model.predict(X_test)
âœ… Visualization: plt.plot(history.history['accuracy'])

ðŸŽ¯ GOOD LUCK ON YOUR EXAM! ðŸŽ¯

============================================================================
All cheat sheets ready!
Location: /home/shaun/Downloads/pro/DeeplearningLAB/CheatSheets/

âœ… Files created:
   1. ANN_CheatSheet.py
   2. DNN_CheatSheet.py
   3. CNN_CheatSheet.py
   4. RNN_CheatSheet.py
   5. ExplainableAI_CheatSheet.py
   6. INDEX_QuickGuide.py (this file)



=============================================================================
RNN (Recurrent Neural Network) - GENERIC TEMPLATE
=============================================================================
Use Case: Text Classification, Sentiment Analysis, Next Word Prediction
Works with: ANY text dataset
=============================================================================

============================================================================
PART 1: TEXT CLASSIFICATION / SENTIMENT ANALYSIS
============================================================================

============================================================================
STEP 1: IMPORT LIBRARIES
============================================================================
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM
from sklearn.model_selection import train_test_split

============================================================================
STEP 2: PREPARE TEXT DATA
============================================================================
# Load your text data (choose one method)

# Method 1: Manual list (small datasets)
reviews = [
    "Your first text sample here",
    "Your second text sample here",
    "Your third text sample here",
    # Add more samples...
]
labels = [0, 1, 2]  # Corresponding labels

# Method 2: Load from CSV
# import pandas as pd
# df = pd.read_csv('your_dataset.csv')
# reviews = df['text_column'].tolist()
# labels = df['label_column'].tolist()

# Method 3: Load from text files
# with open('data.txt', 'r') as f:
#     reviews = f.readlines()

print(f"Total samples: {len(reviews)}")
print(f"Sample text: {reviews[0][:100]}...")  # First 100 chars

============================================================================
STEP 3: TOKENIZATION
============================================================================
tokenizer = Tokenizer()
tokenizer.fit_on_texts(reviews)
sequences = tokenizer.texts_to_sequences(reviews)

# Check vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary size: {vocab_size}")

============================================================================
STEP 4: PADDING SEQUENCES
============================================================================
maxlen = max(len(x) for x in sequences)
X = pad_sequences(sequences, maxlen=maxlen, padding='post')
y = np.array(labels)

# Ensure X and y have same length
min_len = min(len(X), len(y))
X = X[:min_len]
y = y[:min_len]

============================================================================
STEP 5: TRAIN-TEST SPLIT
============================================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

============================================================================
STEP 6: BUILD RNN MODEL
============================================================================
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=maxlen))
model.add(SimpleRNN(16))

# For multi-class classification (3 classes)
model.add(Dense(3, activation='softmax'))

# For binary classification
# model.add(Dense(1, activation='sigmoid'))

model.summary()

============================================================================
STEP 7: COMPILE MODEL
============================================================================
# For multi-class
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# For binary
# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

============================================================================
STEP 8: TRAIN MODEL
============================================================================
model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    verbose=1,
    validation_data=(X_test, y_test)
)

============================================================================
STEP 9: PREDICT SENTIMENT
============================================================================
def predict_sentiment(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=maxlen, padding='post')
    pred = model.predict(padded)[0]
    
    # Get class with highest probability
    predicted_class = np.argmax(pred)
    confidence = np.max(pred)
    
    # Map numeric class to label
    sentiment_map = {0: "Negative ðŸ˜¡", 1: "Neutral ðŸ˜", 2: "Positive ðŸ˜Š"}
    sentiment = sentiment_map[predicted_class]
    
    print(f"\nReview: {text}")
    print(f"Predicted Sentiment: {sentiment} (confidence: {confidence:.2f})")

# Test prediction
text = input("Enter review: ")
predict_sentiment(text)

============================================================================
PART 2: NEXT WORD PREDICTION
============================================================================

============================================================================
STEP 1: PREPARE CORPUS
============================================================================
data = [
    "I am IronMan, the protector of Earth.",
    "I am BatMan, the silent guardian of Gotham City.",
    "I am Groot, a tree-like creature who believes in friendship.",
    "I want to travel the world and experience different cultures.",
    "I love to eat spicy Indian food.",
    "I usually wake up at six thirty in the morning.",
    "I will start learning advanced Python programming next month.",
]

============================================================================
STEP 2: TOKENIZATION
============================================================================
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data)
total_words = len(tokenizer.word_index) + 1

print(f"Total words: {total_words}")

============================================================================
STEP 3: CREATE INPUT SEQUENCES (N-GRAMS)
============================================================================
input_sequences = []
for line in data:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

============================================================================
STEP 4: PAD SEQUENCES
============================================================================
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(
    input_sequences, 
    maxlen=max_sequence_len, 
    padding='pre'
))

print(f"Max sequence length: {max_sequence_len}")

============================================================================
STEP 5: SPLIT FEATURES AND LABELS
============================================================================
X = input_sequences[:, :-1]
y = input_sequences[:, -1]

# One-hot encode labels
y = np.eye(total_words)[y]

============================================================================
STEP 6: BUILD MODEL FOR NEXT WORD PREDICTION
============================================================================
model = Sequential()
model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))
model.add(SimpleRNN(100))
model.add(Dense(total_words, activation='softmax'))

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()

============================================================================
STEP 7: TRAIN MODEL
============================================================================
model.fit(X, y, epochs=200, verbose=1)

============================================================================
STEP 8: PREDICT NEXT WORD (SIMPLE VERSION)
============================================================================
def predict_next_word_simple(send_text):
    token_list = tokenizer.texts_to_sequences([send_text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
    predicted = np.argmax(model.predict(token_list), axis=1)
    
    for word, index in tokenizer.word_index.items():
        if index in predicted:
            return word

# Test
print(predict_next_word_simple("I am"))

============================================================================
STEP 9: PREDICT NEXT WORD (ADVANCED VERSION)
============================================================================
def predict_next_word(send_text, top_n=5):
    # Preprocess input
    send_text_lower = send_text.lower().strip()
    token_list = tokenizer.texts_to_sequences([send_text_lower])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
    
    # Get prediction probabilities
    predicted_probs = model.predict(token_list, verbose=0)[0]
    
    # Get top N predicted word indices
    top_indices = np.argsort(predicted_probs)[::-1][:top_n]
    
    # Map indices to words
    index_to_word = {index: word for word, index in tokenizer.word_index.items()}
    
    top_words = []
    for i in top_indices:
        word = index_to_word.get(i)
        prob = predicted_probs[i]
        if word:
            top_words.append((word, prob))
    
    # Print results
    print(f"\nInput: {send_text}")
    print(f"Predicted next word: **{top_words[0][0]}** (prob={top_words[0][1]:.4f})")
    print("Other possible next words:")
    for w, p in top_words[1:]:
        print(f"  - {w} ({p:.4f})")
    
    return {
        "predicted": top_words[0][0],
        "suggestions": [{"word": w, "prob": float(p)} for w, p in top_words]
    }

# Test advanced prediction
text = input("Enter words: ")
print(predict_next_word(text))

============================================================================
QUICK REFERENCE GUIDE
============================================================================

ðŸ“Œ RNN LAYERS:
   - SimpleRNN: Basic RNN (good for learning)
   - LSTM: Better for long sequences (use instead of SimpleRNN)
   - GRU: Faster alternative to LSTM

ðŸ“Œ TYPICAL RNN ARCHITECTURE:
   
   Text Classification:
   Embedding â†’ SimpleRNN/LSTM â†’ Dense(classes, softmax)
   
   Next Word Prediction:
   Embedding â†’ SimpleRNN/LSTM â†’ Dense(vocab_size, softmax)

ðŸ“Œ EMBEDDING LAYER:
   - Converts words to dense vectors
   - Parameters: (vocab_size, embedding_dim, input_length)
   - embedding_dim: 10, 50, 100, 128, 300
   - Must be first layer for text data

ðŸ“Œ TOKENIZATION STEPS:
   1. Create Tokenizer()
   2. fit_on_texts(texts) - Build vocabulary
   3. texts_to_sequences(texts) - Convert to numbers
   4. pad_sequences() - Make all same length

ðŸ“Œ PADDING:
   - padding='post': Add zeros at end
   - padding='pre': Add zeros at beginning
   - maxlen: Maximum sequence length
   - Use 'pre' for next word prediction

ðŸ“Œ LOSS FUNCTIONS:
   - Binary classification: 'binary_crossentropy'
   - Multi-class (integer): 'sparse_categorical_crossentropy'
   - Multi-class (one-hot): 'categorical_crossentropy'

ðŸ“Œ LABELS:
   - Text classification: Use integer labels (0, 1, 2)
   - Next word prediction: One-hot encode with np.eye()

ðŸ“Œ COMMON PARAMETERS:
   - vocab_size = len(tokenizer.word_index) + 1
   - maxlen = max(len(x) for x in sequences)
   - epochs: 50, 100, 200
   - batch_size: 32, 64

ðŸ“Œ PREDICTION INTERPRETATION:
   - model.predict() returns probabilities
   - np.argmax(): Get class with highest probability
   - Use sentiment_map to convert numbers to labels

ðŸ“Œ N-GRAM SEQUENCES:
   For "I am Groot":
   - "I" â†’ predicts "am"
   - "I am" â†’ predicts "Groot"
   Creates training pairs from text

ðŸ“Œ YOUR CODING PATTERN:
   
   Sentiment Analysis:
   1. Prepare text data with labels
   2. Tokenize and pad
   3. Embedding â†’ RNN â†’ Dense
   4. sparse_categorical_crossentropy loss
   5. Predict with sentiment mapping
   
   Next Word:
   1. Create n-gram sequences
   2. Pad with 'pre'
   3. One-hot encode output
   4. Embedding â†’ RNN â†’ Dense(vocab_size)
   5. categorical_crossentropy loss

ðŸ“Œ VOCABULARY:
   - tokenizer.word_index: Dictionary of wordâ†’index
   - Add 1 for vocab_size (index 0 reserved)
   - Use index_to_word for reverse mapping

ðŸ“Œ TIPS:
   - More training data = better predictions
   - Larger embedding_dim = more parameters
   - LSTM usually better than SimpleRNN
   - Use validation_data to monitor overfitting
