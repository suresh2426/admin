***Preprocessing Text***
text = """
ЁЯФе MUMBAI MERI JAN ЁЯТЦ <b>Mumbai</b> (/m╩Кm╦Иba╔к/ muum-BY; Marathi: <i>Mumba─л</i>, pronounced [╦Иmumb╔Щi] тУШ), also known as <u>Bombay</u> (/b╔Тm╦Иbe╔к/ bom-BAY; its official name until 1995 ЁЯШЕ ЁЯСЙ), is the capital city of the Indian state of Maharashtra ЁЯМП.
Visit: https://en.wikipedia.org/wiki/Mumbai OR http://mumbai-life.com for more!!! ЁЯОЙ

Mumbai = the financial capital & the most populous city proper of India ЁЯШОЁЯТ░ with an estimated population of 12.5 million (1.25 crore)!!! [20] ЁЯУК.
Mumbai is the centre of the "Mumbai Metropolitan Region", which is among the most populous metropolitan areas in the world ЁЯМН with a population of over 23,000,000 (2.3 crore) ЁЯдпЁЯШ▓ [21].
Mumbai lies on the Konkan coast ЁЯМК on the west coast of India and has a deep natural harbour тЪУ.
In 2008 ЁЯШЬ, Mumbai was named an <a href="https://alpha-world-cities.com">alpha world city</a> ЁЯШНЁЯФе [22][23].
Mumbai has the highest number of billionaires ЁЯТ╡ЁЯТ╡ЁЯТ╡ out of ANY city in Asia ЁЯдй [a].

The 7я╕ПтГг islands that constitute Mumbai were earlier ЁЯРЯ home to communities of Marathi language-speaking Koli people [25][26][27]. For centuries!!! ЁЯдФ the seven islands of "Bombay" were under the control of successive indigenous rulers ЁЯШ▒ before being ceded to the Portuguese Empire ЁЯЗ╡ЁЯЗ╣, and subsequently to the East India Company in 1661, as part of the dowry ЁЯОБ of Catherine of Braganza ЁЯТГ in her marriage to Charles II of England ЁЯСС [28].

Beginning in 1782 (yep, looong ago ЁЯШВ), Mumbai was reshaped by the Hornby Vellard project ЁЯПЧя╕П [29], which undertook reclamation of the area between the seven islands from the Arabian Sea ЁЯМК [30].
Along with the construction of major roads ЁЯЫгя╕П & railways ЁЯЪВ, the reclamation project (completed in 1845 ЁЯШо) transformed Mumbai into a major seaport тЪУ on the Arabian Sea.

Mumbai in the 19th century = economic ЁЯТ╝ and educational ЁЯУЪ development.
During the early 20th century it became a strong base for the Indian independence movement ЁЯЗоЁЯЗ│тЬК.
Upon India's independence in 1947 ЁЯОЖ the city was incorporated into Bombay State.
In 1960 (after the Samyukta Maharashtra Movement тЬКЁЯТк), a new state of Maharashtra was created with Mumbai as the capital [31].

Mumbai is the financial, commercial, and ЁЯОм entertainment capital of India.
It's often compared to NYC ЁЯЧ╜ЁЯНО [33][34], and is home to the Bombay Stock Exchange ЁЯТ╣ (Dalal Street).
ItтАЩs also one of the world's top ЁЯФЯ centres of commerce ЁЯТ╡ЁЯТ╝ in terms of global financial flow ЁЯМР [35], generating 6.16% of India's GDP ЁЯШ▓ [36], & accounting for 25% of the nation's industrial output, 70% of maritime trade ЁЯЪв (Mumbai Port Trust, Dharamtar Port & JNPT), and 70% of capital transactions ЁЯТ░ЁЯТ░ [38][39].

The city houses financial institutions, HQs of MNCs, scientific institutes ЁЯзк, nuclear institutes тШвя╕П, + the Hindi & Marathi film industries ЁЯОеЁЯО╢.
Mumbai's business opportunities attract migrants from all over India ЁЯМНтЮбя╕ПЁЯЗоЁЯЗ│."""

pip install emoji
from nltk import sent_tokenize,word_tokenize
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
import string
import re
import emoji
clean_sentences=[]
sentences=sent_tokenize(text)
for sentence in sentences:
  #processsing at sentence level html tags, urls, punctuations or any other special char
  #This is for the html tags
  sentence=re.sub(r'https?://\S+',' ', sentence) # Fixed the regex for URLs
  sentence=re.sub(r'<.*?>','',sentence)
  #remove single quotes
  sentence=re.sub(r"'",'',sentence)
  # Remove punctuation, but keep emojis for now
  sentence = sentence.translate(str.maketrans('', '', string.punctuation.replace("","")))
  words = word_tokenize(sentence)
  processed_words = []
  for word in words:
    # Keep digits as they are
    if word.isdigit():
        processed_words.append(word)
    # Convert emojis to text
    elif word in emoji.EMOJI_DATA:
      processed_words.append(emoji.demojize(word).replace(":"," ").replace("_"," ").strip())
    else:
      # Handle multi-character emojis and regular words
      multiemojis = ""
      for ch in word:
        if ch in emoji.EMOJI_DATA:
          multiemojis += " " + emoji.demojize(ch).replace(":", " ").replace("_", " ").strip()
        else:
          multiemojis += ch
      processed_words.append(multiemojis.strip())
  clean_sentences.append(" ".join(processed_words))
print(clean_sentences)

words = word_tokenize(sentence)
for i,word in enumerate(words):
  if word not in string.digits:
    words[i] = word
  #convert the emoji to text
  if word in emoji.EMOJI_DATA:
    words[i] = emoji.demojize(word).replace(":"," ").replace("_"," ").strip()
  else:
    multiemojis=" "
    for ch in word:
      if ch in emoji.EMOJI_DATA:
        print(ch)
        multiemojis +=" " +emoji.demojize(ch).replace(":"," ").replace("_"," ").strip()
      else:
        multiemojis +=ch
      words[i] = multiemojis
  clean_sentences.append(" ".join(words))
  #print words
  print(words)

***Feature Engineering (1. Bag of words(english))***
import sklearn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import pandas as pd
reviews = [
    "Battery Battery life Battery is life excellent and the camera amazing amazing quality is good.",
    "Camera is awesome but battery drains very fast.",
    "Display is bright and the design looks premium.",
    "The phone design and display are amazing.",
    "Sound amazing quality is average but battery backup is strong.",
    "Camera performs well in low light conditions.",
    "The design is sleek and the phone feels premium in hand.",
    "Display and sound are both crystal clear.",
    "Battery amazing charging takes too long but the backup is decent.",
    "Camera excellent quality and display are the best at this price."
]
english_stopwords=stopwords.words('english')
vectorizer= CountVectorizer(stop_words=english_stopwords)
vectorizer=CountVectorizer()
vectorizer
X=vectorizer.fit_transform(reviews)
print(vectorizer.get_feature_names_out())
print(X.toarray())
df=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())
df

***Feature Engineering(1. Bag of words(Hindi)***
reviews_hindi = [
    "рдпрд╣ рдЙрддреНрдкрд╛рдж рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рд╣реИ рдХреЗ",
    "рдореБрдЭреЗ рдпрд╣ рдЙрддреНрдкрд╛рдж рдкрд╕рдВрдж рдЖрдпрд╛",
    "рдЙрддреНрдкрд╛рдж рдХреА рдЧреБрдгрд╡рддреНрддрд╛ рдмреЗрд╣рддрд░реАрди рд╣реИ",
    "рдбрд┐рд▓реАрд╡рд░реА рдмрд╣реБрдд рддреЗрдЬрд╝ рдереА рдФрд░ рдкреИрдХрд┐рдВрдЧ рдЕрдЪреНрдЫреА рдереА",
    "рдореИрдВ рдЗрд╕ рдЙрддреНрдкрд╛рдж рд╕реЗ рд╕рдВрддреБрд╖реНрдЯ рдирд╣реАрдВ рд╣реВрдБ",
    "рдЙрддреНрдкрд╛рдж рдорд╣рдВрдЧрд╛ рд╣реИ рд▓реЗрдХрд┐рди рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рд╣реИ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдмрд╣реБрдд рдЕрдЪреНрдЫреА рд╣реИ",
    "рдЙрддреНрдкрд╛рдж рд╡реИрд╕рд╛ рдирд╣реАрдВ рдЬреИрд╕рд╛ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рдерд╛",
    "рд░рдВрдЧ рдФрд░ рдбрд┐рдЬрд╛рдЗрди рд╢рд╛рдирджрд╛рд░ рд╣реИрдВ",
    "рдбрд┐рд▓реАрд╡рд░реА рдореЗрдВ рдереЛрдбрд╝реА рджреЗрд░реА рд╣реБрдИ рд▓реЗрдХрд┐рди рдЙрддреНрдкрд╛рдж рдЕрдЪреНрдЫрд╛ рд╣реИ"
]
hindi_stopwords = ['рдХреЗ' 'рдХрд╛' 'рдХреА' 'рд╕реЗ' 'рдХреЛ' 'рдФрд░' 'рдкрд░'
                   'рд╣реИ' 'рдерд╛' 'рдереЗ' 'рд╣реВрдБ' 'рд╣реЛ' 'рд╣реИрдВ' 'рдХрд┐' 'рддреЛ'
                   'рднреА' 'рдирд╣реАрдВ' 'рд▓реЗрдХрд┐рди' 'рдпрд╣' 'рд╡рд╣' 'рд╡рд╣рд╛рдБ' 'рдпрд╣рд╛рдБ'
                   'рд╣рдордиреЗ' 'рд╣рдо' 'рдореИрдВ' 'рддреБрдо' 'рдЖрдк' 'рдЙрд╕рдиреЗ' 'рдЙрдиреНрд╣реЛрдВрдиреЗ'
                   'рдХрд┐рдпрд╛' 'рдХрд░рдирд╛' 'рд▓рд┐рдП' 'рдЬрдм' 'рдЬрд╣рд╛рдВ' 'рдХреНрдпреЛрдВрдХрд┐' 'рдпрджрд┐'
                    'рдпрд╛' 'рдЕрдкрдиреЗ' 'рд╕рд╛рде' 'рдмреАрдЪ' 'рдмрд╛рдж' 'рдкрд╣рд▓реЗ' 'рддрдХ'
                   'рдЬреИрд╕реЗ' 'рдХреБрдЫ' 'рд╕рднреА' 'рдЕрдкрдиреА' 'рдЙрд╕рдХреА' 'рдЙрдирдХреЗ' 'рд╣реА'
                   'рдмрд╣реБрдд' 'рдХрд╣рд╛рдБ' 'рдХреМрди' 'рдХреНрдпрд╛' 'рдХреИрд╕реЗ' 'рдХрдм' 'рдиреАрдЪреЗ' 'рдКрдкрд░'
                   'рдРрд╕рд╛' 'рдПрдХ' 'рджреЛ' 'рддреАрди' 'рдХрднреА' 'рд╣рдореЗрд╢рд╛']
vectorizer= CountVectorizer(stop_words=hindi_stopwords,token_pattern="[\u0900-\u097F]+")
# vectorizer=CountVectorizer()
vectorizer
X=vectorizer.fit_transform(reviews_hindi)
print(vectorizer.get_feature_names_out())
print(X.toarray())
df=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())
df

***Feature Engineering(1. Bag of words(Marathi)***
docs_marathi = [
    "рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЦреВрдк рдЪрд╛рдВрдЧрд▓рд╛ рдЖрд╣реЗ",
    "рдорд▓рд╛ рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЖрд╡рдбрд▓рд╛",
    "рдЙрддреНрдкрд╛рджрдирд╛рдЪреА рдЧреБрдгрд╡рддреНрддрд╛ рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗ",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЬрд▓рдж рдЭрд╛рд▓реА рдЖрдгрд┐ рдкреЕрдХрд┐рдВрдЧ рдЫрд╛рди рд╣реЛрддреЗ",
    "рдореА рдпрд╛ рдЙрддреНрдкрд╛рджрдирд╛рдиреЗ рд╕рдорд╛рдзрд╛рдиреА рдирд╛рд╣реА",
    "рдЙрддреНрдкрд╛рджрди рдорд╣рд╛рдЧ рдЖрд╣реЗ рдкрдг рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рдЖрд╣реЗ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдЪрд╛рдВрдЧрд▓реА рдЖрд╣реЗ",
    "рдЙрддреНрдкрд╛рджрди рд╡рд░реНрдгрдирд╛рдкреНрд░рдорд╛рдгреЗ рдирд╛рд╣реА",
    "рд░рдВрдЧ рдЖрдгрд┐ рдбрд┐рдЭрд╛рдИрди рдЦреВрдк рд╕реБрдВрджрд░ рдЖрд╣реЗрдд",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЙрд╢рд┐рд░рд╛ рдЖрд▓реА рдкрдг рдЙрддреНрдкрд╛рджрди рдЫрд╛рди рдЖрд╣реЗ"
]
marathi_stopwords = [
    "рд╣рд╛", "рдорд▓рд╛", "рдЖрд╣реЗ", "рдЖрдгрд┐", "рдЪрд╛рдВрдЧрд▓рд╛", "рдкрдг", "рдирд╛рд╣реА", "рдордзреНрдпрдо",
    "рдЙрддреНрдкрд╛рджрди", "рдЧреБрдгрд╡рддреНрддрд╛", "рдЫрд╛рди", "рдХрд┐рдВрд╡рд╛", "рддреНрдпрд╛рдд", "рддрд░реА", "рдЦреВрдк",
    "рд░рдВрдЧ", "рдбрд┐рдЭрд╛рдИрди", "рдЖрд▓реЗ", "рдордзреНрдпреЗ", "рд╡рд░реНрдгрдирд╛рдкреНрд░рдорд╛рдгреЗ", "рдЙрддреНрдкрд╛рджрди",
    "рд╕реЗрд╡рд╛", "рджрд┐рд▓реЗ", "рд╡рд╛рдкрд░", "рддреБрдореНрд╣реА", "рдЖрд╣реЗрдд", "рддреЗ", "рддреЛ", "рддрд┐рдЪреНрдпрд╛",
    "рдЖрдгрд┐", "рдирдВрддрд░", "рд╕рдВрдкреВрд░реНрдг", "рдЕрд╕рд▓реЗ", "рдХрджрд╛рдЪрд┐рдд", "рддреБрдореНрд╣реА", "рддреЗ", "рдЬреЗ",
    "рддреНрдпрд╛рдЪрд╛", "рдкреНрд░рдХрд╛рд░", "рд╕рд░реНрд╡", "рд╕рд╛рдареА", "рддреЗ", "рддреБрдо", "рдЙрддрд░"
]
vectorizer= CountVectorizer(stop_words=marathi_stopwords,token_pattern="[\u0900-\u097F]+")
# vectorizer=CountVectorizer()
vectorizer
X=vectorizer.fit_transform(docs_marathi)
print(vectorizer.get_feature_names_out())
print(X.toarray())
df=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())
df

***Frequent used term***
eng_reviews = [
    "Battery life is excellent and the camera quality is good.",
    "Camera is awesome but battery drains very fast.",
    "Display is bright and the design looks premium.",
    "The phone design and display are amazing.",
    "Sound quality is average but battery backup is strong.",
    "Camera performs well in low light conditions.",
    "The design is sleek and the phone feels premium in hand.",
    "Display and sound are both crystal clear.",
    "Battery charging takes too long but the backup is decent.",
    "Camera quality and display are the best at this price."
]
eng_stop_words=['amazing' 'and' 'are' 'at' 'average' 'awesome' 'backup' 'battery' 'best'
 'both' 'bright' 'but' 'camera' 'charging' 'clear' 'conditions' 'crystal'
 'decent' 'design' 'display' 'drains' 'excellent' 'fast' 'feels' 'good'
 'hand' 'in' 'is' 'life' 'light' 'long' 'looks' 'low' 'performs' 'phone'
 'premium' 'price' 'quality' 'sleek' 'sound' 'strong' 'takes' 'the' 'this'
 'too' 'very' 'well']
hindi_reviews = [
    "рдпрд╣ рдЙрддреНрдкрд╛рдж рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рд╣реИ",
    "рдореБрдЭреЗ рдпрд╣ рдЙрддреНрдкрд╛рдж рдкрд╕рдВрдж рдЖрдпрд╛",
    "рдЙрддреНрдкрд╛рдж рдХреА рдЧреБрдгрд╡рддреНрддрд╛ рдмреЗрд╣рддрд░реАрди рд╣реИ",
    "рдбрд┐рд▓реАрд╡рд░реА рдмрд╣реБрдд рддреЗрдЬрд╝ рдереА рдФрд░ рдкреИрдХрд┐рдВрдЧ рдЕрдЪреНрдЫреА рдереА",
    "рдореИрдВ рдЗрд╕ рдЙрддреНрдкрд╛рдж рд╕реЗ рд╕рдВрддреБрд╖реНрдЯ рдирд╣реАрдВ рд╣реВрдБ",
    "рдЙрддреНрдкрд╛рдж рдорд╣рдВрдЧрд╛ рд╣реИ рд▓реЗрдХрд┐рди рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рд╣реИ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдмрд╣реБрдд рдЕрдЪреНрдЫреА рд╣реИ",
    "рдЙрддреНрдкрд╛рдж рд╡реИрд╕рд╛ рдирд╣реАрдВ рдЬреИрд╕рд╛ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рдерд╛",
    "рд░рдВрдЧ рдФрд░ рдбрд┐рдЬрд╛рдЗрди рд╢рд╛рдирджрд╛рд░ рд╣реИрдВ",
    "рдбрд┐рд▓реАрд╡рд░реА рдореЗрдВ рдереЛрдбрд╝реА рджреЗрд░реА рд╣реБрдИ рд▓реЗрдХрд┐рди рдЙрддреНрдкрд╛рдж рдЕрдЪреНрдЫрд╛ рд╣реИ"
]
hindi_stopwords = ['рдХреЗ' 'рдХрд╛' 'рдХреА' 'рд╕реЗ' 'рдХреЛ' 'рдФрд░' 'рдкрд░'
                   'рд╣реИ' 'рдерд╛' 'рдереЗ' 'рд╣реВрдБ' 'рд╣реЛ' 'рд╣реИрдВ' 'рдХрд┐' 'рддреЛ'
                   'рднреА' 'рдирд╣реАрдВ' 'рд▓реЗрдХрд┐рди' 'рдпрд╣' 'рд╡рд╣' 'рд╡рд╣рд╛рдБ' 'рдпрд╣рд╛рдБ'
                   'рд╣рдордиреЗ' 'рд╣рдо' 'рдореИрдВ' 'рддреБрдо' 'рдЖрдк' 'рдЙрд╕рдиреЗ' 'рдЙрдиреНрд╣реЛрдВрдиреЗ'
                   'рдХрд┐рдпрд╛' 'рдХрд░рдирд╛' 'рд▓рд┐рдП' 'рдЬрдм' 'рдЬрд╣рд╛рдВ' 'рдХреНрдпреЛрдВрдХрд┐' 'рдпрджрд┐'
                    'рдпрд╛' 'рдЕрдкрдиреЗ' 'рд╕рд╛рде' 'рдмреАрдЪ' 'рдмрд╛рдж' 'рдкрд╣рд▓реЗ' 'рддрдХ'
                   'рдЬреИрд╕реЗ' 'рдХреБрдЫ' 'рд╕рднреА' 'рдЕрдкрдиреА' 'рдЙрд╕рдХреА' 'рдЙрдирдХреЗ' 'рд╣реА'
                   'рдмрд╣реБрдд' 'рдХрд╣рд╛рдБ' 'рдХреМрди' 'рдХреНрдпрд╛' 'рдХреИрд╕реЗ' 'рдХрдм' 'рдиреАрдЪреЗ' 'рдКрдкрд░'
                   'рдРрд╕рд╛' 'рдПрдХ' 'рджреЛ' 'рддреАрди' 'рдХрднреА' 'рд╣рдореЗрд╢рд╛']
marathi_reviews = [
    "рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЦреВрдк рдЪрд╛рдВрдЧрд▓рд╛ рдЖрд╣реЗ рд╢реНрд░реЗрдпрд╛ рдвреЛрдХреЗ рдЕрдореЗрдЬрд╝рди",
    "рдорд▓рд╛ рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЖрд╡рдбрд▓рд╛",
    "рдЙрддреНрдкрд╛рджрдирд╛рдЪреА рдЧреБрдгрд╡рддреНрддрд╛ рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗ",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЬрд▓рдж рдЭрд╛рд▓реА рдЖрдгрд┐ рдкреЕрдХрд┐рдВрдЧ рдЫрд╛рди рд╣реЛрддреЗ",
    "рдореА рдпрд╛ рдЙрддреНрдкрд╛рджрдирд╛рдиреЗ рд╕рдорд╛рдзрд╛рдиреА рдирд╛рд╣реА",
    "рдЙрддреНрдкрд╛рджрди рдорд╣рд╛рдЧ рдЖрд╣реЗ рдкрдг рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рдЖрд╣реЗ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдЪрд╛рдВрдЧрд▓реА рдЖрд╣реЗ",
    "рдЙрддреНрдкрд╛рджрди рд╡рд░реНрдгрдирд╛рдкреНрд░рдорд╛рдгреЗ рдирд╛рд╣реА",
    "рд░рдВрдЧ рдЖрдгрд┐ рдбрд┐рдЭрд╛рдИрди рдЦреВрдк рд╕реБрдВрджрд░ рдЖрд╣реЗрдд",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЙрд╢рд┐рд░рд╛ рдЖрд▓реА рдкрдг рдЙрддреНрдкрд╛рджрди рдЫрд╛рди рдЖрд╣реЗ"
]
marathi_stopwords = [
    "рд╣рд╛", "рдорд▓рд╛", "рдЖрд╣реЗ", "рдЖрдгрд┐", "рдЪрд╛рдВрдЧрд▓рд╛", "рдкрдг", "рдирд╛рд╣реА", "рдордзреНрдпрдо",
    "рдЙрддреНрдкрд╛рджрди", "рдЧреБрдгрд╡рддреНрддрд╛", "рдЫрд╛рди", "рдХрд┐рдВрд╡рд╛", "рддреНрдпрд╛рдд", "рддрд░реА", "рдЦреВрдк",
    "рд░рдВрдЧ", "рдбрд┐рдЭрд╛рдИрди", "рдЖрд▓реЗ", "рдордзреНрдпреЗ", "рд╡рд░реНрдгрдирд╛рдкреНрд░рдорд╛рдгреЗ", "рдЙрддреНрдкрд╛рджрди",
    "рд╕реЗрд╡рд╛", "рджрд┐рд▓реЗ", "рд╡рд╛рдкрд░", "рддреБрдореНрд╣реА", "рдЖрд╣реЗрдд", "рддреЗ", "рддреЛ", "рддрд┐рдЪреНрдпрд╛",
    "рдЖрдгрд┐", "рдирдВрддрд░", "рд╕рдВрдкреВрд░реНрдг", "рдЕрд╕рд▓реЗ", "рдХрджрд╛рдЪрд┐рдд", "рддреБрдореНрд╣реА", "рддреЗ", "рдЬреЗ",
    "рддреНрдпрд╛рдЪрд╛", "рдкреНрд░рдХрд╛рд░", "рд╕рд░реНрд╡", "рд╕рд╛рдареА", "рддреЗ", "рддреБрдо", "рдЙрддрд░"
]

import sklearn
import nltk
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('stopwords')
from nltk.corpus import stopwords
eng_stopwords = stopwords.words('english')
# tfidf_vectorizer = TfidfVectorizer()
tfidf_vectorizer = TfidfVectorizer(stop_words = marathi_stopwords, token_pattern="[\u0900-\u0975]+")
tfidf_vectorizer
matrix = tfidf_vectorizer.fit_transform(marathi_reviews)
# print(matrix)
tfidf_matrix = matrix.sum(0).A1
words = tfidf_vectorizer.get_feature_names_out()
tfidf_matrix.argsort()[::-1][:5]
# most frequently used words in reviews
for i in tfidf_matrix.argsort()[::-1][:5]:
  print(words[i])
print("\n")
# unique used words in reviews
idf_matrix = tfidf_vectorizer.idf_
idf_matrix.argsort()[::-1][:5]
for i in idf_matrix.argsort()[::-1][:5]:
  print(words[i])
# matrix.to_array()
# tfidf_vectorizer.get_feature_names_out()

****Biagrams, Trigrams, ngrams****
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')
bigrams = vectorizer.fit_transform(eng_reviews)
my_bigrams = vectorizer.get_feature_names_out()
my_bigrams
frequency = bigrams.sum(0).A1
frequency
# print("biagrams", vectorizer.get_feature_names_out())
for i in frequency.argsort()[::-1][:5]:
  print(f"biagram: {my_bigrams[i]} || frequency: {frequency[i]}")

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english')
trigrams = vectorizer.fit_transform(eng_reviews)
my_trigrams = vectorizer.get_feature_names_out()
my_trigrams
frequency = trigrams.sum(0).A1
frequency
# print("biagrams", vectorizer.get_feature_names_out())
for i in frequency.argsort()[::-1][:5]:
  print(f"biagram: {my_trigrams[i]} || frequency: {frequency[i]}")

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(2,2), stop_words=hindi_stopwords)
bigrams = vectorizer.fit_transform(hindi_reviews)
my_bigrams = vectorizer.get_feature_names_out()
my_bigrams
frequency = bigrams.sum(0).A1
frequency
# print("biagrams", vectorizer.get_feature_names_out())
for i in frequency.argsort()[::-1][:5]:
  print(f"biagram: {my_bigrams[i]} || frequency: {frequency[i]}")

shape_of_you = ["The club isn't the best place to find a lover",
"So the bar is where I go (mm)",
"Me and my friends at the table doing shots",
"Drinking fast and then we talk slow (mm)",
"Come over and start up a conversation with just me",
"And trust me I'll give it a chance now (mm)",
"Take my hand, stop, put Van the Man on the jukebox",
"And then we start to dance, and now I'm singing like",
"Girl, you know I want your love",
"Your love was handmade for somebody like me",
"Come on now, follow my lead",
"I may be crazy, don't mind me",
"Say, boy, let's not talk too much",
"Grab on my waist and put that body on me",
"Come on now, follow my lead",
"Come, come on now, follow my lead, mm",
"I'm in love with the shape of you",
"We push and pull like a magnet do",
"Although my heart is falling too",
"I'm in love with your body",
"And last night you were in my room",
"And now my bedsheets smell like you",
"Every day discovering something brand new",
"I'm in love with your body",
"(Oh-I-oh-I-oh-I-oh-I)",
"I'm in love with your body",
"(Oh-I-oh-I-oh-I-oh-I)",
"I'm in love with your body",
"(Oh-I-oh-I-oh-I-oh-I)",
"I'm in love with your body",
"Every day discovering something brand new",
"I'm in love with the shape of you",
"One week in we let the story begin",
"We're going out on our first date (mm)",
"You and me are thrifty, so go all you can eat",
"Fill up your bag and I fill up a plate (mm)",
"We talk for hours and hours about the sweet and the sour",
"And how your family is doing okay (mm)",
"And leave and get in a taxi, then kiss in the backseat",
"Tell the driver make the radio play, and I'm singing like",
"Girl, you know I want your love",
"Your love was handmade for somebody like me",
"Come on now, follow my lead",
"I may be crazy, don't mind me",
"Say, boy, let's not talk too much",
"Grab on my waist and put that body on me",
"Come on now, follow my lead",
"Come, come on now, follow my lead, mm",
"I'm in love with the shape of you",
"We push and pull like a magnet do",
"Although my heart is falling too",
"I'm in love with your body",
"And last night you were in my room",
"And now my bedsheets smell like you",
"Every day discovering something brand new",
"I'm in love with your body",
"(Oh-I-oh-I-oh-I-oh-I)",
"I'm in love with your body",
"(Oh-I-oh-I-oh-I-oh-I)",
"I'm in love with your body",
"(Oh-I-oh-I-oh-I-oh-I)",
"I'm in love with your body",
"Every day discovering something brand new",
"I'm in love with the shape of you",
"Come on, be my baby, come on",
"Come on, be my baby, come on",
"Come on, be my baby, come on",
"Come on, be my baby, come on",
"Come on, be my baby, come on",
"Come on, be my baby, come on",
"Come on, be my baby, come on",
"Come on, be my baby, come on",
"I'm in love with the shape of you",
"We push and pull like a magnet do",
"Although my heart is falling too",
"I'm in love with your body",
"And last night you were in my room",
"And now my bedsheets smell like you",
"Every day discovering something brand new",
"I'm in love with your body",
"Come on, be my baby, come on",
"Come on (I'm in love with your body), be my baby, come on",
"Come on, be my baby, come on",
"Come on (I'm in love with your body), be my baby, come on",
"Come on, be my baby, come on",
"Come on (I'm in love with your body), be my baby, come on",
"Every day discovering something brand new",
"I'm in love with the shape of you"]

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(7,7))
trigrams = vectorizer.fit_transform(shape_of_you)
my_trigrams = vectorizer.get_feature_names_out()
my_trigrams
frequency = trigrams.sum(0).A1
frequency
# print("biagrams", vectorizer.get_feature_names_out())
for i in frequency.argsort()[::-1][:5]:
  print(f"biagram: {my_trigrams[i]} || frequency: {frequency[i]}")

****Part of speech tagging****
nltk.download('averaged_perception_tagger')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize
from nltk import pos_tag
for i,doc in enumerate(eng_reviews,1):
  print(f"review {i}:")
  tokens=word_tokenize(doc)
  pos_tags=pos_tag(tokens)
  print(pos_tags)

import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
for i,doc in enumerate(eng_reviews,1):
  print(f"Review {i}: ")
  doc = nlp(doc)
  for token in doc:
    print(token.text, token.pos_)
  print("\n")

from nltk.tokenize import word_tokenize
from nltk import pos_tag
for i,doc in enumerate(marathi_reviews,1):
  print(f"review {i}:")
  tokens=word_tokenize(doc)
  pos_tags=pos_tag(tokens)
  print(pos_tags)

!python -m spacy download xx_ent_wiki_sm

import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
for i,doc in enumerate(hindi_reviews,1):
  print(f"Review {i}: ")
  doc = nlp(doc)
  for token in doc:
    print(token.text, token.pos_)
  print("\n")

***POS using stanza***
!pip install stanza
eng_reviews = [
    "Battery life is excellent and the camera quality is good.",
    "Camera is awesome but battery drains very fast.",
    "Display is bright and the design looks premium.",
    "The phone design and display are amazing.",
    "Sound quality is average but battery backup is strong.",
    "Camera performs well in low light conditions.",
    "The design is sleek and the phone feels premium in hand.",
    "Display and sound are both crystal clear.",
    "Battery charging takes too long but the backup is decent.",
    "Camera quality and display are the best at this price."
]
eng_stop_words=['amazing' 'and' 'are' 'at' 'average' 'awesome' 'backup' 'battery' 'best'
 'both' 'bright' 'but' 'camera' 'charging' 'clear' 'conditions' 'crystal'
 'decent' 'design' 'display' 'drains' 'excellent' 'fast' 'feels' 'good'
 'hand' 'in' 'is' 'life' 'light' 'long' 'looks' 'low' 'performs' 'phone'
 'premium' 'price' 'quality' 'sleek' 'sound' 'strong' 'takes' 'the' 'this'
 'too' 'very' 'well']
hindi_reviews = [
    "рдпрд╣ рдЙрддреНрдкрд╛рдж рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рд╣реИ",
    "рдореБрдЭреЗ рдпрд╣ рдЙрддреНрдкрд╛рдж рдкрд╕рдВрдж рдЖрдпрд╛",
    "рдЙрддреНрдкрд╛рдж рдХреА рдЧреБрдгрд╡рддреНрддрд╛ рдмреЗрд╣рддрд░реАрди рд╣реИ",
    "рдбрд┐рд▓реАрд╡рд░реА рдмрд╣реБрдд рддреЗрдЬрд╝ рдереА рдФрд░ рдкреИрдХрд┐рдВрдЧ рдЕрдЪреНрдЫреА рдереА",
    "рдореИрдВ рдЗрд╕ рдЙрддреНрдкрд╛рдж рд╕реЗ рд╕рдВрддреБрд╖реНрдЯ рдирд╣реАрдВ рд╣реВрдБ",
    "рдЙрддреНрдкрд╛рдж рдорд╣рдВрдЧрд╛ рд╣реИ рд▓реЗрдХрд┐рди рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рд╣реИ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдмрд╣реБрдд рдЕрдЪреНрдЫреА рд╣реИ",
    "рдЙрддреНрдкрд╛рдж рд╡реИрд╕рд╛ рдирд╣реАрдВ рдЬреИрд╕рд╛ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рдерд╛",
    "рд░рдВрдЧ рдФрд░ рдбрд┐рдЬрд╛рдЗрди рд╢рд╛рдирджрд╛рд░ рд╣реИрдВ",
    "рдбрд┐рд▓реАрд╡рд░реА рдореЗрдВ рдереЛрдбрд╝реА рджреЗрд░реА рд╣реБрдИ рд▓реЗрдХрд┐рди рдЙрддреНрдкрд╛рдж рдЕрдЪреНрдЫрд╛ рд╣реИ"
]
hindi_stopwords = ['рдХреЗ' 'рдХрд╛' 'рдХреА' 'рд╕реЗ' 'рдХреЛ' 'рдФрд░' 'рдкрд░'
                   'рд╣реИ' 'рдерд╛' 'рдереЗ' 'рд╣реВрдБ' 'рд╣реЛ' 'рд╣реИрдВ' 'рдХрд┐' 'рддреЛ'
                   'рднреА' 'рдирд╣реАрдВ' 'рд▓реЗрдХрд┐рди' 'рдпрд╣' 'рд╡рд╣' 'рд╡рд╣рд╛рдБ' 'рдпрд╣рд╛рдБ'
                   'рд╣рдордиреЗ' 'рд╣рдо' 'рдореИрдВ' 'рддреБрдо' 'рдЖрдк' 'рдЙрд╕рдиреЗ' 'рдЙрдиреНрд╣реЛрдВрдиреЗ'
                   'рдХрд┐рдпрд╛' 'рдХрд░рдирд╛' 'рд▓рд┐рдП' 'рдЬрдм' 'рдЬрд╣рд╛рдВ' 'рдХреНрдпреЛрдВрдХрд┐' 'рдпрджрд┐'
                    'рдпрд╛' 'рдЕрдкрдиреЗ' 'рд╕рд╛рде' 'рдмреАрдЪ' 'рдмрд╛рдж' 'рдкрд╣рд▓реЗ' 'рддрдХ'
                   'рдЬреИрд╕реЗ' 'рдХреБрдЫ' 'рд╕рднреА' 'рдЕрдкрдиреА' 'рдЙрд╕рдХреА' 'рдЙрдирдХреЗ' 'рд╣реА'
                   'рдмрд╣реБрдд' 'рдХрд╣рд╛рдБ' 'рдХреМрди' 'рдХреНрдпрд╛' 'рдХреИрд╕реЗ' 'рдХрдм' 'рдиреАрдЪреЗ' 'рдКрдкрд░'
                   'рдРрд╕рд╛' 'рдПрдХ' 'рджреЛ' 'рддреАрди' 'рдХрднреА' 'рд╣рдореЗрд╢рд╛']
marathi_reviews = [
    "рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЦреВрдк рдЪрд╛рдВрдЧрд▓рд╛ рдЖрд╣реЗ рд╢реНрд░реЗрдпрд╛ рдвреЛрдХреЗ рдЕрдореЗрдЬрд╝рди",
    "рдорд▓рд╛ рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЖрд╡рдбрд▓рд╛",
    "рдЙрддреНрдкрд╛рджрдирд╛рдЪреА рдЧреБрдгрд╡рддреНрддрд╛ рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗ",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЬрд▓рдж рдЭрд╛рд▓реА рдЖрдгрд┐ рдкреЕрдХрд┐рдВрдЧ рдЫрд╛рди рд╣реЛрддреЗ",
    "рдореА рдпрд╛ рдЙрддреНрдкрд╛рджрдирд╛рдиреЗ рд╕рдорд╛рдзрд╛рдиреА рдирд╛рд╣реА",
    "рдЙрддреНрдкрд╛рджрди рдорд╣рд╛рдЧ рдЖрд╣реЗ рдкрдг рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рдЖрд╣реЗ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдЪрд╛рдВрдЧрд▓реА рдЖрд╣реЗ",
    "рдЙрддреНрдкрд╛рджрди рд╡рд░реНрдгрдирд╛рдкреНрд░рдорд╛рдгреЗ рдирд╛рд╣реА",
    "рд░рдВрдЧ рдЖрдгрд┐ рдбрд┐рдЭрд╛рдИрди рдЦреВрдк рд╕реБрдВрджрд░ рдЖрд╣реЗрдд",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЙрд╢рд┐рд░рд╛ рдЖрд▓реА рдкрдг рдЙрддреНрдкрд╛рджрди рдЫрд╛рди рдЖрд╣реЗ"
]
marathi_stopwords = [
    "рд╣рд╛", "рдорд▓рд╛", "рдЖрд╣реЗ", "рдЖрдгрд┐", "рдЪрд╛рдВрдЧрд▓рд╛", "рдкрдг", "рдирд╛рд╣реА", "рдордзреНрдпрдо",
    "рдЙрддреНрдкрд╛рджрди", "рдЧреБрдгрд╡рддреНрддрд╛", "рдЫрд╛рди", "рдХрд┐рдВрд╡рд╛", "рддреНрдпрд╛рдд", "рддрд░реА", "рдЦреВрдк",
    "рд░рдВрдЧ", "рдбрд┐рдЭрд╛рдИрди", "рдЖрд▓реЗ", "рдордзреНрдпреЗ", "рд╡рд░реНрдгрдирд╛рдкреНрд░рдорд╛рдгреЗ", "рдЙрддреНрдкрд╛рджрди",
    "рд╕реЗрд╡рд╛", "рджрд┐рд▓реЗ", "рд╡рд╛рдкрд░", "рддреБрдореНрд╣реА", "рдЖрд╣реЗрдд", "рддреЗ", "рддреЛ", "рддрд┐рдЪреНрдпрд╛",
    "рдЖрдгрд┐", "рдирдВрддрд░", "рд╕рдВрдкреВрд░реНрдг", "рдЕрд╕рд▓реЗ", "рдХрджрд╛рдЪрд┐рдд", "рддреБрдореНрд╣реА", "рддреЗ", "рдЬреЗ",
    "рддреНрдпрд╛рдЪрд╛", "рдкреНрд░рдХрд╛рд░", "рд╕рд░реНрд╡", "рд╕рд╛рдареА", "рддреЗ", "рддреБрдо", "рдЙрддрд░"
]

docs_hindi = [
    "рдпрд╣ рдЙрддреНрдкрд╛рдж рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рд╣реИ",
    "рдореБрдЭреЗ рдпрд╣ рдЙрддреНрдкрд╛рдж рдкрд╕рдВрдж рдЖрдпрд╛",
    "рдЙрддреНрдкрд╛рдж рдХреА рдЧреБрдгрд╡рддреНрддрд╛ рдмреЗрд╣рддрд░реАрди рд╣реИ",
    "рдбрд┐рд▓реАрд╡рд░реА рдмрд╣реБрдд рддреЗрдЬрд╝ рдереА рдФрд░ рдкреИрдХрд┐рдВрдЧ рдЕрдЪреНрдЫреА рдереА",
    "рдореИрдВ рдЗрд╕ рдЙрддреНрдкрд╛рдж рд╕реЗ рд╕рдВрддреБрд╖реНрдЯ рдирд╣реАрдВ рд╣реВрдБ",
    "рдЙрддреНрдкрд╛рдж рдорд╣рдВрдЧрд╛ рд╣реИ рд▓реЗрдХрд┐рди рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рд╣реИ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдмрд╣реБрдд рдЕрдЪреНрдЫреА рд╣реИ",
    "рдЙрддреНрдкрд╛рдж рд╡реИрд╕рд╛ рдирд╣реАрдВ рдЬреИрд╕рд╛ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рдерд╛",
    "рд░рдВрдЧ рдФрд░ рдбрд┐рдЬрд╛рдЗрди рд╢рд╛рдирджрд╛рд░ рд╣реИрдВ",
    "рдбрд┐рд▓реАрд╡рд░реА рдореЗрдВ рдереЛрдбрд╝реА рджреЗрд░реА рд╣реБрдИ рд▓реЗрдХрд┐рди рдЙрддреНрдкрд╛рдж рдЕрдЪреНрдЫрд╛ рд╣реИ"
]
docs_marathi = [
    "рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЦреВрдк рдЪрд╛рдВрдЧрд▓рд╛ рдЖрд╣реЗ",
    "рдорд▓рд╛ рд╣рд╛ рдЙрддреНрдкрд╛рджрди рдЖрд╡рдбрд▓рд╛",
    "рдЙрддреНрдкрд╛рджрдирд╛рдЪреА рдЧреБрдгрд╡рддреНрддрд╛ рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗ",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЬрд▓рдж рдЭрд╛рд▓реА рдЖрдгрд┐ рдкреЕрдХрд┐рдВрдЧ рдЫрд╛рди рд╣реЛрддреЗ",
    "рдореА рдпрд╛ рдЙрддреНрдкрд╛рджрдирд╛рдиреЗ рд╕рдорд╛рдзрд╛рдиреА рдирд╛рд╣реА",
    "рдЙрддреНрдкрд╛рджрди рдорд╣рд╛рдЧ рдЖрд╣реЗ рдкрдг рдЧреБрдгрд╡рддреНрддрд╛ рдареАрдХ рдЖрд╣реЗ",
    "рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдЪрд╛рдВрдЧрд▓реА рдЖрд╣реЗ",
    "рдЙрддреНрдкрд╛рджрди рд╡рд░реНрдгрдирд╛рдкреНрд░рдорд╛рдгреЗ рдирд╛рд╣реА",
    "рд░рдВрдЧ рдЖрдгрд┐ рдбрд┐рдЭрд╛рдИрди рдЦреВрдк рд╕реБрдВрджрд░ рдЖрд╣реЗрдд",
    "рдбрд┐рд▓рд┐рд╡реНрд╣рд░реА рдЙрд╢рд┐рд░рд╛ рдЖрд▓реА рдкрдг рдЙрддреНрдкрд╛рджрди рдЫрд╛рди рдЖрд╣реЗ"
]
import stanza
stanza.download("hi")
stanza.download('mr')
nlp_hi = stanza.Pipeline("mr")
for i,doc in enumerate(docs_marathi,1):
    doc_obj = nlp_hi(doc)
    print(f"\nSentence {i}: {doc}")
    for sentence in doc_obj.sentences:
        for word in sentence.words:
            print(f"{word.text}/{word.upos}", end=' ')
    print()
for i,doc in enumerate(docs_hindi,1):
    doc_obj = nlp_hi(doc)
    print(f"\nSentence {i}: {doc}")
    for sentence in doc_obj.sentences:
        for word in sentence.words:
            print(f"{word.text}/{word.upos}", end=' ')
    print()

****Named Entity Recognition****
!pip install jupyter
!pip install jupyter displacy

sentences = {
    "Barack Obama was the 44th President of the United States.",
    "Apple Inc. released the iPhone 15 in September 2023.",
    "Sachin Tendulkar scored his 100th century at Mirpur Stadium.",
    "The Eiffel Tower is one of the most famous landmarks in Paris.",
    "Google was founded by Larry Page and Sergey Brin at Stanford University.",
    "The Olympic Games will be held in Los Angeles in 2028.",
    "Amazon opened a new data center in Mumbai last year.",
    "NASA launched the Artemis mission to the Moon in November 2022."
}

import jupyter
import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
for i, review in enumerate(sentences, i):
  print(review)
  doc = nlp(review)
  if doc.ents:
    for ent in doc.ents:
      print(f"Entitiy {i}: {ent.text} - {ent.label_} - {spacy.explain(ent.label_)}")
      print("\n")
  else:
    print("not found")

sentences_mr = {
    "Barack Obama рд╣реЗ рдЕрдореЗрд░рд┐рдХреЗрдЪреЗ рдорд╛рдЬреА рд░рд╛рд╖реНрдЯреНрд░рд╛рдзреНрдпрдХреНрд╖ рдЖрд╣реЗрдд.",
    "рдореА рдЧреЗрд▓реНрдпрд╛ рд╡рд░реНрд╖реА Paris рд▓рд╛ рдкреНрд░рд╡рд╛рд╕ рдХреЗрд▓рд╛ рд╣реЛрддрд╛.",
    "Ratan Tata рд╣реЗ рднрд╛рд░рддрд╛рддреАрд▓ рд╕рд░реНрд╡рд╛рдд рдЖрджрд░рдгреАрдп рдЙрджреНрдпреЛрдЧрдкрддреАрдВрдкреИрдХреА рдПрдХ рдЖрд╣реЗрдд.",
    "Apple рдХрдВрдкрдиреАрдиреЗ рдирд╡реАрди iPhone 15 рдмрд╛рдЬрд╛рд░рд╛рдд рдЖрдгрд▓рд╛ рдЖрд╣реЗ.",
    "Shravya рдпрд╛рдВрдирд╛ рдХреНрд░рд┐рдХреЗрдЯрдЪрд╛ рджреЗрд╡ рдорд╛рдирд▓реЗ рдЬрд╛рддреЗ.",
    "Google рдЪреЗ рдореБрдЦреНрдпрд╛рд▓рдп California рдордзреНрдпреЗ рдЖрд╣реЗ.",
    "Taj Mahal рд╣рд╛ рднрд╛рд░рддрд╛рддреАрд▓ рд╕рд░реНрд╡рд╛рдд рдкреНрд░рд╕рд┐рджреНрдз рд╕реНрдорд╛рд░рдХ рдЖрд╣реЗ.",
    "Elon Musk рдпрд╛рдВрдиреА SpaceX рдЖрдгрд┐ Tesla рдпрд╛ рдХрдВрдкрдиреНрдпрд╛ рд╕реНрдерд╛рдкрди рдХреЗрд▓реНрдпрд╛."
}

import jupyter
import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
for i, review in enumerate(sentences_mr, i):
  print(review)
  doc = nlp(review)
  if doc.ents:
    for ent in doc.ents:
      print(f"Entitiy {i}: {ent.text} - {ent.label_} - {spacy.explain(ent.label_)}")
      print("\n")
  else:
    print("not found")

pure_marathi = {
    "рдЫрддреНрд░рдкрддреА рд╢рд┐рд╡рд╛рдЬреА рдорд╣рд╛рд░рд╛рдЬ рдпрд╛рдВрдиреА рд╕реНрд╡рд░рд╛рдЬреНрдпрд╛рдЪреА рд╕реНрдерд╛рдкрдирд╛ рдХреЗрд▓реА.",
    "рдбреЙ. рдмрд╛рдмрд╛рд╕рд╛рд╣реЗрдм рдЖрдВрдмреЗрдбрдХрд░ рдпрд╛рдВрдиреА рднрд╛рд░рддреАрдп рд╕рдВрд╡рд┐рдзрд╛рди рддрдпрд╛рд░ рдХреЗрд▓реЗ.",
    "рд╕рдЪрд┐рди рддреЗрдВрдбреБрд▓рдХрд░ рд╣рд╛ рдЬрдЧрдкреНрд░рд╕рд┐рджреНрдз рдХреНрд░рд┐рдХреЗрдЯрдкрдЯреВ рдЖрд╣реЗ.",
    "рд▓рддрд╛ рдордВрдЧреЗрд╢рдХрд░ рдпрд╛рдВрдЪрд╛ рдЖрд╡рд╛рдЬ рд╕рдВрдкреВрд░реНрдг рдЬрдЧрднрд░ рдЧрд╛рдЬрд▓рд╛.",
    "рдкреБрдгреЗ рд╣реЗ рдорд╣рд╛рд░рд╛рд╖реНрдЯреНрд░рд╛рддреАрд▓ рдПрдХ рдРрддрд┐рд╣рд╛рд╕рд┐рдХ рд╢рд╣рд░ рдЖрд╣реЗ.",
    "рдорд╣рд╛рд░рд╛рд╖реНрдЯреНрд░рд╛рдЪреЗ рдореБрдЦреНрдпрдордВрддреНрд░реА рдПрдХрдирд╛рде рд╢рд┐рдВрджреЗ рдпрд╛рдВрдиреА рд╕рднрд╛ рдШреЗрддрд▓реА.",
    "рддрд╛рдЬрдорд╣рд╛рд▓ рдЖрдЧреНрд░рд╛ рдпреЗрдереЗ рд╕реНрдерд┐рдд рдЖрд╣реЗ.",
    "рднрд╛рд░рддреАрдп рдЕрд╡рдХрд╛рд╢ рд╕рдВрд╢реЛрдзрди рд╕рдВрд╕реНрдерд╛ рдореНрд╣рдгрдЬреЗрдЪ рдЗрд╕реНрд░реЛ рдпрд╛рдВрдиреА рдЪрд╛рдВрджреНрд░рдпрд╛рди рдореЛрд╣рд┐рдо рдпрд╢рд╕реНрд╡реА рдХреЗрд▓реА.",
    "рдореБрдВрдмрдИрдд рдЧреЗрдЯрд╡реЗ рдСрдл рдЗрдВрдбрд┐рдпрд╛ рд╣реЗ рдкреНрд░рд╕рд┐рджреНрдз рдкрд░реНрдпрдЯрдирд╕реНрдерд│ рдЖрд╣реЗ.",
    "рдкрдВрдбрд┐рдд рднреАрдорд╕реЗрди рдЬреЛрд╢реА рд╣реЗ рдкреНрд░рд╕рд┐рджреНрдз рд╢рд╛рд╕реНрддреНрд░реАрдп рдЧрд╛рдпрдХ рд╣реЛрддреЗ."
}

#ЁЯз╛ Stanza (Indic Language) NER Tag Reference
#Tag	Meaning / Description	Example
#NEP	Person / Name	Shivaji, Ambedkar, Sachin
#NEL	Location / Place	Pune, Mumbai, Agra
#NEO	Organization / Institution	ISRO, Tata Group, LIC
#ED	Honorific / Title / Designation	Dr., Sir, Maharaj, Chhatrapati
#TIM	Time Expression (Date / Time)	2023, Monday, Yesterday
#NUM	Numeric / Quantity	10, 5000, 3 crore
#MISC	Miscellaneous / Other Entity Types	(anything not covered above)

import stanza
nlp_mr = stanza.Pipeline("mr")
for i, review in enumerate(pure_marathi, i):
  print(review)
  doc = nlp_mr(review)
  if doc.ents:
    for ent in doc.ents:
      print(f"Entitiy {i}: {ent.text} - {ent.type}")
      print("\n")
  else:
    print("not found")

text = """
In 2023, Prime Minister Narendra Modi inaugurated the new Parliament building in New Delhi.
Later, he attended a conference organized by the United Nations on climate change.
Meanwhile, Ratan Tata visited the Tata Motors plant in Pune.
Apple Inc. announced its new iPhone model during an event held in California.
Sachin Tendulkar was invited as a special guest at the Mumbai Marathon 2023.
"""
doc = nlp(text)
displacy.render(doc, style="ent", jupyter=True)

****Detecting Features****
from numpy import vectorize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
vectorizer = CountVectorizer(stop_words='english')
vectorizer
lda = LatentDirichletAllocation(n_components=6, random_state=42)
x = vectorizer.fit_transform(sentences)
lda.fit(x)
words = vectorizer.get_feature_names_out()
for i, topic in enumerate(lda.components_):
  topics = topic.argsort()[-5:]
  print([words[i] for i in topics])
print("\n")

from numpy import vectorize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
vectorizer = CountVectorizer(stop_words=marathi_stopwords)
vectorizer
lda = LatentDirichletAllocation(n_components=6, random_state=42)
x = vectorizer.fit_transform(pure_marathi)
lda.fit(x)
words = vectorizer.get_feature_names_out()
for i, topic in enumerate(lda.components_):
  topics = topic.argsort()[-5:]
  print([words[i] for i in topics])
print("\n")

***Language identification for regional Indian languages***
from transformers import pipeline
texts = [
    "I love programming in Python.",                           # English
    "Hola amigo, ┬┐c├│mo est├бs?",                                # Spanish
    "Je suis heureux aujourdтАЩhui.",                            # French
    "Ich liebe Programmieren.",                                # German
    "Ciao, come stai?",                                        # Italian
    "уБУуВУуБлуБбуБпуАБхЕГц░ЧуБзуБЩуБЛя╝Я",                                      # Japanese
    "ф╜ахе╜я╝Мф╗Кхдйхе╜хРЧя╝Я",                                           # Chinese
    "╨Я╤А╨╕╨▓╨╡╤В, ╨║╨░╨║ ╨┤╨╡╨╗╨░?",                                       # Russian
    "ьХИыЕХэХШьД╕ьЪФ, ьШдыКШ ъ╕░ы╢ДьЭ┤ ьЦ┤ыХМьЪФ?",                               # Korean
    "рд╣реИрд▓реЛ, рдЖрдк рдХреИрд╕реЗ рд╣реИрдВ?",                                      # Hindi
    "ржЖржорж┐ ржмрж╛ржВрж▓рж╛ржпрж╝ ржХржерж╛ ржмрж▓рж┐ред",                                    # Bengali
    "р░др▒Жр░▓р▒Бр░Чр▒Б р░ир░╛р░Хр▒Б р░Зр░╖р▒Нр░Яр░В.",                                       # Telugu
    "родрооро┐ро┤рпН роОройроХрпНроХрпБ рокро┐роЯро┐роХрпНроХрпБроорпН.",                                   # Tamil
    "р┤ор┤▓р┤пр┤╛р┤│р┤В р┤Тр┤░р╡Б р┤ор┤ир╡Лр┤╣р┤░р┤ор┤╛р┤п р┤нр┤╛р┤╖р┤пр┤╛р┤гр╡Н.",                           # Malayalam
    "р▓Хр▓ир│Нр▓ир▓б р▓Тр▓Вр▓жр│Б р▓╕р│Бр▓Вр▓жр▓░ р▓нр▓╛р▓╖р│Ж.",                                    # Kannada
    "рдорд░рд╛рдареА рдорд╛рдЭреА рдорд╛рддреГрднрд╛рд╖рд╛ рдЖрд╣реЗ.",                                 # Marathi
    "ркЧрлБркЬрк░рк╛ркдрлА ркорк╛рк░рлА ркорк╛ркдрлГркнрк╛рк╖рк╛ ркЫрлЗ.",                                # Gujarati
    "рикрй░риЬри╛римрйА риорйЗри░рйА риори╛ридрйНри░ринри╛ри╕ри╝ри╛ ри╣рйИред",                               # Punjabi
    "рмУрмбрм╝рм┐рмЖ рморнЛ рморм╛рмдрнГрмнрм╛рм╖рм╛ рмЕрмЯрнЗред",                                 # Odia
    "ржЕрж╕ржорзАржпрж╝рж╛ ржнрж╛рж╖рж╛ ржорзЛрз░ ржЧрзМрз░рз▒ред",                                   # Assamese
    "рд╕рдВрд╕реНрдХреГрддрдВ рднрд╛рд░рддрд╕реНрдп рдкреНрд░рд╛рдЪреАрдирддрдордВ рднрд╛рд╖рд╛рд╜рд╕реНрддрд┐ред",                    # Sanskrit
    "рдореИрдерд┐рд▓реА рднрд╛рд╖рд╛ рдмрд╣реБрдд рдордзреБрд░ рд╣реИред",                                 # Maithili
    "ржмрж╛ржВрж▓рж╛ ржПржмржВ ржЕрж╕ржорзАржпрж╝рж╛ ржжрзБржЗржЯрж┐ ржЖрж▓рж╛ржжрж╛ ржнрж╛рж╖рж╛ред",                         # BengaliтАУAssamese variant
    "╪з╪▒╪п┘И ╪з█М┌й ╪о┘И╪и╪╡┘И╪▒╪к ╪▓╪и╪з┘Ж █Б█Т█Ф",                                # Urdu
    "рдХреЛрдХрдгреА рдорд╛рдЭреА рдЖрд╡рдбрддреА рднрд╛рд╖рд╛ рдЖрд╣реЗ.",                               # Konkani
    "рдиреЗрдкрд╛рд▓реА рдореЗрд░реЛ рдорд╛рддреГрднрд╛рд╖рд╛ рд╣реЛред",                                 # Nepali
    "рднреЛрдЬрдкреБрд░реА рдореЗрдВ рдмрд╛рдд рдХрд░реАрдВ?",                                    # Bhojpuri
    "рдбреЛрдЧрд░реА рднрд╛рд╖рд╛ рдЬрдореНрдореВ рдХреНрд╖реЗрддреНрд░ рдХреА рдкреНрд░рдореБрдЦ рднрд╛рд╖рд╛ рд╣реИред",                # Dogri
    "рдХрд╢реНрдореАрд░реА рднрд╛рд╖рд╛ рдмрд╣реБрдд рдкреБрд░рд╛рдиреА рд╣реИред",                              # Kashmiri
    "рдордгрд┐рдкреБрд░реА ржПржХржЯрж┐ рж╕рзБржирзНржжрж░ ржнрж╛рж╖рж╛ред",                                 # Manipuri (Meitei)
    "рд╕рдВрддрд╛рд▓реА рдЖрдорд╛рд░ рднрд╛рд╖рд╛ рдЖрд╣реЗред",                                    # Santali (Ol Chiki transliteration)
    "рдмреЛрдбреЛ рднрд╛рд╖рд╛ рдЕрд╕рдо рдореЗрдВ рдмреЛрд▓реА рдЬрд╛рддреА рд╣реИред",                           # Bodo
    "рдореИрдерд┐рд▓реА рдФрд░ рднреЛрдЬрдкреБрд░реА рджреЛрдиреЛрдВ рдкреВрд░реНрд╡реА рднрд╛рд░рдд рдореЗрдВ рдмреЛрд▓реА рдЬрд╛рддреА рд╣реИрдВред"       # Eastern group
]
lang_identifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
for text in texts:
  result = lang_identifier(text)
  print(result)

pip install langid langdetect

import langdetect
import langid
texts = [
    "I love programming in Python.",                           # English
    "рд╣реИрд▓реЛ, рдЖрдк рдХреИрд╕реЗ рд╣реИрдВ?",                                      # Hindi
    "ржЖржорж┐ ржмрж╛ржВрж▓рж╛ржпрж╝ ржХржерж╛ ржмрж▓рж┐ред",                                    # Bengali
    "р░др▒Жр░▓р▒Бр░Чр▒Б р░ир░╛р░Хр▒Б р░Зр░╖р▒Нр░Яр░В.",                                       # Telugu
    "родрооро┐ро┤рпН роОройроХрпНроХрпБ рокро┐роЯро┐роХрпНроХрпБроорпН.",                                   # Tamil
    "р┤ор┤▓р┤пр┤╛р┤│р┤В р┤Тр┤░р╡Б р┤ор┤ир╡Лр┤╣р┤░р┤ор┤╛р┤п р┤нр┤╛р┤╖р┤пр┤╛р┤гр╡Н.",                           # Malayalam
    "р▓Хр▓ир│Нр▓ир▓б р▓Тр▓Вр▓жр│Б р▓╕р│Бр▓Вр▓жр▓░ р▓нр▓╛р▓╖р│Ж.",                                    # Kannada
    "рдорд░рд╛рдареА рдорд╛рдЭреА рдорд╛рддреГрднрд╛рд╖рд╛ рдЖрд╣реЗ.",                                 # Marathi
    "ркЧрлБркЬрк░рк╛ркдрлА ркорк╛рк░рлА ркорк╛ркдрлГркнрк╛рк╖рк╛ ркЫрлЗ.",                                # Gujarati
    "рикрй░риЬри╛римрйА риорйЗри░рйА риори╛ридрйНри░ринри╛ри╕ри╝ри╛ ри╣рйИред",                               # Punjabi
    "ржЕрж╕ржорзАржпрж╝рж╛ ржнрж╛рж╖рж╛ ржорзЛрз░ ржЧрзМрз░рз▒ред",                                   # Assamese
      "рдбреЛрдЧрд░реА рднрд╛рд╖рд╛ рдЬрдореНрдореВ рдХреНрд╖реЗрддреНрд░ рдХреА рдкреНрд░рдореБрдЦ рднрд╛рд╖рд╛ рд╣реИред",                # Dogri
     "рдордгрд┐рдкреБрд░реА ржПржХржЯрж┐ рж╕рзБржирзНржжрж░ ржнрж╛рж╖рж╛ред",                                 # Manipuri (Meitei)
    "рд╕рдВрддрд╛рд▓реА рдЖрдорд╛рд░ рднрд╛рд╖рд╛ рдЖрд╣реЗред",                                    # Santali (Ol Chiki transliteration)
    "рдмреЛрдбреЛ рднрд╛рд╖рд╛ рдЕрд╕рдо рдореЗрдВ рдмреЛрд▓реА рдЬрд╛рддреА рд╣реИред",                           # Bodo
    "рдореИрдерд┐рд▓реА рдФрд░ рднреЛрдЬрдкреБрд░реА рджреЛрдиреЛрдВ рдкреВрд░реНрд╡реА рднрд╛рд░рдд рдореЗрдВ рдмреЛрд▓реА рдЬрд╛рддреА рд╣реИрдВред"       # Eastern group
    "рдХреЛрдХрдгреА рдорд╛рдЭреА рдЖрд╡рдбрддреА рднрд╛рд╖рд╛ рдЖрд╣реЗ.",                               # Konkani
]
for text in texts:
  lang,confidence= langid.classify(text)
  print(f"Language : {lang}, Confidence: {confidence}")

from langdetect import detect, detect_langs
for text in texts:
  lang = detect_langs(text)
  print(f"{text} - Language: {lang}")

***Sentiment Emotion Detection***
movie_reviews = [
    "Absolutely loved the movie! The acting and story were brilliant.",
    "Terrible film. I wasted two hours of my life watching this nonsense.",
    "The visuals were stunning, but the story was weak.",
    "A heartwarming tale with powerful performances by the cast.",
    "The movie was too slow and boring. Fell asleep halfway.",
    "Fantastic direction and cinematography. Truly a masterpiece!",
    "Mediocre at best. Not worth the hype.",
    "The visuals were stunning, but the story was weak.",
    "The movie was too slow and boring. Fell asleep halfway.",
    "Brilliant plot twists and engaging screenplay kept me hooked.",
    "The dialogues were cringe-worthy and predictable.",
    "Good movie overall, but the ending felt rushed.",
    "An emotional rollercoaster that made me cry and smile.",
    "I didnтАЩt understand what was going on. Very confusing script.",
    "A perfect family entertainer with great music.",
    "The lead actor carried the entire film. Superb performance!",
    "Too much violence and unnecessary drama ruined the movie.",
    "Decent watch, but could have been much better.",
    "An inspiring story that stays with you long after watching.",
    "Poor editing and weak character development.",
    "A fresh and original concept beautifully executed.",
    "One of the worst movies IтАЩve seen this year."
]

!pip install vaderSentiment
!pip install --upgrade vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')
import pandas as pd
analyzer = SentimentIntensityAnalyzer()
data = []
for reviews in movie_reviews:
    sentiment_scores = analyzer.polarity_scores(reviews)
    # print(f"{reviews} --> {sentiment_scores}")
    data.append({
        'review':reviews,
        'neutral':sentiment_scores['neu'],
        'positive':sentiment_scores['pos'],
        'negative':sentiment_scores['neg'],
        'compound':sentiment_scores['compound']
    })
df = pd.DataFrame(data)
df
df['sentiments'] = df['compound'].apply(lambda x: 'positive' if x >= 0.05 else ('negative' if x <= -0.05 else 'neutral'))
df

***emotion detection using pre-trained***
from transformers import pipeline
emotion_analyzer = pipeline('text-classification', model="j-hartmann/emotion-english-distilroberta-base",
                            return_all_scores=True)
emotion_analyzer
for r in movie_reviews:
    result = emotion_analyzer(r)[0]
    top_emotion = max(result, key=lambda x: x['score'])
    print(f"{r}\nтЖТ Emotion: {top_emotion['label']} (score: {top_emotion['score']:.2f})\n")

from transformers import pipeline
emotion_classifier = pipeline(
    "text-classification",
    model="SamLowe/roberta-base-go_emotions",
    return_all_scores=False
)
reviews = [
    "Absolutely loved the movie! The acting and story were brilliant.",
    "Terrible film. I wasted two hours of my life watching this nonsense.",
    "The movie was too slow and boring. Fell asleep halfway.",
    "The visuals were stunning and full of admiration for the craft."
]
for r in reviews:
    print(f"{r}\nтЖТ {emotion_classifier(r)[0]}\n")
# Absolutely loved the movie! The acting and story were brilliant.
# тЖТ {'label': 'admiration', 'score': 0.8873493671417236}
# Terrible film. I wasted two hours of my life watching this nonsense.
# тЖТ {'label': 'disgust', 'score': 0.2688419818878174}
# The movie was too slow and boring. Fell asleep halfway.
# тЖТ {'label': 'disappointment', 'score': 0.6511252522468567}
# The visuals were stunning and full of admiration for the craft.
# тЖТ {'label': 'admiration', 'score': 0.9596165418624878}

data = []
for review in movie_reviews:
    emotion_scores = emotion_analyzer(review)[0]
    # print(review)
    # print(emotion_scores)
    top_emotion = max(emotion_scores, key=lambda x: x['score'])
    # print(top_emotion)
    data.append({
        'review':review,
        'emotion':top_emotion['label'],
        'score':top_emotion['score']
    })
df = pd.DataFrame(data)
df

from transformers import pipeline
lang_identifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
texts = [
    "I love programming in Python.",                 # English
    "Je suis heureux aujourdтАЩhui.",                 # French
    "рдирдорд╕реНрддреЗ, рдЖрдкрдХрд╛ рджрд┐рди рд╢реБрдн рд╣реЛред",                     # Hindi
    "рдореА рдорд░рд╛рдареА рдмреЛрд▓рддреЛ.",                              # Marathi
    "рждрзБржорж┐ ржХрзЗржоржи ржЖржЫрзЛ?",                              # Bengali
    "рк╣рлБркВ ркЧрлБркЬрк░рк╛ркдрлАркорк╛ркВ рк╡рк╛ркд ркХрк░рлБркВ ркЫрлБркВ.",                   # Gujarati
    "╪в┘╛ ┌й█М╪│█Т █Б█М┌║╪Я",                                 # Urdu
    "Hola amigo, ┬┐c├│mo est├бs?"                      # Spanish
]

for text in texts:
  result = lang_identifier(text)[0]
  print(result)

!pip install langdetect

from langdetect import detect, detect_langs
for text in texts:
  try:
    print(f"{text} --> {detect(text)}")
  except:
    print(f"{text} --> {detect_langs(text)}")

!pip install langid
import langid
for t in texts:
    lang, confidence = langid.classify(t)
    print(f"{t} тЖТ {lang} ({confidence:.2f})")

from langdetect import detect, detect_langs
import pandas as pd
data = []
for text in texts:
    try:
        lang = detect(text)
    except:
        lang = "unknown"
    data.append({'Text': text, 'Detected_Language': lang})
df = pd.DataFrame(data)
print(df)

!pip install transformers torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-xlm-roberta-base-sentiment")
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-xlm-roberta-base-sentiment")
hindi_reviews = [
    "рдореБрдЭреЗ рдпрд╣ рдлрд┐рд▓реНрдо рдмрд╣реБрдд рдкрд╕рдВрдж рдЖрдИред",  # I really liked this movie (Positive)
    "рдпрд╣ рдХрд╣рд╛рдиреА рдмрд╣реБрдд рдмреЛрд░рд┐рдВрдЧ рдереАред",     # The story was very boring (Negative)
    "рдЕрднрд┐рдирдп рдЕрдЪреНрдЫрд╛ рдерд╛ рд▓реЗрдХрд┐рди рдХрд╣рд╛рдиреА рдХрдордЬреЛрд░ рдереАред",  # Acting was good but story was weak (Neutral/Mixed)
    "рд╕рдВрдЧреАрдд рд╢рд╛рдирджрд╛рд░ рдерд╛!",             # The music was amazing (Positive)
    "рджреВрд╕рд░рд╛ рднрд╛рдЧ рдЙрддрдирд╛ рдЕрдЪреНрдЫрд╛ рдирд╣реАрдВ рдерд╛ред",  # The sequel wasnтАЩt that good (Negative)
    "рдлрд┐рд▓реНрдо рдареАрдХ-рдард╛рдХ рдереА, рдХреБрдЫ рдЦрд╛рд╕ рдирд╣реАрдВред"   # The movie was okay, nothing special (Neutral)
]

# Step 5: Sentiment labels
labels = ['negative', 'neutral', 'positive']
results = []
for review in hindi_reviews:
   # Convert text to model input format
    inputs = tokenizer(review, return_tensors="pt", truncation=True, padding=True)
    # Run model
    outputs = model(**inputs)
    #  Convert model output to probabilities
    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)
    # Find which label (negative/neutral/positive) has the highest score
    sentiment = labels[torch.argmax(scores)]
    # Save result
    results.append({'Review': review, 'Sentiment': sentiment})
# Step 7: Show output as a table
df = pd.DataFrame(results)
print(df)
